{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihoods we have used:\n",
    "1. regression: Gaussian\n",
    "2. count regression: \n",
    "    Poisson: lambda=e^f,  Poisson2: lambda=ln(1+e^f)\n",
    "3. binary classification: \n",
    "    Bernoulli: sigmoid,   Bernoulli2: Probit\n",
    "    \n",
    "For the sigmoid function, we use **expit** which is provided by *scipy.special*, because it is stable, fast and fairly accurate. Additionally, it is equivalent to sigmoid. For all methods, initial variational parameters are found by running the Laplace approximation on the subset/active set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv,norm,lstsq,cholesky\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy.io as sio\n",
    "from sklearn import preprocessing\n",
    "import random,time,GPy\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import loggamma,roots_hermitenorm,gamma,expit\n",
    "from scipy.special import ndtr as std_norm_cdf\n",
    "\n",
    "_sqrt_2pi = np.sqrt(2*np.pi)\n",
    "_lim_val = np.finfo(np.float64).max\n",
    "_lim_val_exp = np.log(_lim_val)\n",
    "\n",
    "def std_norm_pdf(x): # define a standard normal pdf(from GPy)\n",
    "    x = np.clip(x,-1e300,1e300)\n",
    "    return np.exp(-np.square(x)/2)/_sqrt_2pi\n",
    "\n",
    "def safe_exp(f):\n",
    "    clip_f = np.clip(f, -np.inf, _lim_val_exp)\n",
    "    return np.exp(clip_f)\n",
    "\n",
    "def safe_ln(x, minval=0.0000000001):\n",
    "    return np.log(x.clip(min=minval))\n",
    "\n",
    "def kernel(X1, X2=None, l=1.0, sigma_f=1.0,K='SE'):\n",
    "    '''\n",
    "    Isotropic squared exponential kernel. Computes \n",
    "    a covariance matrix from points in X1 and X2.    \n",
    "    Args:\n",
    "        X1: Array of m points (m x d).\n",
    "        X2: Array of n points (n x d).\n",
    "\n",
    "    Returns:\n",
    "        Covariance matrix (m x n).\n",
    "    '''\n",
    "    if K=='diag': return sigma_f**2*np.ones((X1.shape[0], 1))\n",
    "    sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T);sqdist=np.absolute(sqdist)#.ravel()))\n",
    "    if K=='SE': return sigma_f**2 * np.exp(-0.5 / l**2 * sqdist);\n",
    "    elif K=='Matern32': return sigma_f**2 * (1+3**0.5*np.sqrt(sqdist)/l) * np.exp(-3**0.5*np.sqrt(sqdist)/l);\n",
    "    elif K=='Matern52': return sigma_f**2 * (1+3**0.5*np.sqrt(sqdist)/l+5*sqdist/(3*l**2)) * np.exp(-5**0.5*np.sqrt(sqdist)/l);\n",
    "\n",
    "def MFE(true_labels, pred_labels):\n",
    "    '''\n",
    "    Calculating mean fraction error(MFE) for count regression, the math is used as follows:\n",
    "    MFE = mean( abs( (true_labels - pred_labels)./true_labels ) ) \n",
    "    Written by Kaikai\n",
    "    '''    \n",
    "    if true_labels.size != pred_labels.size:\n",
    "        print('The size of true_labels and pred_labels is supposed to be identical.')\n",
    "        return -1    \n",
    "    true_labels = true_labels.flatten().astype('double'); pred_labels = pred_labels.flatten().astype('double')\n",
    "    true_labels_temp = true_labels.copy()\n",
    "    if 0 in true_labels: # replace zero with a very small positive value in order to avoid dividing by zero\n",
    "        print('There are elements of zero value in true labels.')\n",
    "        true_labels_temp[true_labels==0] = 1\n",
    "    \n",
    "    MFE = np.mean( np.abs( (true_labels - pred_labels)/true_labels_temp ) )\n",
    "    return MFE\n",
    "\n",
    "def calc_vlb(m,V, a, lik='Gaussian'):\n",
    "    prior_mean_u = a[0]; prior_mean_f = a[1] # prior mean for inducing points    \n",
    "    A = a[2] # Knm*inv(Kmm)\n",
    "    Kmm = a[3]; Kmm_inv = a[4]; Kmn = a[5]; Knn_diag = a[6]; y = a[7] # the ground truth for training data\n",
    "    noise_var = a[8]\n",
    "    num_train = len(prior_mean_f);num_inducing = len(prior_mean_u)\n",
    "    m_q = prior_mean_f + np.dot(A, (m-prior_mean_u)) # Eq.(3a) in paper\n",
    "    v_q = ( Knn_diag.ravel() + np.diag(np.dot(A, np.dot(V-Kmm, A.T))) )[:, None] # Eq.(3b) in paper\n",
    "#     v_q = np.absolute(v_q)\n",
    "    c1 = m - prior_mean_u; c2 = np.dot(Kmm_inv, c1);#print(v_q[:20],min(v_q))\n",
    "    (Sign,LogDetKmm) = np.linalg.slogdet(Kmm); LogDetKmm = Sign*LogDetKmm\n",
    "    (SignV,LogDetV) = np.linalg.slogdet(V); LogDetV = SignV*LogDetV;#print(v_q[:50])     \n",
    "    if lik=='Bernoulli':\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q));#vlb_lik = expit(y*f);print('yf',expit(y*f))\n",
    "        vlb_lik = np.sum( 1/_sqrt_2pi*np.dot( safe_ln(expit(y*f)) ,w) ) # sigmoid liklihood\n",
    "#         vlb_lik = np.sum( 1.0/np.sqrt(2*np.pi)*np.dot( np.log(std_norm_cdf(y*f)+1e-10) ,w) ) # Probit liklihood\n",
    "    elif lik=='Gaussian':\n",
    "        vlb_lik = -np.log(np.sqrt(2*np.pi*noise_var)) - np.sum((y-m_q)**2+v_q)/(2*noise_var)\n",
    "    elif lik=='Poisson':\n",
    "        vlb_lik = np.dot(y.T,m_q) - np.sum(loggamma(y+1)) - np.sum(np.exp(m_q+0.5*v_q))\n",
    "    elif lik=='Poisson2': # another link func: lambda=ln(1+e^f)\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q));\n",
    "        term1 = -np.sum(loggamma(y+1)); term2 = -np.sum( 1/_sqrt_2pi*np.dot( np.log(1+safe_exp(f)),w) )\n",
    "        term3 = np.sum( 1/_sqrt_2pi*np.dot( y*safe_ln(np.log(1+safe_exp(f))),w) )\n",
    "        vlb_lik = term1 + term2 + term3\n",
    "    vlb_kl = 0.5*( LogDetKmm - LogDetV + np.dot(c1.T, c2) + np.trace(np.dot(Kmm_inv,V)) - len(prior_mean_u) )\n",
    "    vlb = vlb_lik - vlb_kl\n",
    "    return vlb \n",
    "\n",
    "def get_init_hyperparameters(Z,Z_label,lik='Poisson',K='SE'):\n",
    "    # For all methods, initial variational parameters are found by running the Laplace approximation on the subset/active set.\n",
    "    dim_data = Z.shape[1]\n",
    "    if K=='SE': kern=GPy.kern.RBF(dim_data, variance=1.0, lengthscale=1.0);\n",
    "    elif K=='Matern32': kern=GPy.kern.Matern32(dim_data, variance=1.0, lengthscale=1.0);\n",
    "    elif K=='Matern52': kern=GPy.kern.Matern52(dim_data, variance=1.0, lengthscale=1.0);\n",
    "    \n",
    "    if lik=='Poisson': likelihood=GPy.likelihoods.Poisson();\n",
    "    elif lik=='Poisson2': likelihood=GPy.likelihoods.Poisson(GPy.likelihoods.link_functions.Log_ex_1());\n",
    "    elif lik=='Gaussian': likelihood=GPy.likelihoods.Gaussian();\n",
    "    elif lik=='Bernoulli': likelihood=GPy.likelihoods.Bernoulli();\n",
    "\n",
    "    laplace_inf = GPy.inference.latent_function_inference.Laplace()\n",
    "    model_lap = GPy.core.GP(X=Z, Y=Z_label, likelihood=likelihood, inference_method=laplace_inf, kernel=kern)\n",
    "    model_lap.optimize();\n",
    "    if K=='SE': length_scale=model_lap.rbf.lengthscale[0];signal_variance=model_lap.rbf.variance[0]\n",
    "    elif K=='Matern32': length_scale=model_lap.Mat32.lengthscale[0];signal_variance=model_lap.Mat32.variance[0]\n",
    "    elif K=='Matern52': length_scale=model_lap.Mat52.lengthscale[0];signal_variance=model_lap.Mat52.variance[0]\n",
    "    print(model_lap)\n",
    "    return model_lap,length_scale,signal_variance\n",
    "\n",
    "def Adam(theta, g_t, t, alpha=0.01, m_t=0, v_t=0, opt='minimize'):\n",
    "    beta_1 = 0.9; beta_2 = 0.999; epsilon = 1e-8     #initialize the values of the parameters    \n",
    "    m_t = beta_1*m_t + (1-beta_1)*g_t                #updates the moving averages of the gradient\n",
    "    v_t = beta_2*v_t + (1-beta_2)*(g_t*g_t)          #updates the moving averages of the squared gradient\n",
    "    m_cap = m_t/(1-(beta_1**t))                      #calculates the bias-corrected estimates\n",
    "    v_cap = v_t/(1-(beta_2**t))                      #calculates the bias-corrected estimates\n",
    "    if opt=='maximize':\n",
    "        theta = theta + (alpha*m_cap)/(np.sqrt(v_cap)+epsilon)    #updates the parameters\n",
    "    else:\n",
    "        theta = theta - (alpha*m_cap)/(np.sqrt(v_cap)+epsilon)    #updates the parameters\n",
    "    return theta, m_t, v_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GH_quad(mu_star,std_star): # gauss_hermite_quad to calculate numerical integration, w and z denote weights and sample points, respectively. \n",
    "    z,w = roots_hermitenorm(n=50, mu=False); z,w=z[:,None],w[:,None]\n",
    "    z = np.kron(std_star,z.T) + mu_star\n",
    "    return z,w\n",
    "\n",
    "def calc_err(X, xStar, yTrue, l, sigma2, m, V,Kmm,Kmm_inv,noise_var=0.1,lik='Poisson',K='SE'):\n",
    "    num_xStar = xStar.shape[0]; Kmn = kernel(X, xStar, l=l, sigma_f=np.sqrt(sigma2),K=K)\n",
    "    Knn_diag = kernel(xStar, l=l, sigma_f=np.sqrt(sigma2),K='diag')+noise_var*np.eye(num_xStar)#np.eye(num_xStar)*sigma2+noise_var*np.eye(num_xStar)\n",
    "    A = np.dot(Kmn.T, Kmm_inv);\n",
    "    mu_star = np.dot(A,m); v2_star = Knn_diag + np.dot(A, np.dot(V-Kmm,A.T))\n",
    "    std_star = np.sqrt(np.diag(v2_star))[:,None]\n",
    "    if lik=='Bernoulli':\n",
    "        p = calc_Bernoulli_pred(mu_star,std_star); res = np.where(p>=0.5,1,-1)\n",
    "        return np.sum( np.where(yTrue*res<0,1,0) )/num_xStar         \n",
    "    elif lik=='Gaussian':\n",
    "        mu, std = calc_Gauss_pred(mu_star,std_star,noise_var)\n",
    "        return 1/num_xStar*( np.sum( (yTrue-mu)**2 ) )        \n",
    "    else: # Poisson, Poisson2\n",
    "        y_min = min(yTrue); y_max = max(yTrue); \n",
    "        pmf = calc_Poisson_pred(mu_star,std_star,num_xStar,y_min,y_max,lik)\n",
    "        res = np.argmax(pmf, axis=1)+y_min; err = MFE(yTrue, res)\n",
    "        return err\n",
    "\n",
    "def calc_Gauss_pred(mu_star,std_star,noise_var):\n",
    "    return mu_star, std_star+np.sqrt(noise_var)\n",
    "\n",
    "def calc_Poisson_pred(mu_star,std_star,num_xStar,y_min,y_max,lik='Poisson'):\n",
    "    f,w = GH_quad(mu_star,std_star)\n",
    "    y_range = np.arange(y_min,y_max+1)\n",
    "    lik_func = {\n",
    "        'Poisson': lambda f,y: 1/gamma(y+1)*safe_exp(-safe_exp(f)+f*y),\n",
    "        'Poisson2': lambda f,y: 1/gamma(y+1)*expit(-f)*( safe_ln(1.0+safe_exp(f))**y )\n",
    "    }[lik]\n",
    "    poisson_lik = np.zeros((num_xStar,len(y_range)))\n",
    "    for i,y in enumerate(y_range):\n",
    "        poisson_lik[:,i] = 1/_sqrt_2pi*np.dot(lik_func(f,y),w).ravel()\n",
    "    return poisson_lik\n",
    "\n",
    "def calc_Bernoulli_pred(mu_star,std_star):\n",
    "#     v = std_star**2; kappa_v = (1+np.pi*v/8.0)**(-1/2) # Probit liklihood\n",
    "#     p = sigmoid(kappa_v*mu_star) # p(y=1|x_*,m,V)\n",
    "    f,w = GH_quad(mu_star,std_star) # sigmoid liklihood\n",
    "    p = 1/np.sqrt(2*np.pi)*np.dot( expit(f), w ) # p(y=1|x_*,m,V)\n",
    "    return p\n",
    "\n",
    "def calc_m_q(m, A, prior_mean_u, prior_mean_f):\n",
    "    return prior_mean_f + np.dot(A, (m-prior_mean_u))\n",
    "\n",
    "def calc_v_q(V, A, Kmm, Knn_diag):\n",
    "    return ( Knn_diag.ravel() + np.diag(np.dot(A, np.dot(V-Kmm, A.T))) )[:,None] # Eq.(3b) in paper\n",
    "\n",
    "def calc_rho(m_q, v_q, y, lik='Poisson', noise_var=0.1):\n",
    "    if lik=='Bernoulli':\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q))\n",
    "        return 1.0/_sqrt_2pi*np.dot(  y*expit(-y*f) , w ) # sigmoid liklihood\n",
    "#         return 1.0/_sqrt_2pi*np.dot( y*std_norm_pdf(f) / (std_norm_cdf(y*f)+1e-10), w ) # Probit liklihood        \n",
    "    elif lik=='Gaussian':\n",
    "        return 1.0/noise_var*(y-m_q)\n",
    "    elif lik=='Poisson':\n",
    "        return -np.exp(m_q + 0.5*v_q) + y\n",
    "    elif lik=='Poisson2':\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q))\n",
    "        y = np.tile(y,[1,f.shape[1]]); term2 = expit(f); t0 = safe_ln(1+safe_exp(f))# avoid dividing by zeros\n",
    "        d1_log_lik = np.zeros(t0.shape)\n",
    "        d1_log_lik[t0!=0] = (y[t0!=0]/t0[t0!=0] - 1)*term2[t0!=0]\n",
    "        return 1.0/_sqrt_2pi*np.dot(  d1_log_lik, w )\n",
    "    \n",
    "def calc_lambda(m_q, v_q, y, lik='Poisson', noise_var=0.1):\n",
    "    if lik=='Bernoulli':\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q))\n",
    "        return 1.0/np.sqrt(2*np.pi)*np.dot(  -expit(y*f)*expit(-y*f), w ) # sigmoid\n",
    "#         return 1.0/_sqrt_2pi*np.dot( -std_norm_pdf(f)**2 / (std_norm_cdf(y*f)**2+1e-10) - y*f*std_norm_pdf(f) / (std_norm_cdf(y*f)+1e-10), w )\n",
    "    elif lik=='Gaussian':\n",
    "        return -1.0/noise_var*np.ones((len(m_q),1))\n",
    "    elif lik=='Poisson':\n",
    "        return -np.exp(m_q + 0.5*v_q)\n",
    "    elif lik=='Poisson2':\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q));y = np.tile(y,[1,f.shape[1]]);\n",
    "        term2 = expit(f)*expit(f); t0 = safe_ln(1+safe_exp(f));d2_log_lik = np.zeros(t0.shape)\n",
    "        d2_log_lik[t0!=0] = ((y[t0!=0]/t0[t0!=0]-1)*safe_exp(-f[t0!=0])-y[t0!=0]/(t0[t0!=0])**2)*term2[t0!=0]\n",
    "        return 1.0/_sqrt_2pi*np.dot(  d2_log_lik, w )\n",
    "\n",
    "def dVLb_dm(m, rho, prior_mean_u, Kmm_inv, A):\n",
    "    dm = np.dot(A.T,rho) - np.dot(Kmm_inv, m-prior_mean_u) # Eq.(11a) in paper\n",
    "    return dm\n",
    "\n",
    "def dVLb_dL(L, lam, Kmm_inv, A, noise_var=1e-8):# optimizing the cholesky factor L guarantees the PSD of V automatically\n",
    "    dL = np.dot( np.dot( np.dot(A.T, np.diag(lam.ravel())), A ), L ) + inv(L+np.sqrt(noise_var)*np.eye(len(L))).T - np.dot(Kmm_inv, L)\n",
    "    return dL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(X,Y,ix,X_test=None,y_test=None,max_iter=100,lr=1*10**-5,FPb_cond=1*10**0,stop_cond=1*10**-3,VLB_opt='GD',lik='Poisson',K='SE'):\n",
    "    num_train = X.shape[0];num_inducing=len(ix);num_test=X_test.shape[0]; \n",
    "    prior_mean_u = np.zeros((num_inducing, 1)); prior_mean_f = np.zeros((num_train, 1))\n",
    "    Z = X[ix, :]; Z_label = Y[ix] # Z is the inducing set\n",
    "    model_lap,ls,sf2 = get_init_hyperparameters(Z,Z_label,lik=lik,K=K)#(Z_label+1)/2    \n",
    "    # variational parameters from initialization\n",
    "    f_mean, f_var = model_lap._raw_predict(X) ; #print(ls,sf2)\n",
    "#     ls = model_lap.rbf.lengthscale[0]; sf2 = model_lap.rbf.variance[0]; # length_scale,sigma_f2\n",
    "    if lik=='Gaussian': noise_var=model_lap.Gaussian_noise.variance[0] \n",
    "    else: noise_var=1*10**-8\n",
    "    #  we use variational mean from Laplace appr\n",
    "    m = f_mean[ix]; V = np.diag(f_var[ix].ravel()); L = cholesky(V);#print(min(f_var))\n",
    "    Kmm = kernel(Z, Z, l = ls, sigma_f = np.sqrt(sf2),K=K) + noise_var*np.eye(len(Z))\n",
    "    Kmm_inv = inv(Kmm); Kmn = kernel(Z, X, l = ls, sigma_f = np.sqrt(sf2),K=K); \n",
    "    Knn_diag =kernel(X, l = ls, sigma_f = np.sqrt(sf2),K='diag'); #sf2*np.ones((num_train, 1))\n",
    "    A = np.dot(Kmn.T, Kmm_inv) # Knm*inv(Kmm)    \n",
    "    a = (prior_mean_u,prior_mean_f,A,Kmm,Kmm_inv,Kmn,Knn_diag,Y,noise_var)\n",
    "    num_iter = 0; VLB = []; VLB_time = []; err = []; FPb_converge = False# True\n",
    "    var = np.hstack([m.flatten(), L.flatten()])\n",
    "    start_time = time.time(); VLB.append(-calc_vlb(m,V, a,lik)[0][0])\n",
    "    err.append(calc_err(Z,X_test,y_test,ls,sf2,m,V,Kmm,Kmm_inv,noise_var=noise_var,lik=lik,K=K))   \n",
    "    VLB_time.append(time.time()-start_time);print('Before iterations, VLB:{:.6f} err:{:.6f}'.format(VLB[-1],err[-1]) )\n",
    "    while 1:\n",
    "        num_iter = num_iter +1;#break\n",
    "        m_q = calc_m_q(m, A, prior_mean_u, prior_mean_f); v_q = calc_v_q(V, A, Kmm, Knn_diag);#print('m_q:',m_q[:10])\n",
    "        # According to Table 1 in paper, expectations of the derivatives wrt N(f|m,v) for Possion likelihood\n",
    "        rho = calc_rho(m_q, v_q, Y,lik, noise_var); lam = calc_lambda(m_q, v_q,Y,lik, noise_var);        \n",
    "        if VLB_opt=='GD':\n",
    "            dm = dVLb_dm(m, rho, prior_mean_u, Kmm_inv, A)\n",
    "            dL = dVLb_dL(L, lam, Kmm_inv, A, noise_var)\n",
    "            gradients = np.hstack([dm.flatten(), dL.flatten()])\n",
    "            var, m_t, v_t = Adam(var,gradients,num_iter,alpha=lr,opt='maximize')\n",
    "            m = var[:num_inducing][:,None] # variantional mean\n",
    "            L = var[num_inducing:].reshape(num_inducing,num_inducing) # variational variance V=L*L.T\n",
    "            V = np.dot(L, L.T);\n",
    "            \n",
    "        elif VLB_opt=='FPi':\n",
    "            dm = dVLb_dm(m, rho, prior_mean_u, Kmm_inv, A)\n",
    "            m, m_t, v_t = Adam(m,dm,num_iter,alpha=lr,opt='maximize') # print('old V',V[:1,:1])\n",
    "            V = inv(Kmm_inv-np.dot( np.dot(A.T, np.diag(lam.ravel())), A ));\n",
    "        elif VLB_opt=='FPb':            \n",
    "            if FPb_converge:\n",
    "                dm = dVLb_dm(m, rho, prior_mean_u, Kmm_inv, A); #print('old m:',m[:1]);\n",
    "                m, m_t, v_t = Adam(m,dm,num_iter,alpha=lr,opt='maximize')\n",
    "            else:\n",
    "                V = inv(Kmm_inv-np.dot( np.dot(A.T, np.diag(lam.ravel())), A))\n",
    "                \n",
    "        elif VLB_opt=='FPi-mean':\n",
    "            d = A.T; gamma = -lam; #print('old m:',m[:1]); print('old V:',V[:2,:2])             \n",
    "            V = inv(Kmm_inv-np.dot( np.dot(A.T, np.diag(lam.ravel())), A))\n",
    "            m = np.dot(V, np.dot(d, rho + np.dot(A,m)*gamma) );\n",
    "\n",
    "        VLB.append(-calc_vlb(m,V, a,lik)[0][0]); VLB_time.append(time.time()-start_time)\n",
    "        err.append(calc_err(Z,X_test,y_test,ls,sf2,m,V,Kmm,Kmm_inv,noise_var=noise_var,lik=lik,K=K))\n",
    "        if num_iter>0:\n",
    "            delta_vlb = abs(VLB[-1]-VLB[-2])\n",
    "            if num_iter%4 == 0:\n",
    "                print('iter:{}, delta_VLB:{:.6f}, VLB:{:.6f}, err:{:.6f}'.format(num_iter, VLB[-2]-VLB[-1], VLB[-1], err[-1]));\n",
    "            if VLB_opt=='FPb' and delta_vlb<=FPb_cond:\n",
    "                FPb_converge = not FPb_converge; print('m or V converged')\n",
    "            if num_iter>4 and delta_vlb<=stop_cond:\n",
    "                if np.average(VLB[-2:])<=np.average(VLB[-4:]):\n",
    "                    if num_iter>=max_iter: print('It has reached the maximum number of iterations.');break\n",
    "                    else:continue\n",
    "                else: print('After {} iterations it converged: delta_VLB:{:.6f}, VLB:{:.6f}, err:{:.6f}'.format(num_iter,delta_vlb,VLB[-1],err[-1]) );break                \n",
    "            if num_iter>8 and np.average(VLB[-4:])>=np.average(VLB[-8:]): print('negVLB did not decrease any more.'); break;            \n",
    "            elif num_iter==max_iter:\n",
    "                print('It has reached the maximum number of iterations, i.e. {}, with delta_VLB:{:.6f}, VLB:{:.6f} and err:{:.6f}'.format(max_iter,delta_vlb,VLB[-1],err[-1]));break            \n",
    "    return Z,Z_label, VLB, ls, sf2, m, V,noise_var, Kmm, Kmm_inv, A,VLB_time,err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name : gp\n",
      "Objective : 459.6938763251678\n",
      "Number of Parameters : 2\n",
      "Number of Optimization Parameters : 2\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1mgp.            \u001b[0;0m  |              value  |  constraints  |  priors\n",
      "  \u001b[1mrbf.variance   \u001b[0;0m  |  310.3404860882573  |      +ve      |        \n",
      "  \u001b[1mrbf.lengthscale\u001b[0;0m  |   50.8516387512666  |      +ve      |        \n",
      "Before iterations, VLB:176982.742789 err:0.091356\n",
      "iter:4, delta_VLB:0.132706, VLB:4446.605078, err:0.083233\n",
      "iter:8, delta_VLB:0.009952, VLB:4446.597426, err:0.083197\n",
      "iter:12, delta_VLB:0.027976, VLB:4446.552045, err:0.083197\n",
      "iter:16, delta_VLB:0.018300, VLB:4446.549424, err:0.083197\n",
      "iter:20, delta_VLB:0.022457, VLB:4446.549321, err:0.083197\n",
      "negVLB did not decrease any more.\n",
      "\n",
      "Name : gp\n",
      "Objective : 460.6225266678892\n",
      "Number of Parameters : 2\n",
      "Number of Optimization Parameters : 2\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1mgp.              \u001b[0;0m  |               value  |  constraints  |  priors\n",
      "  \u001b[1mMat32.variance   \u001b[0;0m  |    792.756853967755  |      +ve      |        \n",
      "  \u001b[1mMat32.lengthscale\u001b[0;0m  |  142.15070394098495  |      +ve      |        \n",
      "Before iterations, VLB:7671.871755 err:0.084476\n",
      "iter:4, delta_VLB:0.070204, VLB:4449.841825, err:0.083304\n",
      "iter:8, delta_VLB:0.053619, VLB:4449.609310, err:0.083103\n",
      "iter:12, delta_VLB:0.047497, VLB:4449.411785, err:0.082940\n",
      "iter:16, delta_VLB:0.044609, VLB:4449.229937, err:0.082715\n",
      "iter:20, delta_VLB:0.043179, VLB:4449.055453, err:0.082674\n",
      "iter:24, delta_VLB:0.042706, VLB:4448.884357, err:0.082719\n",
      "iter:28, delta_VLB:0.042749, VLB:4448.713047, err:0.082719\n",
      "iter:32, delta_VLB:0.042791, VLB:4448.541538, err:0.082687\n",
      "iter:36, delta_VLB:0.043291, VLB:4448.368756, err:0.082725\n",
      "iter:40, delta_VLB:0.043301, VLB:4448.195589, err:0.082770\n",
      "iter:44, delta_VLB:0.043633, VLB:4448.021672, err:0.082489\n",
      "iter:48, delta_VLB:0.044053, VLB:4447.846983, err:0.082453\n",
      "iter:52, delta_VLB:0.043330, VLB:4447.672694, err:0.082459\n",
      "iter:56, delta_VLB:0.043109, VLB:4447.498586, err:0.082323\n",
      "iter:60, delta_VLB:0.043336, VLB:4447.323234, err:0.082196\n",
      "iter:64, delta_VLB:0.043298, VLB:4447.148617, err:0.082051\n",
      "iter:68, delta_VLB:0.043469, VLB:4446.975233, err:0.082048\n",
      "iter:72, delta_VLB:0.042902, VLB:4446.803568, err:0.082077\n",
      "iter:76, delta_VLB:0.041898, VLB:4446.634036, err:0.081920\n",
      "iter:80, delta_VLB:0.041797, VLB:4446.465097, err:0.081684\n",
      "iter:84, delta_VLB:0.041359, VLB:4446.299503, err:0.081589\n",
      "iter:88, delta_VLB:0.040559, VLB:4446.135758, err:0.081363\n",
      "iter:92, delta_VLB:0.039756, VLB:4445.975187, err:0.081425\n",
      "iter:96, delta_VLB:0.039559, VLB:4445.816440, err:0.081392\n",
      "iter:100, delta_VLB:0.039495, VLB:4445.659408, err:0.081221\n",
      "iter:104, delta_VLB:0.038675, VLB:4445.505311, err:0.080864\n",
      "iter:108, delta_VLB:0.037216, VLB:4445.356098, err:0.080784\n",
      "iter:112, delta_VLB:0.035760, VLB:4445.208502, err:0.080660\n",
      "iter:116, delta_VLB:0.036224, VLB:4445.061283, err:0.080656\n",
      "iter:120, delta_VLB:0.034507, VLB:4444.918851, err:0.080869\n",
      "iter:124, delta_VLB:0.034248, VLB:4444.778861, err:0.080913\n",
      "iter:128, delta_VLB:0.033974, VLB:4444.642042, err:0.080666\n",
      "iter:132, delta_VLB:0.032372, VLB:4444.507167, err:0.080674\n",
      "iter:136, delta_VLB:0.032155, VLB:4444.375278, err:0.080777\n",
      "iter:140, delta_VLB:0.031056, VLB:4444.248144, err:0.080600\n",
      "iter:144, delta_VLB:0.030130, VLB:4444.122233, err:0.080521\n",
      "iter:148, delta_VLB:0.029621, VLB:4443.999844, err:0.080178\n",
      "iter:152, delta_VLB:0.028959, VLB:4443.883734, err:0.079924\n",
      "iter:156, delta_VLB:0.027926, VLB:4443.770221, err:0.079903\n",
      "iter:160, delta_VLB:0.028113, VLB:4443.658948, err:0.079809\n",
      "iter:164, delta_VLB:0.026574, VLB:4443.551587, err:0.079658\n",
      "iter:168, delta_VLB:0.023891, VLB:4443.450385, err:0.079651\n",
      "iter:172, delta_VLB:0.023378, VLB:4443.349067, err:0.079759\n",
      "iter:176, delta_VLB:0.022746, VLB:4443.253525, err:0.079601\n",
      "iter:180, delta_VLB:0.024660, VLB:4443.155353, err:0.079545\n",
      "iter:184, delta_VLB:0.020527, VLB:4443.064996, err:0.079308\n",
      "iter:188, delta_VLB:0.023464, VLB:4442.974007, err:0.079248\n",
      "iter:192, delta_VLB:0.020771, VLB:4442.889325, err:0.079177\n",
      "iter:196, delta_VLB:0.019077, VLB:4442.806376, err:0.079173\n",
      "iter:200, delta_VLB:0.019067, VLB:4442.725973, err:0.079136\n",
      "iter:204, delta_VLB:0.020952, VLB:4442.644276, err:0.079259\n",
      "iter:208, delta_VLB:0.017602, VLB:4442.573729, err:0.079250\n",
      "iter:212, delta_VLB:0.019924, VLB:4442.498938, err:0.079267\n",
      "iter:216, delta_VLB:0.020181, VLB:4442.426960, err:0.079186\n",
      "iter:220, delta_VLB:0.015872, VLB:4442.360389, err:0.079221\n",
      "iter:224, delta_VLB:0.016680, VLB:4442.294479, err:0.079070\n",
      "iter:228, delta_VLB:0.017939, VLB:4442.232119, err:0.078876\n",
      "iter:232, delta_VLB:0.015209, VLB:4442.168277, err:0.078950\n",
      "iter:236, delta_VLB:0.018039, VLB:4442.107726, err:0.078976\n",
      "iter:240, delta_VLB:0.014099, VLB:4442.053413, err:0.078893\n",
      "iter:244, delta_VLB:0.015583, VLB:4442.001543, err:0.078888\n",
      "iter:248, delta_VLB:0.016227, VLB:4441.944080, err:0.078813\n",
      "iter:252, delta_VLB:0.014567, VLB:4441.895012, err:0.078944\n",
      "iter:256, delta_VLB:0.013078, VLB:4441.848952, err:0.078864\n",
      "iter:260, delta_VLB:0.014321, VLB:4441.798655, err:0.078538\n",
      "iter:264, delta_VLB:0.010614, VLB:4441.755544, err:0.078363\n",
      "iter:268, delta_VLB:0.010108, VLB:4441.711342, err:0.078354\n",
      "iter:272, delta_VLB:0.010584, VLB:4441.671897, err:0.078250\n",
      "iter:276, delta_VLB:0.013579, VLB:4441.627242, err:0.078314\n",
      "iter:280, delta_VLB:0.013748, VLB:4441.584910, err:0.078031\n",
      "iter:284, delta_VLB:0.008925, VLB:4441.548872, err:0.078058\n",
      "iter:288, delta_VLB:0.006063, VLB:4441.513443, err:0.077973\n",
      "iter:292, delta_VLB:0.009559, VLB:4441.475070, err:0.078048\n",
      "iter:296, delta_VLB:0.006950, VLB:4441.441457, err:0.078087\n",
      "iter:300, delta_VLB:0.003050, VLB:4441.409617, err:0.078199\n",
      "It has reached the maximum number of iterations, i.e. 300, with delta_VLB:0.003050, VLB:4441.409617 and err:0.078199\n"
     ]
    }
   ],
   "source": [
    "FPim1 = Train(X_train,y_train,ix,X_test,y_test,max_iter=300,lr=1*10**-3,VLB_opt='FPi',lik='Poisson2',K='SE')#Gaussian,Bernoulli\n",
    "Zs,Zs_label, VLB1, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time1,err1=FPim1#Matern52,SE\n",
    "FPim2 = Train(X_train,y_train,ix,X_test,y_test,max_iter=300,lr=1*10**-3,VLB_opt='FPi',lik='Poisson2',K='Matern32')#Gaussian,Bernoulli\n",
    "Zs,Zs_label, VLB2, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time2,err2=FPim2#Matern52,SE\n",
    "# FPim3 = Train(X_train,y_train,ix,X_test,y_test,max_iter=300,lr=1*10**-2,VLB_opt='FPi',lik='Poisson2',K='Matern52')#Gaussian,Bernoulli\n",
    "# Zs,Zs_label, VLB3, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time3,err3=FPim3#Matern32,SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import matplotlib.pyplot as plt\n",
    "host = host_subplot(111)\n",
    "par = host.twinx()\n",
    "host.set_xlabel(\"Training time(log)\")\n",
    "host.set_ylabel(\"Error\")\n",
    "par.set_ylabel(\"negVLB\")\n",
    "p0, = host.plot(VLB_time1, err1, 'bo--', lw=1, markersize=5, fillstyle='none', label=\"error RBF\")\n",
    "p1, = host.plot(VLB_time2, err2, 'ro--', lw=1, markersize=5, fillstyle='none', label=\"error Matern32\")\n",
    "p2, = host.plot(VLB_time3, err3, 'go--', lw=1, markersize=5, fillstyle='none', label=\"error Matern52\")\n",
    "# p3, = host.plot(VLB_time_FPim, err_FPim, 'co--', markersize=5, lw=1, fillstyle='none', label=\"error FPi-mean\")\n",
    "p3, = par.plot(VLB_time1, VLB1, 'bo-', lw=1, markersize=5, label=\"negVLB RBF\")\n",
    "p4, = par.plot(VLB_time2, VLB2, 'ro-', lw=1, markersize=5, label=\"negVLB Matern32\")\n",
    "p5, = par.plot(VLB_time3, VLB3, 'go-', lw=1, markersize=5, label=\"negVLB Matern52\")\n",
    "# p7, = par.plot(VLB_time_FPim, VLB_FPim, 'c+-', lw=1, markersize=5, label=\"negVLB FPi-mean\")\n",
    "host.set_xscale('log',basex=10)\n",
    "leg = plt.legend()\n",
    "host.yaxis.get_label().set_color(p0.get_color());par.yaxis.get_label().set_color(p0.get_color())\n",
    "leg.texts[0].set_color(p0.get_color());\n",
    "leg.texts[1].set_color(p1.get_color())\n",
    "leg.texts[2].set_color(p2.get_color());\n",
    "# leg.texts[3].set_color(p3.get_color())\n",
    "leg.texts[3].set_color(p3.get_color());\n",
    "leg.texts[4].set_color(p4.get_color())\n",
    "leg.texts[5].set_color(p5.get_color());\n",
    "# leg.texts[7].set_color(p7.get_color())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data(count regression) count_dataset_ucsdpeds1l_N4000_D30,count_dataset_abalone_N4177_D9,count_dataset_flares_N1065_D24\n",
    "mat_contents = sio.loadmat('count_dataset_ucsdpeds1l_N4000_D30.mat')#count_dataset_epid_N6238_D17,count_dataset_bikehour2011_N8734_D49\n",
    "x = mat_contents['x']; y = mat_contents['y'] #count_dataset_segment_N2319_D19;class_dataset_madelon_N2600_D500;class_dataset_usps35_N1540_D256\n",
    "x = x.astype('double')#class_dataset_musk_N6598_D166;class_dataset_yeast_N1484_D8,reg_dataset_mg_N1385_D6\n",
    "#reg_dataset_cpusmall_N8192_D12,reg_dataset_spacega_N3107_D6\n",
    "# shuffle the data and split data into training set and test set\n",
    "data = np.concatenate((x,y), axis=1)\n",
    "np.random.shuffle(data)\n",
    "num_data = data.shape[0]; dim_data = data.shape[1] - 1;\n",
    "num_train = int(0.5*np.ceil(num_data)); num_test = num_data - num_train\n",
    "x_train = data[:num_train, :-1]; y_train = data[:num_train, -1];\n",
    "x_test = data[num_train:, :-1]; y_test = data[num_train:, -1];\n",
    "y_train = y_train[:, None]; y_test = y_test[:, None]\n",
    "# data Standardization with zero mean and unit variance\n",
    "scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "X_train = scaler.transform(x_train); X_test = scaler.transform(x_test)\n",
    "np.random.seed(1);num_inducing=200\n",
    "ix = random.sample(range(num_train), num_inducing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name : gp\n",
      "Objective : -126.24028967005158\n",
      "Number of Parameters : 3\n",
      "Number of Optimization Parameters : 3\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1mgp.                    \u001b[0;0m  |                 value  |  constraints  |  priors\n",
      "  \u001b[1mMat52.variance         \u001b[0;0m  |     19.59111811261873  |      +ve      |        \n",
      "  \u001b[1mMat52.lengthscale      \u001b[0;0m  |    23.465802504325442  |      +ve      |        \n",
      "  \u001b[1mGaussian_noise.variance\u001b[0;0m  |  0.012082894237187026  |      +ve      |        \n",
      "23.465802504325442 19.59111811261873\n",
      "Before iterations, VLB:14902.259485 err:0.017965\n",
      "iter:1, delta_VLB:-12.747364, VLB:14915.006850, err:0.017815\n",
      "iter:2, delta_VLB:32.731588, VLB:14882.275261, err:0.017707\n",
      "iter:3, delta_VLB:9.038333, VLB:14873.236928, err:0.017614\n",
      "iter:4, delta_VLB:10.921779, VLB:14862.315150, err:0.017532\n",
      "iter:5, delta_VLB:10.519986, VLB:14851.795164, err:0.017458\n",
      "iter:6, delta_VLB:13.035584, VLB:14838.759579, err:0.017388\n",
      "iter:7, delta_VLB:4.743044, VLB:14834.016536, err:0.017320\n",
      "iter:8, delta_VLB:11.415428, VLB:14822.601108, err:0.017254\n",
      "iter:9, delta_VLB:3.302951, VLB:14819.298157, err:0.017192\n",
      "iter:10, delta_VLB:10.204159, VLB:14809.093999, err:0.017130\n",
      "iter:11, delta_VLB:2.906437, VLB:14806.187562, err:0.017067\n",
      "iter:12, delta_VLB:9.247153, VLB:14796.940408, err:0.017008\n",
      "iter:13, delta_VLB:2.623037, VLB:14794.317371, err:0.016949\n",
      "iter:14, delta_VLB:9.005844, VLB:14785.311527, err:0.016891\n",
      "iter:15, delta_VLB:2.109088, VLB:14783.202440, err:0.016833\n",
      "iter:16, delta_VLB:8.744390, VLB:14774.458050, err:0.016778\n",
      "iter:17, delta_VLB:1.764901, VLB:14772.693149, err:0.016719\n",
      "iter:18, delta_VLB:8.575472, VLB:14764.117677, err:0.016663\n",
      "iter:19, delta_VLB:1.743808, VLB:14762.373869, err:0.016606\n",
      "iter:20, delta_VLB:8.185405, VLB:14754.188464, err:0.016552\n",
      "iter:21, delta_VLB:0.909915, VLB:14753.278549, err:0.016498\n",
      "iter:22, delta_VLB:8.384962, VLB:14744.893587, err:0.016444\n",
      "iter:23, delta_VLB:0.770243, VLB:14744.123344, err:0.016393\n",
      "iter:24, delta_VLB:7.820513, VLB:14736.302831, err:0.016339\n",
      "iter:25, delta_VLB:0.731753, VLB:14735.571078, err:0.016287\n",
      "iter:26, delta_VLB:7.937285, VLB:14727.633794, err:0.016234\n",
      "iter:27, delta_VLB:0.060625, VLB:14727.573168, err:0.016184\n",
      "iter:28, delta_VLB:7.723481, VLB:14719.849687, err:0.016137\n",
      "iter:29, delta_VLB:-0.216028, VLB:14720.065715, err:0.016087\n",
      "iter:30, delta_VLB:7.551250, VLB:14712.514465, err:0.016041\n",
      "iter:31, delta_VLB:-0.558853, VLB:14713.073318, err:0.015991\n",
      "iter:32, delta_VLB:7.583550, VLB:14705.489768, err:0.015946\n",
      "iter:33, delta_VLB:-1.052651, VLB:14706.542420, err:0.015894\n",
      "iter:34, delta_VLB:7.710595, VLB:14698.831825, err:0.015845\n",
      "iter:35, delta_VLB:-1.261322, VLB:14700.093147, err:0.015798\n",
      "iter:36, delta_VLB:7.821434, VLB:14692.271713, err:0.015752\n",
      "iter:37, delta_VLB:-1.001317, VLB:14693.273030, err:0.015701\n",
      "iter:38, delta_VLB:7.471544, VLB:14685.801486, err:0.015654\n",
      "iter:39, delta_VLB:-1.156522, VLB:14686.958008, err:0.015604\n",
      "iter:40, delta_VLB:7.518665, VLB:14679.439342, err:0.015555\n",
      "iter:41, delta_VLB:-1.130025, VLB:14680.569367, err:0.015502\n",
      "iter:42, delta_VLB:7.393921, VLB:14673.175446, err:0.015461\n",
      "iter:43, delta_VLB:-1.345819, VLB:14674.521265, err:0.015410\n",
      "iter:44, delta_VLB:7.389381, VLB:14667.131884, err:0.015364\n",
      "iter:45, delta_VLB:-1.724330, VLB:14668.856214, err:0.015318\n",
      "iter:46, delta_VLB:7.694238, VLB:14661.161976, err:0.015273\n",
      "iter:47, delta_VLB:-2.558999, VLB:14663.720975, err:0.015225\n",
      "iter:48, delta_VLB:8.425479, VLB:14655.295495, err:0.015184\n",
      "iter:49, delta_VLB:-2.927819, VLB:14658.223314, err:0.015129\n",
      "iter:50, delta_VLB:8.822540, VLB:14649.400773, err:0.015090\n",
      "iter:51, delta_VLB:-2.572104, VLB:14651.972877, err:0.015041\n",
      "iter:52, delta_VLB:8.436110, VLB:14643.536767, err:0.015000\n",
      "iter:53, delta_VLB:-2.350980, VLB:14645.887747, err:0.014954\n",
      "iter:54, delta_VLB:8.229409, VLB:14637.658339, err:0.014915\n",
      "iter:55, delta_VLB:-2.583401, VLB:14640.241740, err:0.014870\n",
      "iter:56, delta_VLB:8.331978, VLB:14631.909762, err:0.014832\n",
      "iter:57, delta_VLB:-3.003556, VLB:14634.913319, err:0.014789\n",
      "iter:58, delta_VLB:8.583425, VLB:14626.329893, err:0.014747\n",
      "iter:59, delta_VLB:-3.446396, VLB:14629.776290, err:0.014710\n",
      "iter:60, delta_VLB:8.822221, VLB:14620.954069, err:0.014663\n",
      "iter:61, delta_VLB:-4.036541, VLB:14624.990610, err:0.014629\n",
      "iter:62, delta_VLB:9.092113, VLB:14615.898497, err:0.014581\n",
      "iter:63, delta_VLB:-4.404679, VLB:14620.303176, err:0.014546\n",
      "iter:64, delta_VLB:9.297431, VLB:14611.005745, err:0.014505\n",
      "iter:65, delta_VLB:-4.713131, VLB:14615.718876, err:0.014467\n",
      "iter:66, delta_VLB:9.583989, VLB:14606.134886, err:0.014432\n",
      "iter:67, delta_VLB:-5.139477, VLB:14611.274363, err:0.014389\n",
      "iter:68, delta_VLB:9.679936, VLB:14601.594428, err:0.014354\n",
      "iter:69, delta_VLB:-5.479025, VLB:14607.073453, err:0.014312\n",
      "iter:70, delta_VLB:9.846216, VLB:14597.227237, err:0.014272\n",
      "iter:71, delta_VLB:-5.929745, VLB:14603.156981, err:0.014239\n",
      "iter:72, delta_VLB:10.108860, VLB:14593.048121, err:0.014205\n",
      "iter:73, delta_VLB:-6.473287, VLB:14599.521408, err:0.014160\n",
      "iter:74, delta_VLB:10.445589, VLB:14589.075819, err:0.014130\n",
      "iter:75, delta_VLB:-6.810623, VLB:14595.886442, err:0.014091\n",
      "iter:76, delta_VLB:10.589056, VLB:14585.297386, err:0.014070\n",
      "iter:77, delta_VLB:-7.181207, VLB:14592.478593, err:0.014030\n",
      "iter:78, delta_VLB:10.768362, VLB:14581.710231, err:0.013995\n",
      "iter:79, delta_VLB:-7.481008, VLB:14589.191238, err:0.013959\n",
      "iter:80, delta_VLB:10.865853, VLB:14578.325386, err:0.013924\n",
      "iter:81, delta_VLB:-7.856290, VLB:14586.181675, err:0.013891\n",
      "iter:82, delta_VLB:11.076692, VLB:14575.104984, err:0.013863\n",
      "iter:83, delta_VLB:-8.207451, VLB:14583.312435, err:0.013836\n",
      "iter:84, delta_VLB:11.249490, VLB:14572.062944, err:0.013798\n",
      "iter:85, delta_VLB:-8.478004, VLB:14580.540948, err:0.013776\n",
      "iter:86, delta_VLB:11.345855, VLB:14569.195094, err:0.013745\n",
      "iter:87, delta_VLB:-8.847678, VLB:14578.042771, err:0.013714\n",
      "iter:88, delta_VLB:11.582448, VLB:14566.460323, err:0.013683\n",
      "iter:89, delta_VLB:-9.122411, VLB:14575.582734, err:0.013664\n",
      "iter:90, delta_VLB:11.662032, VLB:14563.920703, err:0.013630\n",
      "iter:91, delta_VLB:-9.445548, VLB:14573.366251, err:0.013600\n",
      "iter:92, delta_VLB:11.883468, VLB:14561.482783, err:0.013579\n",
      "iter:93, delta_VLB:-9.674394, VLB:14571.157177, err:0.013554\n",
      "iter:94, delta_VLB:11.951346, VLB:14559.205830, err:0.013532\n",
      "iter:95, delta_VLB:-9.838064, VLB:14569.043894, err:0.013500\n",
      "iter:96, delta_VLB:12.005669, VLB:14557.038226, err:0.013475\n",
      "iter:97, delta_VLB:-9.992421, VLB:14567.030647, err:0.013468\n",
      "iter:98, delta_VLB:12.028431, VLB:14555.002216, err:0.013434\n",
      "iter:99, delta_VLB:-10.105381, VLB:14565.107597, err:0.013407\n",
      "iter:100, delta_VLB:12.045273, VLB:14553.062324, err:0.013384\n",
      "iter:101, delta_VLB:-10.396322, VLB:14563.458646, err:0.013374\n",
      "iter:102, delta_VLB:12.179577, VLB:14551.279070, err:0.013345\n",
      "iter:103, delta_VLB:-10.675836, VLB:14561.954905, err:0.013325\n",
      "iter:104, delta_VLB:12.373152, VLB:14549.581753, err:0.013296\n",
      "iter:105, delta_VLB:-10.942911, VLB:14560.524665, err:0.013286\n",
      "iter:106, delta_VLB:12.539294, VLB:14547.985370, err:0.013262\n",
      "iter:107, delta_VLB:-11.135498, VLB:14559.120868, err:0.013254\n",
      "iter:108, delta_VLB:12.608284, VLB:14546.512584, err:0.013221\n",
      "iter:109, delta_VLB:-11.198187, VLB:14557.710771, err:0.013208\n",
      "iter:110, delta_VLB:12.547965, VLB:14545.162806, err:0.013185\n",
      "iter:111, delta_VLB:-11.917143, VLB:14557.079949, err:0.013179\n",
      "iter:112, delta_VLB:13.255566, VLB:14543.824383, err:0.013147\n",
      "iter:113, delta_VLB:-12.819765, VLB:14556.644148, err:0.013146\n",
      "iter:114, delta_VLB:13.867433, VLB:14542.776715, err:0.013120\n",
      "iter:115, delta_VLB:-12.362180, VLB:14555.138895, err:0.013113\n",
      "iter:116, delta_VLB:13.432852, VLB:14541.706044, err:0.013084\n",
      "iter:117, delta_VLB:-11.981931, VLB:14553.687975, err:0.013070\n",
      "iter:118, delta_VLB:12.951951, VLB:14540.736024, err:0.013066\n",
      "iter:119, delta_VLB:-11.934569, VLB:14552.670593, err:0.013052\n",
      "iter:120, delta_VLB:12.747549, VLB:14539.923043, err:0.013030\n",
      "iter:121, delta_VLB:-11.793286, VLB:14551.716329, err:0.013036\n",
      "iter:122, delta_VLB:12.704800, VLB:14539.011529, err:0.013007\n",
      "iter:123, delta_VLB:-12.471349, VLB:14551.482878, err:0.012991\n",
      "iter:124, delta_VLB:13.299671, VLB:14538.183206, err:0.012979\n",
      "iter:125, delta_VLB:-13.484229, VLB:14551.667435, err:0.012974\n",
      "iter:126, delta_VLB:14.269782, VLB:14537.397653, err:0.012964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:127, delta_VLB:-15.270234, VLB:14552.667888, err:0.012943\n",
      "iter:128, delta_VLB:16.104231, VLB:14536.563657, err:0.012948\n",
      "iter:129, delta_VLB:-13.835735, VLB:14550.399392, err:0.012932\n",
      "iter:130, delta_VLB:14.716807, VLB:14535.682586, err:0.012915\n",
      "iter:131, delta_VLB:-13.912113, VLB:14549.594699, err:0.012920\n",
      "iter:132, delta_VLB:14.340829, VLB:14535.253870, err:0.012901\n",
      "iter:133, delta_VLB:-13.550701, VLB:14548.804571, err:0.012892\n",
      "iter:134, delta_VLB:14.577880, VLB:14534.226691, err:0.012891\n",
      "iter:135, delta_VLB:-14.104080, VLB:14548.330771, err:0.012874\n",
      "iter:136, delta_VLB:14.647924, VLB:14533.682847, err:0.012862\n",
      "iter:137, delta_VLB:-17.650785, VLB:14551.333632, err:0.012859\n",
      "iter:138, delta_VLB:18.665630, VLB:14532.668002, err:0.012850\n",
      "iter:139, delta_VLB:-61.007471, VLB:14593.675473, err:0.012836\n",
      "negVLB did not decrease any more.\n"
     ]
    }
   ],
   "source": [
    "GD = Train(X_train,y_train,ix,X_test,y_test,max_iter=400,lr=2*10**-3,VLB_opt='GD',lik='Gaussian',K='Matern52');\n",
    "Zs,Zs_label, VLB, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time,err = GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name : gp\n",
      "Objective : -139.7463246047188\n",
      "Number of Parameters : 3\n",
      "Number of Optimization Parameters : 3\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1mgp.                    \u001b[0;0m  |                value  |  constraints  |  priors\n",
      "  \u001b[1mrbf.variance           \u001b[0;0m  |   5.1020070037083425  |      +ve      |        \n",
      "  \u001b[1mrbf.lengthscale        \u001b[0;0m  |   7.3702655490680025  |      +ve      |        \n",
      "  \u001b[1mGaussian_noise.variance\u001b[0;0m  |  0.00979866354836197  |      +ve      |        \n",
      "Before iterations, VLB:1439.518614 err:0.015196\n",
      "iter:1, delta_VLB:176.156990, VLB:1263.361624, err:0.015196\n",
      "iter:2, delta_VLB:0.000000, VLB:1263.361624, err:0.015196\n",
      "m or V converged\n",
      "iter:3, delta_VLB:5.717626, VLB:1257.643998, err:0.015110\n",
      "iter:4, delta_VLB:5.045001, VLB:1252.598997, err:0.015034\n",
      "iter:5, delta_VLB:4.601513, VLB:1247.997484, err:0.014963\n",
      "iter:6, delta_VLB:4.284092, VLB:1243.713391, err:0.014896\n",
      "iter:7, delta_VLB:4.043024, VLB:1239.670367, err:0.014833\n",
      "iter:8, delta_VLB:3.851237, VLB:1235.819130, err:0.014772\n",
      "iter:9, delta_VLB:3.694573, VLB:1232.124557, err:0.014712\n",
      "iter:10, delta_VLB:3.561314, VLB:1228.563243, err:0.014654\n",
      "iter:11, delta_VLB:3.445146, VLB:1225.118097, err:0.014597\n",
      "iter:12, delta_VLB:3.341016, VLB:1221.777081, err:0.014541\n",
      "iter:13, delta_VLB:3.246358, VLB:1218.530723, err:0.014488\n",
      "iter:14, delta_VLB:3.159869, VLB:1215.370855, err:0.014434\n",
      "iter:15, delta_VLB:3.078714, VLB:1212.292141, err:0.014381\n",
      "iter:16, delta_VLB:3.001758, VLB:1209.290382, err:0.014330\n",
      "iter:17, delta_VLB:2.929523, VLB:1206.360859, err:0.014280\n",
      "iter:18, delta_VLB:2.861286, VLB:1203.499573, err:0.014230\n",
      "iter:19, delta_VLB:2.792419, VLB:1200.707154, err:0.014182\n",
      "iter:20, delta_VLB:2.723316, VLB:1197.983838, err:0.014134\n",
      "iter:21, delta_VLB:2.653338, VLB:1195.330500, err:0.014087\n",
      "iter:22, delta_VLB:2.582559, VLB:1192.747941, err:0.014041\n",
      "iter:23, delta_VLB:2.512478, VLB:1190.235462, err:0.013996\n",
      "iter:24, delta_VLB:2.444836, VLB:1187.790627, err:0.013952\n",
      "iter:25, delta_VLB:2.376606, VLB:1185.414021, err:0.013907\n",
      "iter:26, delta_VLB:2.310664, VLB:1183.103357, err:0.013865\n",
      "iter:27, delta_VLB:2.242532, VLB:1180.860825, err:0.013823\n",
      "iter:28, delta_VLB:2.174528, VLB:1178.686298, err:0.013781\n",
      "iter:29, delta_VLB:2.105009, VLB:1176.581289, err:0.013744\n",
      "iter:30, delta_VLB:2.036591, VLB:1174.544697, err:0.013702\n",
      "iter:31, delta_VLB:1.965095, VLB:1172.579602, err:0.013669\n",
      "iter:32, delta_VLB:1.893181, VLB:1170.686421, err:0.013631\n",
      "iter:33, delta_VLB:1.819273, VLB:1168.867148, err:0.013599\n",
      "iter:34, delta_VLB:1.749916, VLB:1167.117232, err:0.013566\n",
      "iter:35, delta_VLB:1.678343, VLB:1165.438889, err:0.013534\n",
      "iter:36, delta_VLB:1.604228, VLB:1163.834662, err:0.013507\n",
      "iter:37, delta_VLB:1.536513, VLB:1162.298148, err:0.013480\n",
      "iter:38, delta_VLB:1.466643, VLB:1160.831506, err:0.013450\n",
      "iter:39, delta_VLB:1.391462, VLB:1159.440044, err:0.013426\n",
      "iter:40, delta_VLB:1.316157, VLB:1158.123887, err:0.013396\n",
      "iter:41, delta_VLB:1.247159, VLB:1156.876728, err:0.013375\n",
      "iter:42, delta_VLB:1.174954, VLB:1155.701774, err:0.013350\n",
      "iter:43, delta_VLB:1.105123, VLB:1154.596651, err:0.013335\n",
      "iter:44, delta_VLB:1.036902, VLB:1153.559750, err:0.013309\n",
      "iter:45, delta_VLB:0.975605, VLB:1152.584145, err:0.013297\n",
      "m or V converged\n",
      "iter:46, delta_VLB:0.000000, VLB:1152.584145, err:0.013297\n",
      "m or V converged\n",
      "iter:47, delta_VLB:0.917964, VLB:1151.666181, err:0.013271\n",
      "m or V converged\n",
      "iter:48, delta_VLB:0.000000, VLB:1151.666181, err:0.013271\n",
      "m or V converged\n",
      "iter:49, delta_VLB:0.865039, VLB:1150.801141, err:0.013265\n",
      "m or V converged\n",
      "iter:50, delta_VLB:0.000000, VLB:1150.801141, err:0.013265\n",
      "m or V converged\n",
      "iter:51, delta_VLB:0.819230, VLB:1149.981912, err:0.013242\n",
      "m or V converged\n",
      "iter:52, delta_VLB:0.000000, VLB:1149.981912, err:0.013242\n",
      "m or V converged\n",
      "iter:53, delta_VLB:0.779785, VLB:1149.202127, err:0.013234\n",
      "m or V converged\n",
      "iter:54, delta_VLB:0.000000, VLB:1149.202127, err:0.013234\n",
      "m or V converged\n",
      "iter:55, delta_VLB:0.725127, VLB:1148.477000, err:0.013222\n",
      "m or V converged\n",
      "iter:56, delta_VLB:0.000000, VLB:1148.477000, err:0.013222\n",
      "m or V converged\n",
      "iter:57, delta_VLB:0.675235, VLB:1147.801766, err:0.013215\n",
      "m or V converged\n",
      "iter:58, delta_VLB:0.000000, VLB:1147.801766, err:0.013215\n",
      "m or V converged\n",
      "iter:59, delta_VLB:0.630843, VLB:1147.170922, err:0.013197\n",
      "m or V converged\n",
      "iter:60, delta_VLB:0.000000, VLB:1147.170922, err:0.013197\n",
      "m or V converged\n",
      "iter:61, delta_VLB:0.586170, VLB:1146.584753, err:0.013201\n",
      "m or V converged\n",
      "iter:62, delta_VLB:0.000000, VLB:1146.584753, err:0.013201\n",
      "m or V converged\n",
      "iter:63, delta_VLB:0.558852, VLB:1146.025901, err:0.013181\n",
      "m or V converged\n",
      "iter:64, delta_VLB:0.000000, VLB:1146.025901, err:0.013181\n",
      "m or V converged\n",
      "iter:65, delta_VLB:0.536865, VLB:1145.489036, err:0.013179\n",
      "m or V converged\n",
      "iter:66, delta_VLB:0.000000, VLB:1145.489036, err:0.013179\n",
      "m or V converged\n",
      "iter:67, delta_VLB:0.499443, VLB:1144.989593, err:0.013165\n",
      "m or V converged\n",
      "iter:68, delta_VLB:0.000000, VLB:1144.989593, err:0.013165\n",
      "m or V converged\n",
      "iter:69, delta_VLB:0.460775, VLB:1144.528818, err:0.013162\n",
      "m or V converged\n",
      "iter:70, delta_VLB:0.000000, VLB:1144.528818, err:0.013162\n",
      "m or V converged\n",
      "iter:71, delta_VLB:0.444827, VLB:1144.083991, err:0.013144\n",
      "m or V converged\n",
      "iter:72, delta_VLB:0.000000, VLB:1144.083991, err:0.013144\n",
      "m or V converged\n",
      "iter:73, delta_VLB:0.411910, VLB:1143.672081, err:0.013150\n",
      "m or V converged\n",
      "iter:74, delta_VLB:0.000000, VLB:1143.672081, err:0.013150\n",
      "m or V converged\n",
      "iter:75, delta_VLB:0.395115, VLB:1143.276966, err:0.013129\n",
      "m or V converged\n",
      "iter:76, delta_VLB:0.000000, VLB:1143.276966, err:0.013129\n",
      "m or V converged\n",
      "iter:77, delta_VLB:0.359107, VLB:1142.917859, err:0.013131\n",
      "m or V converged\n",
      "iter:78, delta_VLB:0.000000, VLB:1142.917859, err:0.013131\n",
      "m or V converged\n",
      "iter:79, delta_VLB:0.344944, VLB:1142.572915, err:0.013118\n",
      "m or V converged\n",
      "iter:80, delta_VLB:0.000000, VLB:1142.572915, err:0.013118\n",
      "m or V converged\n",
      "iter:81, delta_VLB:0.309223, VLB:1142.263692, err:0.013114\n",
      "m or V converged\n",
      "iter:82, delta_VLB:0.000000, VLB:1142.263692, err:0.013114\n",
      "m or V converged\n",
      "iter:83, delta_VLB:0.307304, VLB:1141.956388, err:0.013114\n",
      "m or V converged\n",
      "iter:84, delta_VLB:0.000000, VLB:1141.956388, err:0.013114\n",
      "m or V converged\n",
      "iter:85, delta_VLB:0.285461, VLB:1141.670927, err:0.013102\n",
      "m or V converged\n",
      "iter:86, delta_VLB:0.000000, VLB:1141.670927, err:0.013102\n",
      "m or V converged\n",
      "iter:87, delta_VLB:0.247714, VLB:1141.423214, err:0.013115\n",
      "m or V converged\n",
      "iter:88, delta_VLB:0.000000, VLB:1141.423214, err:0.013115\n",
      "m or V converged\n",
      "iter:89, delta_VLB:0.286938, VLB:1141.136276, err:0.013100\n",
      "m or V converged\n",
      "iter:90, delta_VLB:0.000000, VLB:1141.136276, err:0.013100\n",
      "m or V converged\n",
      "iter:91, delta_VLB:0.211647, VLB:1140.924628, err:0.013103\n",
      "m or V converged\n",
      "iter:92, delta_VLB:0.000000, VLB:1140.924628, err:0.013103\n",
      "m or V converged\n",
      "iter:93, delta_VLB:0.225020, VLB:1140.699608, err:0.013092\n",
      "m or V converged\n",
      "iter:94, delta_VLB:0.000000, VLB:1140.699608, err:0.013092\n",
      "m or V converged\n",
      "iter:95, delta_VLB:0.178839, VLB:1140.520769, err:0.013106\n",
      "m or V converged\n",
      "iter:96, delta_VLB:0.000000, VLB:1140.520769, err:0.013106\n",
      "m or V converged\n",
      "iter:97, delta_VLB:0.211733, VLB:1140.309036, err:0.013092\n",
      "m or V converged\n",
      "iter:98, delta_VLB:0.000000, VLB:1140.309036, err:0.013092\n",
      "m or V converged\n",
      "iter:99, delta_VLB:0.156836, VLB:1140.152200, err:0.013097\n",
      "m or V converged\n",
      "iter:100, delta_VLB:0.000000, VLB:1140.152200, err:0.013097\n",
      "m or V converged\n",
      "iter:101, delta_VLB:0.224335, VLB:1139.927865, err:0.013089\n",
      "m or V converged\n",
      "iter:102, delta_VLB:0.000000, VLB:1139.927865, err:0.013089\n",
      "m or V converged\n",
      "iter:103, delta_VLB:0.122527, VLB:1139.805339, err:0.013086\n",
      "m or V converged\n",
      "iter:104, delta_VLB:0.000000, VLB:1139.805339, err:0.013086\n",
      "m or V converged\n",
      "iter:105, delta_VLB:0.175765, VLB:1139.629574, err:0.013081\n",
      "m or V converged\n",
      "iter:106, delta_VLB:0.000000, VLB:1139.629574, err:0.013081\n",
      "m or V converged\n",
      "iter:107, delta_VLB:0.071469, VLB:1139.558105, err:0.013094\n",
      "m or V converged\n",
      "iter:108, delta_VLB:0.000000, VLB:1139.558105, err:0.013094\n",
      "m or V converged\n",
      "iter:109, delta_VLB:0.131273, VLB:1139.426832, err:0.013081\n",
      "m or V converged\n",
      "iter:110, delta_VLB:0.000000, VLB:1139.426832, err:0.013081\n",
      "m or V converged\n",
      "iter:111, delta_VLB:0.104890, VLB:1139.321943, err:0.013087\n",
      "m or V converged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:112, delta_VLB:0.000000, VLB:1139.321943, err:0.013087\n",
      "m or V converged\n",
      "iter:113, delta_VLB:0.125783, VLB:1139.196159, err:0.013072\n",
      "m or V converged\n",
      "iter:114, delta_VLB:0.000000, VLB:1139.196159, err:0.013072\n",
      "m or V converged\n",
      "iter:115, delta_VLB:0.055158, VLB:1139.141001, err:0.013081\n",
      "m or V converged\n",
      "iter:116, delta_VLB:0.000000, VLB:1139.141001, err:0.013081\n",
      "m or V converged\n",
      "iter:117, delta_VLB:0.113090, VLB:1139.027911, err:0.013076\n",
      "m or V converged\n",
      "iter:118, delta_VLB:0.000000, VLB:1139.027911, err:0.013076\n",
      "m or V converged\n",
      "iter:119, delta_VLB:0.049791, VLB:1138.978121, err:0.013075\n",
      "m or V converged\n",
      "iter:120, delta_VLB:0.000000, VLB:1138.978121, err:0.013075\n",
      "m or V converged\n",
      "iter:121, delta_VLB:0.071896, VLB:1138.906224, err:0.013067\n",
      "m or V converged\n",
      "iter:122, delta_VLB:0.000000, VLB:1138.906224, err:0.013067\n",
      "m or V converged\n",
      "iter:123, delta_VLB:0.027046, VLB:1138.879178, err:0.013076\n",
      "m or V converged\n",
      "iter:124, delta_VLB:0.000000, VLB:1138.879178, err:0.013076\n",
      "m or V converged\n",
      "iter:125, delta_VLB:0.083159, VLB:1138.796019, err:0.013066\n",
      "m or V converged\n",
      "iter:126, delta_VLB:0.000000, VLB:1138.796019, err:0.013066\n",
      "m or V converged\n",
      "iter:127, delta_VLB:0.055384, VLB:1138.740634, err:0.013072\n",
      "m or V converged\n",
      "iter:128, delta_VLB:0.000000, VLB:1138.740634, err:0.013072\n",
      "m or V converged\n",
      "iter:129, delta_VLB:0.027635, VLB:1138.712999, err:0.013056\n",
      "m or V converged\n",
      "iter:130, delta_VLB:0.000000, VLB:1138.712999, err:0.013056\n",
      "m or V converged\n",
      "iter:131, delta_VLB:0.063064, VLB:1138.649935, err:0.013068\n",
      "m or V converged\n",
      "iter:132, delta_VLB:0.000000, VLB:1138.649935, err:0.013068\n",
      "m or V converged\n",
      "iter:133, delta_VLB:0.052337, VLB:1138.597599, err:0.013056\n",
      "m or V converged\n",
      "iter:134, delta_VLB:0.000000, VLB:1138.597599, err:0.013056\n",
      "m or V converged\n",
      "iter:135, delta_VLB:0.001422, VLB:1138.596176, err:0.013071\n",
      "m or V converged\n",
      "iter:136, delta_VLB:0.000000, VLB:1138.596176, err:0.013071\n",
      "m or V converged\n",
      "iter:137, delta_VLB:0.079235, VLB:1138.516941, err:0.013056\n",
      "m or V converged\n",
      "iter:138, delta_VLB:0.000000, VLB:1138.516941, err:0.013056\n",
      "m or V converged\n",
      "iter:139, delta_VLB:-0.018537, VLB:1138.535478, err:0.013061\n",
      "m or V converged\n",
      "iter:140, delta_VLB:0.000000, VLB:1138.535478, err:0.013061\n",
      "m or V converged\n",
      "After 140 iterations it converged: delta_VLB:0.000000, VLB:1138.535478, err:0.013061\n"
     ]
    }
   ],
   "source": [
    "FPb = Train(X_train,y_train,ix,X_test,y_test,max_iter=400,lr=2*10**-3,VLB_opt='FPb',lik='Gaussian')#Gaussian,Bernoulli\n",
    "Zs,Zs_label, VLB_FPb, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time_FPb,err_FPb = FPb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name : gp\n",
      "Objective : -139.7463246047188\n",
      "Number of Parameters : 3\n",
      "Number of Optimization Parameters : 3\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1mgp.                    \u001b[0;0m  |                value  |  constraints  |  priors\n",
      "  \u001b[1mrbf.variance           \u001b[0;0m  |   5.1020070037083425  |      +ve      |        \n",
      "  \u001b[1mrbf.lengthscale        \u001b[0;0m  |   7.3702655490680025  |      +ve      |        \n",
      "  \u001b[1mGaussian_noise.variance\u001b[0;0m  |  0.00979866354836197  |      +ve      |        \n",
      "Before iterations, VLB:1439.518614 err:0.015196\n",
      "iter:1, delta_VLB:185.027110, VLB:1254.491504, err:0.015062\n",
      "iter:2, delta_VLB:6.314797, VLB:1248.176707, err:0.014967\n",
      "iter:3, delta_VLB:5.230898, VLB:1242.945809, err:0.014886\n",
      "iter:4, delta_VLB:4.606285, VLB:1238.339524, err:0.014813\n",
      "iter:5, delta_VLB:4.192426, VLB:1234.147097, err:0.014745\n",
      "iter:6, delta_VLB:3.894105, VLB:1230.252992, err:0.014682\n",
      "iter:7, delta_VLB:3.665560, VLB:1226.587433, err:0.014621\n",
      "iter:8, delta_VLB:3.482860, VLB:1223.104572, err:0.014564\n",
      "iter:9, delta_VLB:3.330855, VLB:1219.773717, err:0.014509\n",
      "iter:10, delta_VLB:3.202386, VLB:1216.571331, err:0.014456\n",
      "iter:11, delta_VLB:3.091552, VLB:1213.479779, err:0.014403\n",
      "iter:12, delta_VLB:2.992336, VLB:1210.487443, err:0.014352\n",
      "iter:13, delta_VLB:2.903497, VLB:1207.583946, err:0.014302\n",
      "iter:14, delta_VLB:2.823444, VLB:1204.760502, err:0.014253\n",
      "iter:15, delta_VLB:2.748344, VLB:1202.012158, err:0.014206\n",
      "iter:16, delta_VLB:2.675437, VLB:1199.336721, err:0.014159\n",
      "iter:17, delta_VLB:2.603935, VLB:1196.732786, err:0.014114\n",
      "iter:18, delta_VLB:2.534563, VLB:1194.198223, err:0.014069\n",
      "iter:19, delta_VLB:2.465261, VLB:1191.732962, err:0.014025\n",
      "iter:20, delta_VLB:2.398304, VLB:1189.334658, err:0.013980\n",
      "iter:21, delta_VLB:2.334914, VLB:1186.999743, err:0.013938\n",
      "iter:22, delta_VLB:2.270981, VLB:1184.728762, err:0.013895\n",
      "iter:23, delta_VLB:2.209254, VLB:1182.519509, err:0.013855\n",
      "iter:24, delta_VLB:2.145361, VLB:1180.374148, err:0.013815\n",
      "iter:25, delta_VLB:2.082939, VLB:1178.291209, err:0.013774\n",
      "iter:26, delta_VLB:2.018418, VLB:1176.272791, err:0.013737\n",
      "iter:27, delta_VLB:1.955608, VLB:1174.317183, err:0.013702\n",
      "iter:28, delta_VLB:1.889021, VLB:1172.428163, err:0.013664\n",
      "iter:29, delta_VLB:1.823384, VLB:1170.604778, err:0.013632\n",
      "iter:30, delta_VLB:1.757914, VLB:1168.846865, err:0.013600\n",
      "iter:31, delta_VLB:1.691861, VLB:1167.155004, err:0.013564\n",
      "iter:32, delta_VLB:1.625943, VLB:1165.529060, err:0.013540\n",
      "iter:33, delta_VLB:1.560659, VLB:1163.968401, err:0.013508\n",
      "iter:34, delta_VLB:1.494020, VLB:1162.474381, err:0.013482\n",
      "iter:35, delta_VLB:1.428120, VLB:1161.046260, err:0.013456\n",
      "iter:36, delta_VLB:1.363856, VLB:1159.682404, err:0.013429\n",
      "iter:37, delta_VLB:1.290585, VLB:1158.391819, err:0.013403\n",
      "iter:38, delta_VLB:1.226386, VLB:1157.165433, err:0.013382\n",
      "iter:39, delta_VLB:1.160179, VLB:1156.005254, err:0.013359\n",
      "iter:40, delta_VLB:1.093830, VLB:1154.911424, err:0.013338\n",
      "iter:41, delta_VLB:1.024487, VLB:1153.886937, err:0.013317\n",
      "iter:42, delta_VLB:0.967794, VLB:1152.919142, err:0.013301\n",
      "iter:43, delta_VLB:0.913009, VLB:1152.006133, err:0.013284\n",
      "iter:44, delta_VLB:0.857632, VLB:1151.148502, err:0.013267\n",
      "iter:45, delta_VLB:0.801385, VLB:1150.347116, err:0.013255\n",
      "iter:46, delta_VLB:0.747848, VLB:1149.599269, err:0.013241\n",
      "iter:47, delta_VLB:0.702805, VLB:1148.896463, err:0.013230\n",
      "iter:48, delta_VLB:0.654809, VLB:1148.241655, err:0.013221\n",
      "iter:49, delta_VLB:0.613557, VLB:1147.628098, err:0.013211\n",
      "iter:50, delta_VLB:0.579421, VLB:1147.048677, err:0.013207\n",
      "iter:51, delta_VLB:0.533563, VLB:1146.515114, err:0.013193\n",
      "iter:52, delta_VLB:0.518418, VLB:1145.996696, err:0.013184\n",
      "iter:53, delta_VLB:0.469868, VLB:1145.526828, err:0.013178\n",
      "iter:54, delta_VLB:0.449826, VLB:1145.077002, err:0.013170\n",
      "iter:55, delta_VLB:0.417255, VLB:1144.659746, err:0.013156\n",
      "iter:56, delta_VLB:0.416656, VLB:1144.243090, err:0.013154\n",
      "iter:57, delta_VLB:0.378056, VLB:1143.865034, err:0.013140\n",
      "iter:58, delta_VLB:0.364498, VLB:1143.500535, err:0.013141\n",
      "iter:59, delta_VLB:0.330219, VLB:1143.170316, err:0.013131\n",
      "iter:60, delta_VLB:0.333788, VLB:1142.836528, err:0.013130\n",
      "iter:61, delta_VLB:0.290438, VLB:1142.546090, err:0.013122\n",
      "iter:62, delta_VLB:0.280950, VLB:1142.265140, err:0.013117\n",
      "iter:63, delta_VLB:0.263151, VLB:1142.001989, err:0.013112\n",
      "iter:64, delta_VLB:0.266029, VLB:1141.735960, err:0.013109\n",
      "iter:65, delta_VLB:0.226317, VLB:1141.509643, err:0.013109\n",
      "iter:66, delta_VLB:0.227458, VLB:1141.282184, err:0.013100\n",
      "iter:67, delta_VLB:0.202545, VLB:1141.079639, err:0.013107\n",
      "iter:68, delta_VLB:0.214038, VLB:1140.865602, err:0.013092\n",
      "iter:69, delta_VLB:0.179833, VLB:1140.685769, err:0.013093\n",
      "iter:70, delta_VLB:0.197806, VLB:1140.487963, err:0.013099\n",
      "iter:71, delta_VLB:0.170605, VLB:1140.317358, err:0.013091\n",
      "iter:72, delta_VLB:0.165552, VLB:1140.151806, err:0.013101\n",
      "iter:73, delta_VLB:0.137334, VLB:1140.014472, err:0.013098\n",
      "iter:74, delta_VLB:0.137166, VLB:1139.877306, err:0.013087\n",
      "iter:75, delta_VLB:0.143626, VLB:1139.733680, err:0.013098\n",
      "iter:76, delta_VLB:0.121108, VLB:1139.612572, err:0.013090\n",
      "iter:77, delta_VLB:0.106278, VLB:1139.506294, err:0.013090\n",
      "iter:78, delta_VLB:0.102352, VLB:1139.403942, err:0.013082\n",
      "iter:79, delta_VLB:0.094553, VLB:1139.309389, err:0.013088\n",
      "iter:80, delta_VLB:0.083125, VLB:1139.226263, err:0.013078\n",
      "iter:81, delta_VLB:0.066423, VLB:1139.159840, err:0.013086\n",
      "iter:82, delta_VLB:0.095859, VLB:1139.063981, err:0.013082\n",
      "iter:83, delta_VLB:0.078023, VLB:1138.985958, err:0.013074\n",
      "iter:84, delta_VLB:0.048944, VLB:1138.937015, err:0.013083\n",
      "iter:85, delta_VLB:0.045370, VLB:1138.891645, err:0.013083\n",
      "iter:86, delta_VLB:0.065380, VLB:1138.826264, err:0.013079\n",
      "iter:87, delta_VLB:0.028975, VLB:1138.797290, err:0.013084\n",
      "iter:88, delta_VLB:0.076665, VLB:1138.720625, err:0.013073\n",
      "iter:89, delta_VLB:0.004911, VLB:1138.715714, err:0.013085\n",
      "iter:90, delta_VLB:0.070761, VLB:1138.644952, err:0.013067\n",
      "iter:91, delta_VLB:-0.012262, VLB:1138.657214, err:0.013084\n",
      "iter:92, delta_VLB:0.071387, VLB:1138.585827, err:0.013064\n",
      "iter:93, delta_VLB:0.006713, VLB:1138.579114, err:0.013078\n",
      "iter:94, delta_VLB:0.043701, VLB:1138.535413, err:0.013062\n",
      "iter:95, delta_VLB:0.014352, VLB:1138.521061, err:0.013073\n",
      "iter:96, delta_VLB:0.044853, VLB:1138.476208, err:0.013063\n",
      "iter:97, delta_VLB:-0.001139, VLB:1138.477347, err:0.013074\n",
      "iter:98, delta_VLB:0.055949, VLB:1138.421398, err:0.013056\n",
      "iter:99, delta_VLB:-0.000236, VLB:1138.421634, err:0.013066\n",
      "iter:100, delta_VLB:0.027703, VLB:1138.393931, err:0.013053\n",
      "iter:101, delta_VLB:-0.000096, VLB:1138.394027, err:0.013066\n",
      "iter:102, delta_VLB:0.026691, VLB:1138.367336, err:0.013058\n",
      "iter:103, delta_VLB:0.004291, VLB:1138.363045, err:0.013069\n",
      "iter:104, delta_VLB:0.024251, VLB:1138.338794, err:0.013058\n",
      "iter:105, delta_VLB:-0.004656, VLB:1138.343450, err:0.013071\n",
      "iter:106, delta_VLB:0.028154, VLB:1138.315297, err:0.013061\n",
      "iter:107, delta_VLB:-0.005852, VLB:1138.321149, err:0.013070\n",
      "iter:108, delta_VLB:0.029238, VLB:1138.291911, err:0.013063\n",
      "iter:109, delta_VLB:-0.015805, VLB:1138.307716, err:0.013072\n",
      "iter:110, delta_VLB:0.042745, VLB:1138.264971, err:0.013068\n",
      "iter:111, delta_VLB:-0.032069, VLB:1138.297040, err:0.013072\n",
      "iter:112, delta_VLB:0.050488, VLB:1138.246552, err:0.013066\n",
      "iter:113, delta_VLB:-0.037299, VLB:1138.283851, err:0.013071\n",
      "iter:114, delta_VLB:0.052868, VLB:1138.230983, err:0.013064\n",
      "iter:115, delta_VLB:-0.039415, VLB:1138.270398, err:0.013077\n",
      "iter:116, delta_VLB:0.069243, VLB:1138.201155, err:0.013062\n",
      "iter:117, delta_VLB:-0.050518, VLB:1138.251673, err:0.013069\n",
      "iter:118, delta_VLB:0.059680, VLB:1138.191992, err:0.013064\n",
      "iter:119, delta_VLB:-0.029829, VLB:1138.221821, err:0.013073\n",
      "iter:120, delta_VLB:0.033892, VLB:1138.187929, err:0.013064\n",
      "iter:121, delta_VLB:-0.036740, VLB:1138.224669, err:0.013074\n",
      "iter:122, delta_VLB:0.061246, VLB:1138.163423, err:0.013070\n",
      "iter:123, delta_VLB:-0.059679, VLB:1138.223102, err:0.013070\n",
      "iter:124, delta_VLB:0.052000, VLB:1138.171102, err:0.013066\n",
      "iter:125, delta_VLB:-0.051098, VLB:1138.222200, err:0.013073\n",
      "iter:126, delta_VLB:0.047977, VLB:1138.174222, err:0.013065\n",
      "iter:127, delta_VLB:-0.048577, VLB:1138.222800, err:0.013073\n",
      "iter:128, delta_VLB:0.047239, VLB:1138.175561, err:0.013065\n",
      "negVLB did not decrease any more.\n"
     ]
    }
   ],
   "source": [
    "FPi = Train(X_train,y_train,ix,X_test,y_test,max_iter=400,lr=2*10**-3,VLB_opt='FPi',lik='Gaussian');#Bernoulli,Gaussian\n",
    "Zs,Zs_label, VLB_FPi, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time_FPi,err_FPi = FPi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GD2 = Train(X_train,y_train,ix,X_test,y_test,max_iter=400,lr=4*10**-4,VLB_opt='GD',lik='Poisson2');\n",
    "Zs,Zs_label, VLB2, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time2,err2 = GD2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FPb2 = Train(X_train,y_train,ix,X_test,y_test,max_iter=50,lr=2*10**-4,VLB_opt='FPb',lik='Poisson2')\n",
    "Zs,Zs_label, VLB_FPb2, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time_FPb2,err_FPb2 = FPb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPi2 = Train(X_train,y_train,ix,X_test,y_test,max_iter=50,lr=2*10**-4,VLB_opt='FPi',lik='Poisson2')\n",
    "Zs,Zs_label, VLB_FPi2, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time_FPi2,err_FPi2=FPi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FPi = Train(X_train,y_train,ix,X_test,y_test,max_iter=50,lr=2*10**-4,VLB_opt='FPi',lik='Poisson')\n",
    "Zs,Zs_label, VLB_FPi, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time_FPi,err_FPi=FPi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPi = Train(X_train,y_train,ix,X_test,y_test,max_iter=50,lr=2*10**-4,VLB_opt='FPi',lik='Bernoulli')\n",
    "Zs,Zs_label, VLB_FPi, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time_FPi,err_FPi=FPi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import matplotlib.pyplot as plt\n",
    "host = host_subplot(111)\n",
    "par = host.twinx()\n",
    "host.set_xlabel(\"Training time(log)\")\n",
    "host.set_ylabel(\"Error\")\n",
    "par.set_ylabel(\"negVLB\")\n",
    "# p0, = host.plot(VLB_time, err, 'bo--', lw=1, markersize=5, fillstyle='none', label=\"Poisson GD error\")\n",
    "# p1, = host.plot(VLB_time2, err2, 'ro--', lw=1, markersize=5, fillstyle='none', label=\"Poisson2 GD error\")\n",
    "# p2, = par.plot(VLB_time, VLB, 'bo-', lw=1, markersize=5, label=\"Poisson GD negVLB\")\n",
    "# p3, = par.plot(VLB_time2, VLB2, 'ro-', lw=1, markersize=5, label=\"Poisson2 GD negVLB\")\n",
    "# p0, = host.plot(VLB_time_FPb, err_FPb, 'bo--', lw=1, markersize=5, fillstyle='none', label=\"Poisson FPb error\")\n",
    "# p1, = host.plot(VLB_time_FPb2, err_FPb2, 'ro--', lw=1, markersize=5, fillstyle='none', label=\"Poisson2 FPb error\")\n",
    "# p2, = par.plot(VLB_time_FPb, VLB_FPb, 'bo-', lw=1, markersize=5, label=\"Poisson FPb negVLB\")\n",
    "# p3, = par.plot(VLB_time_FPb2, VLB_FPb2, 'ro-', lw=1, markersize=5, label=\"Poisson2 FPb negVLB\")\n",
    "p0, = host.plot(VLB_time_FPi, err_FPi, 'bo--', lw=1, markersize=5, fillstyle='none', label=\"Poisson FPi error\")\n",
    "p1, = host.plot(VLB_time_FPi2, err_FPi2, 'ro--', lw=1, markersize=5, fillstyle='none', label=\"Poisson2 FPi error\")\n",
    "p2, = par.plot(VLB_time_FPi, VLB_FPi, 'bo-', lw=1, markersize=5, label=\"Poisson FPb negVLB\")\n",
    "p3, = par.plot(VLB_time_FPi2, VLB_FPi2, 'ro-', lw=1, markersize=5, label=\"Poisson2 FPb negVLB\")\n",
    "host.set_xscale('log',basex=10)\n",
    "leg = plt.legend()\n",
    "host.yaxis.get_label().set_color(p0.get_color());par.yaxis.get_label().set_color(p0.get_color())\n",
    "leg.texts[0].set_color(p0.get_color());\n",
    "leg.texts[1].set_color(p1.get_color())\n",
    "leg.texts[2].set_color(p2.get_color());\n",
    "leg.texts[3].set_color(p3.get_color())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import matplotlib.pyplot as plt\n",
    "host = host_subplot(111)\n",
    "par = host.twinx()\n",
    "host.set_xlabel(\"Training time(log)\")\n",
    "host.set_ylabel(\"Error\")\n",
    "par.set_ylabel(\"negVLB\")\n",
    "p0, = host.plot(VLB_time, err, 'bo--', lw=1, markersize=5, fillstyle='none', label=\"error GD\")\n",
    "p1, = host.plot(VLB_time_FPi, err_FPi, 'ro--', lw=1, markersize=5, fillstyle='none', label=\"error FPi\")\n",
    "p2, = host.plot(VLB_time_FPb, err_FPb, 'go--', lw=1, markersize=5, fillstyle='none', label=\"error FPb\")\n",
    "p3, = host.plot(VLB_time_FPim, err_FPim, 'co--', markersize=5, lw=1, fillstyle='none', label=\"error FPi-mean\")\n",
    "p4, = par.plot(VLB_time, VLB, 'bo-', lw=1, markersize=5, label=\"negVLB GD\")\n",
    "p5, = par.plot(VLB_time_FPi, VLB_FPi, 'ro-', lw=1, markersize=5, label=\"negVLB FPi\")\n",
    "p6, = par.plot(VLB_time_FPb, VLB_FPb, 'go-', lw=1, markersize=5, label=\"negVLB FPb\")\n",
    "p7, = par.plot(VLB_time_FPim, VLB_FPim, 'c+-', lw=1, markersize=5, label=\"negVLB FPi-mean\")\n",
    "host.set_xscale('log',basex=10)\n",
    "leg = plt.legend()\n",
    "host.yaxis.get_label().set_color(p0.get_color());par.yaxis.get_label().set_color(p0.get_color())\n",
    "leg.texts[0].set_color(p0.get_color());\n",
    "leg.texts[1].set_color(p1.get_color())\n",
    "leg.texts[2].set_color(p2.get_color());\n",
    "leg.texts[3].set_color(p3.get_color())\n",
    "leg.texts[4].set_color(p4.get_color());\n",
    "leg.texts[5].set_color(p5.get_color())\n",
    "leg.texts[6].set_color(p6.get_color());\n",
    "leg.texts[7].set_color(p7.get_color())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some 1D training data(regression)\n",
    "num_train = 2000;num_test=700                         # 500 training poitns\n",
    "X_train = np.linspace(0, 10, num_train)[:,None]       # Inputs evenly spaced between 0 and 10\n",
    "F = np.sin(X_train)                   # True function (f = sin(x))\n",
    "y_train = F + 0.01*np.random.randn(num_train)[:,None]  # Observations\n",
    "X_test = np.linspace(0, 10, num_test)[:,None]       # Inputs evenly spaced between 0 and 10\n",
    "F_test = np.sin(X_test)                   # True function (f = sin(x))\n",
    "y_test = F_test + 0.01*np.random.randn(num_test)[:,None]  # Observations\n",
    "np.random.seed(4);num_inducing=200\n",
    "ix = random.sample(range(num_train), num_inducing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(y_train >= 0, 1, -1); y_test = np.where(y_test >= 0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4176362])"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data from file, make sure banana.csv is in the same directory as this notebook\n",
    "data = np.genfromtxt('banana.csv', delimiter=',')\n",
    "\n",
    "# Dimension of data\n",
    "D = data.shape[1]-1\n",
    "\n",
    "# Seperate our data (input) from its corresponding label output\n",
    "# .. note we have to rescale from [-1,1] to [0,1] for a Bernoulli distribution\n",
    "X, y = data[:,:D], data[:,-1][:, None]\n",
    "\n",
    "# We will plot our data as well\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "# Plot 0 class in blue\n",
    "plt.plot(X[np.where(y == -1),0],X[np.where(y == -1),1],'bo', mew=0.5, alpha=0.5)\n",
    "# Plot 1 class in red\n",
    "plt.plot(X[np.where(y == 1),0],X[np.where(y == 1),1],'ro', mew=0.5, alpha=0.5)\n",
    "\n",
    "# Annotate plot\n",
    "plt.xlabel(\"$x_1$\"), plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Banana Dataset (red=1, blue=0)\")\n",
    "plt.axis(\"square\"), plt.xlim((-3, 3)), plt.ylim((-3, 3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from file, make sure banana.csv is in the same directory as this notebook\n",
    "data = np.genfromtxt('banana.csv', delimiter=',')\n",
    "\n",
    "# Dimension of data\n",
    "D = data.shape[1]-1\n",
    "n = data.shape[0]\n",
    "# Seperate our data (input) from its corresponding label output\n",
    "# .. note we have to rescale from [-1,1] to [0,1] for a Bernoulli distribution\n",
    "X, y = data[:,:D], data[:,-1][:, None]\n",
    "num_train = 3000; num_test = n - num_train\n",
    "X_train, y_train = X[:num_train,:], y[:num_train]\n",
    "X_test, y_test = X[:num_test,:], y[:num_test]\n",
    "np.random.seed(3);num_inducing=100 # randomly select active set and then keep them fixed\n",
    "ix = random.sample(range(num_train), num_inducing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "num = np.arange(-39,40);print(num)\n",
    "plt.figure(1)\n",
    "a1 = norm.cdf(num);print(a1)\n",
    "a2 = norm.pdf(num);print(a2)\n",
    "plt.plot(a1)\n",
    "plt.plot(a2)\n",
    "plt.figure(2)\n",
    "r = norm.pdf(num)/norm.cdf(num)\n",
    "plt.plot(r);print(r.shape)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4736461348785476e-196"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.pdf(-30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(np.linalg.eigvals(V) > 0)# check PSD condition for V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "  'a': lambda x: x * 5,\n",
    "  'b': lambda x: x + 7,\n",
    "  'c': lambda x: x - 2\n",
    "}[value](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "X_train, y_train = load_svmlight_file(\"segment.txt\")#segment,cpusmall,mg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2310, 19)\n",
      "(2310, 1)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "X_train=scipy.sparse.csr_matrix.todense(X_train)\n",
    "y_train=y_train[:,None]\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "sio.savemat('count_dataset_segment_N2319_D19.mat',{'x':X_train,'y':y_train})#count_dataset_segment_N2319_D19,reg_dataset_cpusmall_N8192_D12,reg_dataset_mg_N1385_D6,reg_dataset_spacega_N3107_D6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "fruitfly_data = fetch_openml(name='fruitfly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, 4)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruitfly_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125,)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruitfly_data.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2310, 19)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
