{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihoods we have used:\n",
    "1. regression: Gaussian\n",
    "2. count regression: \n",
    "    Poisson: lambda=e^f,  Poisson2: lambda=ln(1+e^f)\n",
    "3. binary classification: \n",
    "    Bernoulli: sigmoid,   Bernoulli2: Probit\n",
    "    \n",
    "For the sigmoid function, we use **expit** which is provided by *scipy.special*, because it is stable, fast and fairly accurate. Additionally, it is equivalent to sigmoid. For all methods, initial variational parameters are found by running the Laplace approximation on the subset/active set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv,norm,lstsq,cholesky\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy.io as sio\n",
    "from sklearn import preprocessing\n",
    "import random,time,GPy\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import loggamma,roots_hermitenorm,gamma,expit\n",
    "from scipy.special import ndtr as std_norm_cdf\n",
    "\n",
    "_sqrt_2pi = np.sqrt(2*np.pi)\n",
    "_lim_val = np.finfo(np.float64).max\n",
    "_lim_val_exp = np.log(_lim_val)\n",
    "\n",
    "def std_norm_pdf(x): # define a standard normal pdf(from GPy)\n",
    "    x = np.clip(x,-1e300,1e300)\n",
    "    return np.exp(-np.square(x)/2)/_sqrt_2pi\n",
    "\n",
    "def safe_exp(f):\n",
    "    clip_f = np.clip(f, -np.inf, _lim_val_exp)\n",
    "    return np.exp(clip_f)\n",
    "\n",
    "def safe_ln(x, minval=0.0000000001):\n",
    "    return np.log(x.clip(min=minval))\n",
    "\n",
    "def rbf_kernel(X1, X2, l=1.0, sigma_f=1.0):\n",
    "    '''\n",
    "    Isotropic squared exponential kernel. Computes \n",
    "    a covariance matrix from points in X1 and X2.    \n",
    "    Args:\n",
    "        X1: Array of m points (m x d).\n",
    "        X2: Array of n points (n x d).\n",
    "\n",
    "    Returns:\n",
    "        Covariance matrix (m x n).\n",
    "    '''\n",
    "    sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n",
    "    return sigma_f**2 * np.exp(-0.5 / l**2 * sqdist)\n",
    "\n",
    "def kernel(X1, X2, *args, **kwargs):\n",
    "    return rbf_kernel(X1, X2, *args, **kwargs)\n",
    "\n",
    "def MFE(true_labels, pred_labels):\n",
    "    '''\n",
    "    Calculating mean fraction error(MFE) for count regression, the math is used as follows:\n",
    "    MFE = mean( abs( (true_labels - pred_labels)./true_labels ) ) \n",
    "    Written by Kaikai\n",
    "    '''    \n",
    "    if true_labels.size != pred_labels.size:\n",
    "        print('The size of true_labels and pred_labels is supposed to be identical.')\n",
    "        return -1    \n",
    "    true_labels = true_labels.flatten().astype('double'); pred_labels = pred_labels.flatten().astype('double')\n",
    "    true_labels_temp = true_labels.copy()\n",
    "    if 0 in true_labels: # replace zero with a very small positive value in order to avoid dividing by zero\n",
    "        print('There are elements of zero value in true labels.')\n",
    "        true_labels_temp[true_labels==0] = 1\n",
    "    \n",
    "    MFE = np.mean( np.abs( (true_labels - pred_labels)/true_labels_temp ) )\n",
    "    return MFE\n",
    "\n",
    "def calc_vlb(m,V, a, lik='Gaussian'):\n",
    "    prior_mean_u = a[0]; prior_mean_f = a[1] # prior mean for inducing points    \n",
    "    A = a[2] # Knm*inv(Kmm)\n",
    "    Kmm = a[3]; Kmm_inv = a[4]; Kmn = a[5]; Knn_diag = a[6]; y = a[7] # the ground truth for training data\n",
    "    noise_var = a[8]\n",
    "    num_train = len(prior_mean_f);num_inducing = len(prior_mean_u)\n",
    "    m_q = prior_mean_f + np.dot(A, (m-prior_mean_u)) # Eq.(3a) in paper\n",
    "    v_q = ( Knn_diag.ravel() + np.diag(np.dot(A, np.dot(V-Kmm, A.T))) )[:, None] # Eq.(3b) in paper\n",
    "    c1 = m - prior_mean_u; c2 = np.dot(Kmm_inv, c1)\n",
    "    (Sign,LogDetKmm) = np.linalg.slogdet(Kmm); LogDetKmm = Sign*LogDetKmm\n",
    "    (SignV,LogDetV) = np.linalg.slogdet(V); LogDetV = SignV*LogDetV;#print(v_q[:50])     \n",
    "    if lik=='Bernoulli':\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q));#vlb_lik = expit(y*f);print('yf',expit(y*f))\n",
    "        vlb_lik = np.sum( 1/_sqrt_2pi*np.dot( safe_ln(expit(y*f)) ,w) ) # sigmoid liklihood\n",
    "#         vlb_lik = np.sum( 1.0/np.sqrt(2*np.pi)*np.dot( np.log(std_norm_cdf(y*f)+1e-10) ,w) ) # Probit liklihood\n",
    "    elif lik=='Gaussian':\n",
    "        vlb_lik = -np.log(np.sqrt(2*np.pi*noise_var)) - np.sum((y-m_q)**2+v_q)/(2*noise_var)\n",
    "    elif lik=='Poisson':\n",
    "        vlb_lik = np.dot(y.T,m_q) - np.sum(loggamma(y+1)) - np.sum(np.exp(m_q+0.5*v_q))\n",
    "    elif lik=='Poisson2': # another link func: lambda=ln(1+e^f)\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q));\n",
    "        term1 = -np.sum(loggamma(y+1)); term2 = -np.sum( 1/_sqrt_2pi*np.dot( np.log(1+safe_exp(f)),w) )\n",
    "        term3 = np.sum( 1/_sqrt_2pi*np.dot( y*safe_ln(np.log(1+safe_exp(f))),w) )\n",
    "        vlb_lik = term1 + term2 + term3\n",
    "    vlb_kl = 0.5*( LogDetKmm - LogDetV + np.dot(c1.T, c2) + np.trace(np.dot(Kmm_inv,V)) - len(prior_mean_u) )\n",
    "    vlb = vlb_lik - vlb_kl\n",
    "    return vlb \n",
    "\n",
    "def get_init_hyperparameters(Z,Z_label,lik='Poisson'):\n",
    "    # For all methods, initial variational parameters are found by running the Laplace approximation on the subset/active set.\n",
    "    dim_data = Z.shape[1]\n",
    "    kern = GPy.kern.RBF(dim_data, variance=1.0, lengthscale=1.0)\n",
    "    likelihood = {\n",
    "          'Poisson': GPy.likelihoods.Poisson(),\n",
    "          'Poisson2': GPy.likelihoods.Poisson(GPy.likelihoods.link_functions.Log_ex_1()),\n",
    "          'Gaussian': GPy.likelihoods.Gaussian(),\n",
    "          'Bernoulli': GPy.likelihoods.Bernoulli()\n",
    "        }[lik]\n",
    "    laplace_inf = GPy.inference.latent_function_inference.Laplace()\n",
    "    model_lap = GPy.core.GP(X=Z, Y=Z_label, likelihood=likelihood, inference_method=laplace_inf, kernel=kern)\n",
    "    model_lap.optimize()\n",
    "    print(model_lap)\n",
    "    return model_lap\n",
    "\n",
    "def Adam(theta, g_t, t, alpha=0.01, m_t=0, v_t=0, opt='minimize'):\n",
    "    beta_1 = 0.9; beta_2 = 0.999; epsilon = 1e-8     #initialize the values of the parameters    \n",
    "    m_t = beta_1*m_t + (1-beta_1)*g_t                #updates the moving averages of the gradient\n",
    "    v_t = beta_2*v_t + (1-beta_2)*(g_t*g_t)          #updates the moving averages of the squared gradient\n",
    "    m_cap = m_t/(1-(beta_1**t))                      #calculates the bias-corrected estimates\n",
    "    v_cap = v_t/(1-(beta_2**t))                      #calculates the bias-corrected estimates\n",
    "    if opt=='maximize':\n",
    "        theta = theta + (alpha*m_cap)/(np.sqrt(v_cap)+epsilon)    #updates the parameters\n",
    "    else:\n",
    "        theta = theta - (alpha*m_cap)/(np.sqrt(v_cap)+epsilon)    #updates the parameters\n",
    "    return theta, m_t, v_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GH_quad(mu_star,std_star): # gauss_hermite_quad to calculate numerical integration, w and z denote weights and sample points, respectively. \n",
    "    z,w = roots_hermitenorm(n=50, mu=False); z,w=z[:,None],w[:,None]\n",
    "    z = np.kron(std_star,z.T) + mu_star\n",
    "    return z,w\n",
    "\n",
    "def predict(X, xStar, l, sigma2, m, V,Kmm,Kmm_inv,y_min=4,y_max=20,noise_var=0.1,lik='Poisson'):\n",
    "    num_xStar = xStar.shape[0]; Kmn = kernel(X, xStar, l, np.sqrt(sigma2))\n",
    "    Knn_diag = np.eye(num_xStar)*sigma2+noise_var*np.eye(num_xStar)\n",
    "    A = np.dot(Kmn.T, Kmm_inv)\n",
    "    mu_star = np.dot(A,m); v2_star = Knn_diag + np.dot(A, np.dot(V-Kmm,A.T))\n",
    "    std_star = np.sqrt(np.diag(v2_star))[:,None]\n",
    "    if lik=='Bernoulli':\n",
    "        return calc_Bernoulli_pred(mu_star,std_star)\n",
    "    elif lik=='Gaussian':\n",
    "        return calc_Gauss_pred(mu_star,std_star,noise_var)\n",
    "    else: # Poisson, Poisson2\n",
    "        return calc_Poisson_pred(mu_star,std_star,num_xStar,y_min,y_max,lik)\n",
    "\n",
    "def calc_Gauss_pred(mu_star,std_star,noise_var):\n",
    "    return mu_star, std_star+np.sqrt(noise_var)\n",
    "\n",
    "def calc_Poisson_pred(mu_star,std_star,num_xStar,y_min,y_max,lik='Poisson'):\n",
    "    f,w = GH_quad(mu_star,std_star)\n",
    "    y_range = np.arange(y_min,y_max+1)\n",
    "    lik_func = {\n",
    "        'Poisson': lambda f,y: 1/gamma(y+1)*safe_exp(-safe_exp(f)+f*y),\n",
    "        'Poisson2': lambda f,y: 1/gamma(y+1)*expit(-f)*( safe_ln(1.0+safe_exp(f))**y )\n",
    "    }[lik]\n",
    "    poisson_lik = np.zeros((num_xStar,len(y_range)))\n",
    "    for i,y in enumerate(y_range):\n",
    "        poisson_lik[:,i] = 1/_sqrt_2pi*np.dot(lik_func(f,y),w).ravel()\n",
    "    return poisson_lik\n",
    "\n",
    "def calc_Bernoulli_pred(mu_star,std_star):\n",
    "#     v = std_star**2; kappa_v = (1+np.pi*v/8.0)**(-1/2) # Probit liklihood\n",
    "#     p = sigmoid(kappa_v*mu_star) # p(y=1|x_*,m,V)\n",
    "    f,w = GH_quad(mu_star,std_star) # sigmoid liklihood\n",
    "    p = 1/np.sqrt(2*np.pi)*np.dot( expit(f), w ) # p(y=1|x_*,m,V)\n",
    "    return p\n",
    "\n",
    "def calc_m_q(m, A, prior_mean_u, prior_mean_f):\n",
    "    return prior_mean_f + np.dot(A, (m-prior_mean_u))\n",
    "\n",
    "def calc_v_q(V, A, Kmm, Knn_diag):\n",
    "    return ( Knn_diag.ravel() + np.diag(np.dot(A, np.dot(V-Kmm, A.T))) )[:,None] # Eq.(3b) in paper\n",
    "\n",
    "def calc_rho(m_q, v_q, y, lik='Poisson', noise_var=0.1):\n",
    "    if lik=='Bernoulli':\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q))\n",
    "        return 1.0/_sqrt_2pi*np.dot(  y*expit(-y*f) , w ) # sigmoid liklihood\n",
    "#         return 1.0/_sqrt_2pi*np.dot( y*std_norm_pdf(f) / (std_norm_cdf(y*f)+1e-10), w ) # Probit liklihood        \n",
    "    elif lik=='Gaussian':\n",
    "        return 1.0/noise_var*(y-m_q)\n",
    "    elif lik=='Poisson':\n",
    "        return -np.exp(m_q + 0.5*v_q) + y\n",
    "    elif lik=='Poisson2':\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q))\n",
    "        y = np.tile(y,[1,f.shape[1]]); term2 = expit(f); t0 = safe_ln(1+safe_exp(f))# avoid dividing by zeros\n",
    "        d1_log_lik = np.zeros(t0.shape)\n",
    "        d1_log_lik[t0!=0] = (y[t0!=0]/t0[t0!=0] - 1)*term2[t0!=0]\n",
    "        return 1.0/_sqrt_2pi*np.dot(  d1_log_lik, w )\n",
    "    \n",
    "def calc_lambda(m_q, v_q, y, lik='Poisson', noise_var=0.1):\n",
    "    if lik=='Bernoulli':\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q))\n",
    "        return 1.0/np.sqrt(2*np.pi)*np.dot(  -expit(y*f)*expit(-y*f), w ) # sigmoid\n",
    "#         return 1.0/_sqrt_2pi*np.dot( -std_norm_pdf(f)**2 / (std_norm_cdf(y*f)**2+1e-10) - y*f*std_norm_pdf(f) / (std_norm_cdf(y*f)+1e-10), w )\n",
    "    elif lik=='Gaussian':\n",
    "        return -1.0/noise_var*np.ones((len(m_q),1))\n",
    "    elif lik=='Poisson':\n",
    "        return -np.exp(m_q + 0.5*v_q)\n",
    "    elif lik=='Poisson2':\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q));y = np.tile(y,[1,f.shape[1]]);\n",
    "        term2 = expit(f)*expit(f); t0 = safe_ln(1+safe_exp(f));d2_log_lik = np.zeros(t0.shape)\n",
    "        d2_log_lik[t0!=0] = ((y[t0!=0]/t0[t0!=0]-1)*safe_exp(-f[t0!=0])-y[t0!=0]/(t0[t0!=0])**2)*term2[t0!=0]\n",
    "        return 1.0/_sqrt_2pi*np.dot(  d2_log_lik, w )\n",
    "\n",
    "def dVLb_dm(m, rho, prior_mean_u, Kmm_inv, A):\n",
    "    dm = np.dot(A.T,rho) - np.dot(Kmm_inv, m-prior_mean_u) # Eq.(11a) in paper\n",
    "    return dm\n",
    "\n",
    "def dVLb_dL(L, lam, Kmm_inv, A, noise_var=1e-8):# optimizing the cholesky factor L guarantees the PSD of V automatically\n",
    "    dL = np.dot( np.dot( np.dot(A.T, np.diag(lam.ravel())), A ), L ) + inv(L+np.sqrt(noise_var)*np.eye(len(L))).T - np.dot(Kmm_inv, L)\n",
    "    return dL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(X,Y,ix,X_test=None,y_test=None,max_iter=100,lr=1*10**-5,FPb_cond=1*10**-1,stop_cond=1*10**-3,VLB_opt='GD',lik='Poisson'):\n",
    "    num_train = X.shape[0];num_inducing=len(ix);num_test=X_test.shape[0]; \n",
    "    prior_mean_u = np.zeros((num_inducing, 1)); prior_mean_f = np.zeros((num_train, 1))\n",
    "    Z = X[ix, :]; Z_label = Y[ix] # Z is the inducing set\n",
    "    model_lap = get_init_hyperparameters(Z,Z_label,lik=lik)#(Z_label+1)/2    \n",
    "    # variational parameters from initialization\n",
    "    f_mean, f_var = model_lap._raw_predict(X) \n",
    "    length_scale = model_lap.rbf.lengthscale[0]; sigma2 = model_lap.rbf.variance[0];\n",
    "    if lik=='Gaussian': noise_var=model_lap.Gaussian_noise.variance[0] \n",
    "    else: noise_var=1*10**-8\n",
    "    #  we use variational mean from Laplace appr\n",
    "    m = f_mean[ix]; V = np.diag(f_var[ix].ravel()); L = cholesky(V)\n",
    "    Kmm = kernel(Z, Z, l = length_scale, sigma_f = np.sqrt(sigma2)) + noise_var*np.eye(len(Z))\n",
    "    Kmm_inv = inv(Kmm); Kmn = kernel(Z, X, l = length_scale, sigma_f = np.sqrt(sigma2)); Knn_diag = sigma2*np.ones((num_train, 1))\n",
    "    A = np.dot(Kmn.T, Kmm_inv) # Knm*inv(Kmm)    \n",
    "    a = (prior_mean_u,prior_mean_f,A,Kmm,Kmm_inv,Kmn,Knn_diag,Y,noise_var)\n",
    "    num_iter = 0; VLB = []; VLB_time = []; err = []; FPb_converge = True\n",
    "    var = np.hstack([m.flatten(), L.flatten()])\n",
    "    start_time = time.time(); VLB.append(-calc_vlb(m,V, a,lik)[0][0])\n",
    "    if lik=='Poisson' or lik=='Poisson2':\n",
    "        y_min = min(Y); y_max = max(Y)\n",
    "        poisson_lik = predict(Z, X_test, length_scale, sigma2, m, V,Kmm,Kmm_inv,y_min=y_min,y_max=y_max,noise_var=noise_var,lik=lik)\n",
    "        res = np.argmax(poisson_lik, axis=1)+y_min; err.append( MFE(y_test, res) )\n",
    "    elif lik=='Gaussian':\n",
    "        mu_star, std_star = predict(Z, X_test, length_scale, sigma2, m, V,Kmm,Kmm_inv,noise_var=noise_var,lik=lik)\n",
    "        err.append( 1/num_test*( np.sum( (y_test-mu_star)**2 ) ) )\n",
    "    elif lik=='Bernoulli':\n",
    "        p = predict(Z, X_test, length_scale, sigma2, m, V,Kmm,Kmm_inv,noise_var=noise_var,lik=lik); \n",
    "        res = np.where(p>=0.5,1,-1)\n",
    "        err.append( np.sum( np.where(y_test*res<0,1,0) )/num_test );print('err:', err)\n",
    "    VLB_time.append(time.time()-start_time);\n",
    "    while 1:\n",
    "        num_iter = num_iter +1;\n",
    "        m_q = calc_m_q(m, A, prior_mean_u, prior_mean_f); v_q = calc_v_q(V, A, Kmm, Knn_diag);#print('m_q:',m_q[:10])\n",
    "        # According to Table 1 in paper, expectations of the derivatives wrt N(f|m,v) for Possion likelihood\n",
    "        rho = calc_rho(m_q, v_q, Y,lik, noise_var); lam = calc_lambda(m_q, v_q,Y,lik, noise_var);        \n",
    "        if VLB_opt=='GD':\n",
    "            dm = dVLb_dm(m, rho, prior_mean_u, Kmm_inv, A)\n",
    "            dL = dVLb_dL(L, lam, Kmm_inv, A, noise_var)\n",
    "            gradients = np.hstack([dm.flatten(), dL.flatten()])\n",
    "            var, m_t, v_t = Adam(var,gradients,num_iter,alpha=lr,opt='maximize')\n",
    "            m = var[:num_inducing][:,None] # variantional mean\n",
    "            L = var[num_inducing:].reshape(num_inducing,num_inducing) # variational variance V=L*L.T\n",
    "            V = np.dot(L, L.T);\n",
    "            \n",
    "        elif VLB_opt=='FPi':\n",
    "            dm = dVLb_dm(m, rho, prior_mean_u, Kmm_inv, A)\n",
    "            m, m_t, v_t = Adam(m,dm,num_iter,alpha=lr,opt='maximize') # print('old V',V[:1,:1])\n",
    "            V = inv(Kmm_inv-np.dot( np.dot(A.T, np.diag(lam.ravel())), A ));\n",
    "        elif VLB_opt=='FPb':            \n",
    "            if FPb_converge:\n",
    "                dm = dVLb_dm(m, rho, prior_mean_u, Kmm_inv, A); print('old m:',m[:2]);\n",
    "                m, m_t, v_t = Adam(m,dm,num_iter,alpha=lr,opt='maximize')\n",
    "            else:\n",
    "                V = inv(Kmm_inv-np.dot( np.dot(A.T, np.diag(lam.ravel())), A))\n",
    "                \n",
    "        elif VLB_opt=='FPi-mean':\n",
    "            d = A.T; gamma = -lam; # print('old m:',m[:2]);            \n",
    "            m = np.dot(V, np.dot(d, rho + np.dot(A,m)*gamma) ); \n",
    "            V = inv(Kmm_inv-np.dot( np.dot(A.T, np.diag(lam.ravel())), A))\n",
    "\n",
    "        VLB.append(-calc_vlb(m,V, a,lik)[0][0]); VLB_time.append(time.time()-start_time)\n",
    "        if lik=='Poisson' or lik=='Poisson2':\n",
    "            poisson_lik = predict(Z, X_test, length_scale, sigma2, m, V,Kmm,Kmm_inv,y_min=y_min,y_max=y_max,noise_var=noise_var,lik=lik)\n",
    "            res = np.argmax(poisson_lik, axis=1)+y_min; err.append( MFE(y_test, res) )\n",
    "        elif lik=='Gaussian':\n",
    "            mu_star, std_star = predict(Z, X_test, length_scale, sigma2, m, V,Kmm,Kmm_inv,noise_var=noise_var,lik=lik)\n",
    "            err.append( 1/num_test*( np.sum( (y_test-mu_star)**2 ) ) )\n",
    "        elif lik=='Bernoulli':\n",
    "            p = predict(Z, X_test, length_scale, sigma2, m, V,Kmm,Kmm_inv,noise_var=noise_var,lik=lik); res = np.where(p>=0.5,1,-1)\n",
    "            err.append( np.sum( np.where(y_test*res<0,1,0) )/num_test );\n",
    "        if num_iter>0:\n",
    "            delta_vlb = abs(VLB[-1]-VLB[-2])\n",
    "            if VLB_opt=='FPb' and delta_vlb<=FPb_cond:\n",
    "                FPb_converge = not FPb_converge; print('m or V converged')\n",
    "            if num_iter>=5 and delta_vlb<=stop_cond:\n",
    "                print('After {} iterations it converged: delta_VLB:{:.6f}, VLB:{:.6f}, err:{:.6f}'.format(num_iter,delta_vlb,VLB[-1],err[-1]) );break                \n",
    "            if num_iter==max_iter:\n",
    "                print('It has reached the maximum number of iterations, i.e. {}, with delta_VLB:{:.6f}, VLB:{:.6f} and err:{:.6f}'.format(max_iter,delta_vlb,VLB[-1],err[-1]));break\n",
    "            if num_iter%5 == 0:\n",
    "                print('iter:{}, delta_VLB:{:.6f}, VLB:{:.6f}, err:{:.6f}'.format(num_iter, delta_vlb, VLB[-1], err[-1]));\n",
    "            \n",
    "    return Z,Z_label, VLB, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv, A,VLB_time,err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name : gp\n",
      "Objective : 43.6836449900548\n",
      "Number of Parameters : 2\n",
      "Number of Optimization Parameters : 2\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1mgp.            \u001b[0;0m  |               value  |  constraints  |  priors\n",
      "  \u001b[1mrbf.variance   \u001b[0;0m  |   7.050465253450157  |      +ve      |        \n",
      "  \u001b[1mrbf.lengthscale\u001b[0;0m  |  0.6631713194475982  |      +ve      |        \n",
      "err: [0.18521739130434783]\n",
      "iter:5, delta_VLB=2390238.822964, VLB: 128652061.744649, err: 0.185217\n",
      "iter:10, delta_VLB=1887573.838554, VLB: 118454868.597162, err: 0.183913\n",
      "iter:15, delta_VLB=1687985.971080, VLB: 109673442.017752, err: 0.182609\n",
      "iter:20, delta_VLB=1597009.009117, VLB: 101523561.263213, err: 0.183043\n",
      "iter:25, delta_VLB=1512990.624108, VLB: 93795730.345453, err: 0.181739\n",
      "iter:30, delta_VLB=1454712.953930, VLB: 86409602.349504, err: 0.181739\n",
      "iter:35, delta_VLB=1376033.854687, VLB: 79376649.450643, err: 0.179565\n",
      "iter:40, delta_VLB=1310601.749983, VLB: 72696877.969324, err: 0.180000\n",
      "iter:45, delta_VLB=1227684.673608, VLB: 66395579.781535, err: 0.178261\n",
      "iter:50, delta_VLB=1163286.005268, VLB: 60456692.208657, err: 0.179130\n",
      "iter:55, delta_VLB=1083086.239191, VLB: 54885023.780016, err: 0.176957\n",
      "iter:60, delta_VLB=1025254.255544, VLB: 49649684.324646, err: 0.178261\n",
      "iter:65, delta_VLB=938587.039857, VLB: 44781603.662434, err: 0.176957\n",
      "iter:70, delta_VLB=888054.879933, VLB: 40238431.443476, err: 0.177826\n",
      "iter:75, delta_VLB=798803.761437, VLB: 36060986.281297, err: 0.176087\n",
      "iter:80, delta_VLB=752384.459574, VLB: 32196830.969691, err: 0.176522\n",
      "iter:85, delta_VLB=666532.793123, VLB: 28689848.187063, err: 0.175217\n",
      "iter:90, delta_VLB=629254.276270, VLB: 25463405.246546, err: 0.175652\n",
      "iter:95, delta_VLB=552811.697190, VLB: 22548052.497462, err: 0.172609\n",
      "iter:100, delta_VLB=526786.100605, VLB: 19866730.074509, err: 0.173913\n",
      "iter:105, delta_VLB=455144.195828, VLB: 17449159.275193, err: 0.169130\n",
      "iter:110, delta_VLB=436116.348000, VLB: 15237045.200603, err: 0.169130\n",
      "iter:115, delta_VLB=369763.445224, VLB: 13253027.124290, err: 0.159565\n",
      "iter:120, delta_VLB=362158.617313, VLB: 11434528.104777, err: 0.160000\n",
      "iter:125, delta_VLB=298638.553810, VLB: 9824384.295250, err: 0.155652\n",
      "iter:130, delta_VLB=291830.876155, VLB: 8360593.909162, err: 0.158696\n",
      "iter:135, delta_VLB=231990.279965, VLB: 7086333.850609, err: 0.151739\n",
      "iter:140, delta_VLB=229706.898809, VLB: 5937015.260930, err: 0.154783\n",
      "iter:145, delta_VLB=167677.479039, VLB: 4966789.879481, err: 0.150435\n",
      "iter:150, delta_VLB=173408.551261, VLB: 4107166.663448, err: 0.155217\n",
      "iter:155, delta_VLB=113202.349959, VLB: 3421163.881512, err: 0.149565\n",
      "iter:160, delta_VLB=129308.780424, VLB: 2814717.039491, err: 0.152609\n",
      "iter:165, delta_VLB=68793.591875, VLB: 2350086.231905, err: 0.146522\n",
      "iter:170, delta_VLB=87492.730722, VLB: 1947289.859496, err: 0.152174\n",
      "iter:175, delta_VLB=33443.788472, VLB: 1671116.779063, err: 0.143913\n",
      "iter:180, delta_VLB=62167.151081, VLB: 1415039.966280, err: 0.149565\n",
      "iter:185, delta_VLB=11026.971394, VLB: 1258515.967489, err: 0.139130\n",
      "iter:190, delta_VLB=45949.542579, VLB: 1095690.636067, err: 0.145652\n",
      "iter:195, delta_VLB=3348.967914, VLB: 1011069.786504, err: 0.135217\n",
      "It has reached the maximum number of iterations, i.e. 200, with delta_VLB:38447.623512, VLB:905159.368495 and err:0.143043\n"
     ]
    }
   ],
   "source": [
    "GD = Train(X_train,y_train,ix,X_test,y_test,max_iter=200,lr=2*10**-3,VLB_opt='GD',lik='Bernoulli');\n",
    "Zs,Zs_label, VLB, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time,err = GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name : gp\n",
      "Objective : 43.6836449900548\n",
      "Number of Parameters : 2\n",
      "Number of Optimization Parameters : 2\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1mgp.            \u001b[0;0m  |               value  |  constraints  |  priors\n",
      "  \u001b[1mrbf.variance   \u001b[0;0m  |   7.050465253450157  |      +ve      |        \n",
      "  \u001b[1mrbf.lengthscale\u001b[0;0m  |  0.6631713194475982  |      +ve      |        \n",
      "err: [0.18521739130434783]\n",
      "old m: [[-0.70346524]\n",
      " [-2.45772837]]\n",
      "old m: [[-0.70366524]\n",
      " [-2.45792837]]\n",
      "old m: [[-0.70381406]\n",
      " [-2.4580772 ]]\n",
      "old m: [[-0.70394183]\n",
      " [-2.45820496]]\n",
      "old m: [[-0.70405805]\n",
      " [-2.45832119]]\n",
      "iter: 5, variation_VLB=11.375079, VLB: 145364565.180929\n",
      "err: 0.18565217391304348\n",
      "old m: [[-0.70416715]\n",
      " [-2.45843028]]\n",
      "old m: [[-0.70427157]\n",
      " [-2.45853471]]\n",
      "old m: [[-0.70437285]\n",
      " [-2.45863598]]\n",
      "old m: [[-0.704472  ]\n",
      " [-2.45873513]]\n",
      "old m: [[-0.70456975]\n",
      " [-2.45883288]]\n",
      "iter: 10, variation_VLB=6.913691, VLB: 145364603.978440\n",
      "err: 0.18608695652173912\n",
      "old m: [[-0.70466663]\n",
      " [-2.45892977]]\n",
      "old m: [[-0.70476306]\n",
      " [-2.4590262 ]]\n",
      "old m: [[-0.70485935]\n",
      " [-2.45912248]]\n",
      "old m: [[-0.70495575]\n",
      " [-2.45921888]]\n",
      "old m: [[-0.70505246]\n",
      " [-2.4593156 ]]\n",
      "iter: 15, variation_VLB=16.417562, VLB: 145364633.052318\n",
      "err: 0.1865217391304348\n",
      "old m: [[-0.70514966]\n",
      " [-2.4594128 ]]\n",
      "old m: [[-0.70524749]\n",
      " [-2.45951063]]\n",
      "old m: [[-0.70534607]\n",
      " [-2.4596092 ]]\n",
      "old m: [[-0.70544548]\n",
      " [-2.45970861]]\n",
      "old m: [[-0.70554582]\n",
      " [-2.45980896]]\n",
      "iter: 20, variation_VLB=3.939834, VLB: 145364648.590769\n",
      "err: 0.18695652173913044\n",
      "old m: [[-0.70564716]\n",
      " [-2.4599103 ]]\n",
      "old m: [[-0.70574956]\n",
      " [-2.4600127 ]]\n",
      "old m: [[-0.70585307]\n",
      " [-2.46011621]]\n",
      "old m: [[-0.70595774]\n",
      " [-2.46022088]]\n",
      "old m: [[-0.7060636 ]\n",
      " [-2.46032674]]\n",
      "iter: 25, variation_VLB=2.030920, VLB: 145364670.964137\n",
      "err: 0.1865217391304348\n",
      "old m: [[-0.70617069]\n",
      " [-2.46043383]]\n",
      "old m: [[-0.70627904]\n",
      " [-2.46054218]]\n",
      "old m: [[-0.70638867]\n",
      " [-2.4606518 ]]\n",
      "old m: [[-0.70649959]\n",
      " [-2.46076273]]\n",
      "old m: [[-0.70661183]\n",
      " [-2.46087497]]\n",
      "iter: 30, variation_VLB=2.774259, VLB: 145364713.333901\n",
      "err: 0.1865217391304348\n",
      "old m: [[-0.7067254 ]\n",
      " [-2.46098854]]\n",
      "old m: [[-0.70684031]\n",
      " [-2.46110344]]\n",
      "m or V converged\n",
      "old m: [[-0.70695657]\n",
      " [-2.4612197 ]]\n",
      "old m: [[-0.70695657]\n",
      " [-2.4612197 ]]\n",
      "old m: [[-0.70695657]\n",
      " [-2.4612197 ]]\n",
      "m or V converged\n",
      "iter: 35, variation_VLB=0.052024, VLB: 1258.040095\n",
      "err: 0.11652173913043479\n",
      "old m: [[-0.70695657]\n",
      " [-2.4612197 ]]\n",
      "old m: [[-0.70683487]\n",
      " [-2.4613414 ]]\n",
      "old m: [[-0.7067118 ]\n",
      " [-2.46146446]]\n",
      "old m: [[-0.70658738]\n",
      " [-2.46134004]]\n",
      "old m: [[-0.70646159]\n",
      " [-2.46121425]]\n",
      "iter: 40, variation_VLB=10.723950, VLB: 1131.798273\n",
      "err: 0.11739130434782609\n",
      "old m: [[-0.70633444]\n",
      " [-2.4610871 ]]\n",
      "old m: [[-0.70620594]\n",
      " [-2.4609586 ]]\n",
      "old m: [[-0.70607609]\n",
      " [-2.46082875]]\n",
      "old m: [[-0.70594489]\n",
      " [-2.46069755]]\n",
      "old m: [[-0.70581236]\n",
      " [-2.46056502]]\n",
      "iter: 45, variation_VLB=0.255145, VLB: 1101.347828\n",
      "err: 0.11652173913043479\n",
      "old m: [[-0.70567849]\n",
      " [-2.46043115]]\n",
      "old m: [[-0.70554329]\n",
      " [-2.46029595]]\n",
      "old m: [[-0.70540677]\n",
      " [-2.46015943]]\n",
      "old m: [[-0.70526895]\n",
      " [-2.46002161]]\n",
      "old m: [[-0.70540808]\n",
      " [-2.46016074]]\n",
      "It has reached the maximum number of iterations, i.e. 50, with variation_VLB=4.655793 and VLB: 1089.037326\n",
      "err: 0.11695652173913043\n"
     ]
    }
   ],
   "source": [
    "Zs,Zs_label, VLB_FPi, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time_FPi,err_FPi = Train(X_train,y_train,ix,X_test,y_test,max_iter=50,lr=2*10**-4,VLB_opt='FPi',lik='Bernoulli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zs,Zs_label, VLB_FPi, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time_FPi,err_FPi = Train(X_train,y_train,ix,X_test,y_test,max_iter=50,lr=2*10**-4,VLB_opt='FPi',lik='Bernoulli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Zs,Zs_label, VLB_FPi, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time_FPi,err_FPi = Train(X_train,y_train,ix,X_test,y_test,max_iter=50,lr=2*10**-4,VLB_opt='FPi-mean',lik='Bernoulli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some 1D training data(regression)\n",
    "num_train = 2000;num_test=700                         # 500 training poitns\n",
    "X_train = np.linspace(0, 10, num_train)[:,None]       # Inputs evenly spaced between 0 and 10\n",
    "F = np.sin(X_train)                   # True function (f = sin(x))\n",
    "y_train = F + 0.01*np.random.randn(num_train)[:,None]  # Observations\n",
    "X_test = np.linspace(0, 10, num_test)[:,None]       # Inputs evenly spaced between 0 and 10\n",
    "F_test = np.sin(X_test)                   # True function (f = sin(x))\n",
    "y_test = F_test + 0.01*np.random.randn(num_test)[:,None]  # Observations\n",
    "np.random.seed(3);num_inducing=100\n",
    "ix = random.sample(range(num_train), num_inducing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(y_train >= 0, 1, -1); y_test = np.where(y_test >= 0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data(count regression)\n",
    "mat_contents = sio.loadmat('count_dataset_ucsdpeds1l_N4000_D30.mat')\n",
    "x = mat_contents['x']; y = mat_contents['y']\n",
    "x = x.astype('double')\n",
    "\n",
    "# shuffle the data and split data into training set and test set\n",
    "data = np.concatenate((x,y), axis=1)\n",
    "np.random.shuffle(data)\n",
    "num_data = data.shape[0]; dim_data = data.shape[1] - 1;\n",
    "num_train = int(0.5*np.ceil(num_data)); num_test = num_data - num_train\n",
    "x_train = data[:num_train, :-1]; y_train = data[:num_train, -1];\n",
    "x_test = data[num_train:, :-1]; y_test = data[num_train:, -1];\n",
    "y_train = y_train[:, None]; y_test = y_test[:, None]\n",
    "# data Standardization with zero mean and unit variance\n",
    "scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "X_train = scaler.transform(x_train); X_test = scaler.transform(x_test)\n",
    "np.random.seed(3);num_inducing=100\n",
    "ix = random.sample(range(num_train), num_inducing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data from file, make sure banana.csv is in the same directory as this notebook\n",
    "data = np.genfromtxt('banana.csv', delimiter=',')\n",
    "\n",
    "# Dimension of data\n",
    "D = data.shape[1]-1\n",
    "\n",
    "# Seperate our data (input) from its corresponding label output\n",
    "# .. note we have to rescale from [-1,1] to [0,1] for a Bernoulli distribution\n",
    "X, y = data[:,:D], data[:,-1][:, None]\n",
    "\n",
    "# We will plot our data as well\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "# Plot 0 class in blue\n",
    "plt.plot(X[np.where(y == -1),0],X[np.where(y == -1),1],'bo', mew=0.5, alpha=0.5)\n",
    "# Plot 1 class in red\n",
    "plt.plot(X[np.where(y == 1),0],X[np.where(y == 1),1],'ro', mew=0.5, alpha=0.5)\n",
    "\n",
    "# Annotate plot\n",
    "plt.xlabel(\"$x_1$\"), plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Banana Dataset (red=1, blue=0)\")\n",
    "plt.axis(\"square\"), plt.xlim((-3, 3)), plt.ylim((-3, 3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from file, make sure banana.csv is in the same directory as this notebook\n",
    "data = np.genfromtxt('banana.csv', delimiter=',')\n",
    "\n",
    "# Dimension of data\n",
    "D = data.shape[1]-1\n",
    "n = data.shape[0]\n",
    "# Seperate our data (input) from its corresponding label output\n",
    "# .. note we have to rescale from [-1,1] to [0,1] for a Bernoulli distribution\n",
    "X, y = data[:,:D], data[:,-1][:, None]\n",
    "num_train = 3000; num_test = n - num_train\n",
    "X_train, y_train = X[:num_train,:], y[:num_train]\n",
    "X_test, y_test = X[:num_test,:], y[:num_test]\n",
    "np.random.seed(3);num_inducing=100 # randomly select active set and then keep them fixed\n",
    "ix = random.sample(range(num_train), num_inducing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-40 -39 -38 -37 -36 -35 -34 -33 -32 -31 -30 -29 -28 -27 -26 -25 -24 -23\n",
      " -22 -21 -20 -19 -18 -17 -16 -15 -14 -13 -12 -11 -10  -9  -8  -7  -6  -5\n",
      "  -4  -3  -2  -1   0   1   2   3   4   5   6   7   8   9  10  11  12  13\n",
      "  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31\n",
      "  32  33  34  35  36  37  38  39]\n",
      "[0.00000000e+000 0.00000000e+000 0.00000000e+000 5.72557122e-300\n",
      " 4.18262407e-284 1.12491071e-268 1.11389879e-253 4.06118562e-239\n",
      " 5.45208060e-225 2.69525008e-211 4.90671393e-198 3.28978527e-185\n",
      " 8.12386947e-173 7.38948101e-161 2.47606332e-149 3.05669671e-138\n",
      " 1.39039212e-127 2.33063701e-117 1.43989244e-107 3.27927802e-098\n",
      " 2.75362412e-089 8.52722395e-081 9.74094892e-073 4.10599620e-065\n",
      " 6.38875440e-058 3.67096620e-051 7.79353682e-045 6.11716440e-039\n",
      " 1.77648211e-033 1.91065957e-028 7.61985302e-024 1.12858841e-019\n",
      " 6.22096057e-016 1.27981254e-012 9.86587645e-010 2.86651572e-007\n",
      " 3.16712418e-005 1.34989803e-003 2.27501319e-002 1.58655254e-001\n",
      " 5.00000000e-001 8.41344746e-001 9.77249868e-001 9.98650102e-001\n",
      " 9.99968329e-001 9.99999713e-001 9.99999999e-001 1.00000000e+000\n",
      " 1.00000000e+000 1.00000000e+000 1.00000000e+000 1.00000000e+000\n",
      " 1.00000000e+000 1.00000000e+000 1.00000000e+000 1.00000000e+000\n",
      " 1.00000000e+000 1.00000000e+000 1.00000000e+000 1.00000000e+000\n",
      " 1.00000000e+000 1.00000000e+000 1.00000000e+000 1.00000000e+000\n",
      " 1.00000000e+000 1.00000000e+000 1.00000000e+000 1.00000000e+000\n",
      " 1.00000000e+000 1.00000000e+000 1.00000000e+000 1.00000000e+000\n",
      " 1.00000000e+000 1.00000000e+000 1.00000000e+000 1.00000000e+000\n",
      " 1.00000000e+000 1.00000000e+000 1.00000000e+000 1.00000000e+000]\n",
      "[0.00000000e+000 0.00000000e+000 1.09722105e-314 2.12000655e-298\n",
      " 1.50690472e-282 3.94039628e-267 3.79052640e-252 1.34141967e-237\n",
      " 1.74636626e-223 8.36395161e-210 1.47364613e-196 9.55169454e-184\n",
      " 2.27757748e-171 1.99788926e-159 6.44725997e-148 7.65392974e-137\n",
      " 3.34271444e-126 5.37056037e-116 3.17428155e-106 6.90202942e-097\n",
      " 5.52094836e-088 1.62463604e-079 1.75874954e-071 7.00418213e-064\n",
      " 1.02616307e-056 5.53070955e-050 1.09660656e-043 7.99882776e-038\n",
      " 2.14638374e-032 2.11881925e-027 7.69459863e-023 1.02797736e-018\n",
      " 5.05227108e-015 9.13472041e-012 6.07588285e-009 1.48671951e-006\n",
      " 1.33830226e-004 4.43184841e-003 5.39909665e-002 2.41970725e-001\n",
      " 3.98942280e-001 2.41970725e-001 5.39909665e-002 4.43184841e-003\n",
      " 1.33830226e-004 1.48671951e-006 6.07588285e-009 9.13472041e-012\n",
      " 5.05227108e-015 1.02797736e-018 7.69459863e-023 2.11881925e-027\n",
      " 2.14638374e-032 7.99882776e-038 1.09660656e-043 5.53070955e-050\n",
      " 1.02616307e-056 7.00418213e-064 1.75874954e-071 1.62463604e-079\n",
      " 5.52094836e-088 6.90202942e-097 3.17428155e-106 5.37056037e-116\n",
      " 3.34271444e-126 7.65392974e-137 6.44725997e-148 1.99788926e-159\n",
      " 2.27757748e-171 9.55169454e-184 1.47364613e-196 8.36395161e-210\n",
      " 1.74636626e-223 1.34141967e-237 3.79052640e-252 3.94039628e-267\n",
      " 1.50690472e-282 2.12000655e-298 1.09722105e-314 0.00000000e+000]\n",
      "(80,)\n",
      "[            nan             nan             inf 3.70269877e+001\n",
      " 3.60277351e+001 3.50285250e+001 3.40293611e+001 3.30302476e+001\n",
      " 3.20311893e+001 3.10321913e+001 3.00332597e+001 2.90344012e+001\n",
      " 2.80356238e+001 2.70369361e+001 2.60383486e+001 2.50398730e+001\n",
      " 2.40415232e+001 2.30433154e+001 2.20452686e+001 2.10474055e+001\n",
      " 2.00497531e+001 1.90523439e+001 1.80552178e+001 1.70584233e+001\n",
      " 1.60620210e+001 1.50660868e+001 1.40707176e+001 1.30760386e+001\n",
      " 1.20822142e+001 1.10894650e+001 1.00980932e+001 9.10852311e+000\n",
      " 8.12136811e+000 7.13754561e+000 6.15848260e+000 5.18650397e+000\n",
      " 4.22560714e+000 3.28309865e+000 2.37321553e+000 1.52513528e+000\n",
      " 7.97884561e-001 2.87599971e-001 5.52478627e-002 4.43783904e-003\n",
      " 1.33834464e-004 1.48671994e-006 6.07588286e-009 9.13472041e-012\n",
      " 5.05227108e-015 1.02797736e-018 7.69459863e-023 2.11881925e-027\n",
      " 2.14638374e-032 7.99882776e-038 1.09660656e-043 5.53070955e-050\n",
      " 1.02616307e-056 7.00418213e-064 1.75874954e-071 1.62463604e-079\n",
      " 5.52094836e-088 6.90202942e-097 3.17428155e-106 5.37056037e-116\n",
      " 3.34271444e-126 7.65392974e-137 6.44725997e-148 1.99788926e-159\n",
      " 2.27757748e-171 9.55169454e-184 1.47364613e-196 8.36395161e-210\n",
      " 1.74636626e-223 1.34141967e-237 3.79052640e-252 3.94039628e-267\n",
      " 1.50690472e-282 2.12000655e-298 1.09722105e-314 0.00000000e+000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " C:\\Users\\kevin\\AppData\\Local\\conda\\conda\\envs\\graph_analytics\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning:divide by zero encountered in true_divide\n",
      " C:\\Users\\kevin\\AppData\\Local\\conda\\conda\\envs\\graph_analytics\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning:invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAGTlJREFUeJzt3WtwXOd93/Hvn7jfCJAEJPEOZkw7Yl2LUmBGiRJXji+hNB3xRZ2WbJyorcZ6Y9VJ7dYjTzpqo75pk04cZyrHYVPFl2mkyrIbsyob2aPI004SyYTuFGnaDK8gQREAL1gABLAL/PvinKVWywX2AFxgz7P4fWYw2HP2YPc/2IPfPHjOc57H3B0REaktq6pdgIiIVJ7CXUSkBincRURqkMJdRKQGKdxFRGqQwl1EpAYp3EVEapDCXUSkBincRURqUH213ri7u9t7e3ur9fYiIkF65ZVXht29p9xxVQv33t5e+vv7q/X2IiJBMrPTSY5Tt4yISA1SuIuI1CCFu4hIDVK4i4jUIIW7iEgNKhvuZvakmV00s8NzPG9m9kdmdtzM3jSzuypfpoiILESSlvvXgd3zPH8fsD3+ehj445svS0REbkbZce7u/n/NrHeeQ/YA3/Rovb6XzKzLzNa7+2CFahRJvZGxKd46d5Wr17LR10SW7KyDlrGUEj52+63csblrSd+jEjcxbQTOFmwPxPtuCHcze5iodc+WLVsq8NYi1XdqeJxPfe1vGB6bLvm82TIXJKl3y+rmIMK91Klbsrni7vuB/QB9fX1q0kjwLo5O8htPvszMrPONf7GLjV0tdLY00NnSQGO9xitI9VQi3AeAzQXbm4DzFXhdkVS7ei3Lg392iJGxaf78M3ezc4lbYiILUYmmxQHgN+NRM3cDV9XfLrVuMjvDZ77Zz/GLGb726Z9TsEvqlG25m9lTwL1At5kNAP8OaABw968BB4H7gePABPDPl6pYkbT4gx/8hB+dvMRX9u7kI+8vO0GfyLJLMlpmX5nnHfhsxSoSCcCrpy+zq3cte3ZurHYpIiXpio/IIpwamaC3u7XaZYjMSeEuskCZySzDY1P0drdVuxSROSncRRbo9MgEANvWKdwlvRTuIgt0amQcQC13STWFu8gCnRqOwn3rOvW5S3op3EUW6OTwBLeubqK1sWpLEIuUpXAXWaBTI+P0qr9dUk7hLrJAp0fG2ab+dkk5hbvIAkTDIKfZqpa7pJzCXWQBTg3HwyB1A5OknMJdZAE0DFJCoXAXWYDrwyDXKtwl3RTuIgtwcmSc21Y309JYV+1SROalcBdZgNOaMEwCoXAXWYBTwxoGKWFQuIskNDqZZWRcwyAlDAp3kYROx8MgdXeqhEDhLpLQyXgYpLplJAQKd5GE8sMgt6zVBVVJP4W7SEKnhsdZ36lhkBIGhbtIQpoNUkKicBdJKFoUW+EuYVC4iyRw9VqWS+PT9Gr1JQmEwl0kgdOaMEwCo3AXSeDksIZBSlgU7iIJnL0U3cCkYZASCoW7SAJXJrK0NtbR3KBhkBIGhbtIApnJHB3N9dUuQyQxhbtIAmNTOdqbFO4SDoW7SAKjk1k6mhuqXYZIYonC3cx2m9kxMztuZo+WeH6Lmb1oZq+Z2Ztmdn/lSxWpnrEpdctIWMqGu5nVAU8A9wE7gH1mtqPosH8LPOPudwJ7ga9WulCRalKfu4QmSct9F3Dc3U+4+zTwNLCn6BgHVsePO4HzlStRpPoyk1k6mtQtI+FI0hTZCJwt2B4Afr7omH8PfN/M/iXQBny8ItWJpMSYWu4SmCQtdyuxz4u29wFfd/dNwP3At8zshtc2s4fNrN/M+oeGhhZerUgVzMw649MztCvcJSBJwn0A2FywvYkbu10eAp4BcPe/BZqB7uIXcvf97t7n7n09PT2Lq1hkmY1N5gA0WkaCkiTcDwHbzWybmTUSXTA9UHTMGeBjAGZ2O1G4q2kuNSEzlQWgQ+PcJSBlw93dc8AjwPPAUaJRMW+b2eNm9kB82BeAz5jZG8BTwD9z9+KuG5EgZa633BXuEo5EZ6u7HwQOFu17rODxEeCeypYmkg5jU+qWkfDoDlWRMjKTUbeMLqhKSBTuImWoW0ZCpHAXKeN6uOuCqgRE4S5SRkZDISVACneRMjKTWepXGc0N+nORcOhsFSljbCpHe3M9ZqVu1hZJJ4W7SBmaEVJCpHAXKSMzmdOMkBIchbtIGZnJrMa4S3AU7iJlZCZzrFa4S2AU7iJlaHFsCZHCXaSMjBbHlgAp3EXm4e5aHFuCpHAXmcdUbpbsjOuCqgRH4S4yj9F4Rkh1y0hoFO4i88gvsafRMhIahbvIPPKThmm0jIRG4S4yD80IKaFSuIvMYyxeHFstdwmNwl1kHqNahUkCpXAXmce7F1TVLSNhUbiLzCPf597WVFflSkQWRuEuMo/MZJbWxjrq6/SnImHRGSsyD009IKFSuIvMIzOpGSElTAp3kXmMakZICZTCXWQe6paRUCncReahxbElVAp3kXmMaXFsCZTCXWQeWhxbQqVwF5nDzKwzPj2jbhkJUqJwN7PdZnbMzI6b2aNzHPOPzeyImb1tZn9e2TJFlt/YlKb7lXCVPWvNrA54AvgEMAAcMrMD7n6k4JjtwJeAe9z9spndslQFiyyXTLwKk+aVkRAlabnvAo67+wl3nwaeBvYUHfMZ4Al3vwzg7hcrW6bI8stoRkgJWJJw3wicLdgeiPcVej/wfjP7azN7ycx2l3ohM3vYzPrNrH9oaGhxFYssk+vdMgp3CVCScLcS+7xoux7YDtwL7AP+1My6bvgh9/3u3ufufT09PQutVWRZZbQ4tgQsSbgPAJsLtjcB50sc8z13z7r7SeAYUdiLBEvdMhKyJOF+CNhuZtvMrBHYCxwoOuYvgI8CmFk3UTfNiUoWKrLcroe7RstIgMqGu7vngEeA54GjwDPu/raZPW5mD8SHPQ+MmNkR4EXg37j7yFIVLbIctDi2hCxRk8TdDwIHi/Y9VvDYgc/HXyI1YWwqS90qo7lB9/pJeHTWiswhP2mYWakxBSLppnAXmcOYZoSUgCncReYwOpmjXTNCSqAU7iJzyExm1XKXYCncReYwNpVjtcJdAqVwF5mDFseWkCncReaQ0eLYEjCFu0gJ7s7YVE6ThkmwFO4iJUzlZsnOuC6oSrAU7iIlaOoBCZ3CXaSE69P96oKqBErhLlKCpvuV0CncRUrQ4tgSOoW7SAlahUlCp3AXKWFU3TISOIW7SAljCncJnMJdpIT8BVX1uUuoFO4iJYxNZWltrKO+Tn8iEiaduSIlaNIwCZ3CXaSEjFZhksAp3EVKyEzlaNcwSAmYwl2khMxkVgt1SNAU7iIlqFtGQqdwFylhTBdUJXAKd5EStAqThE7hLlJkZtYZn55Ry12CpnAXKZKfEVJ97hIyhbtIkfyMkKvVLSMBU7iLFLk+l7ta7hIwhbtIEa3CJLUgUbib2W4zO2Zmx83s0XmO+5SZuZn1Va5EkeU1phkhpQaUDXczqwOeAO4DdgD7zGxHieM6gM8BL1e6SJHlNKpVmKQGJGm57wKOu/sJd58Gngb2lDjuPwC/B0xWsD6RZZfvltH0AxKyJOG+EThbsD0Q77vOzO4ENrv7cxWsTaQqxqZy9HCZNf/vMchNV7sckUVJEu5WYp9ff9JsFfBl4AtlX8jsYTPrN7P+oaGh5FWKLKPMZJb76vtpOPQncOGtapcjsihJwn0A2FywvQk4X7DdAXwQ+KGZnQLuBg6Uuqjq7vvdvc/d+3p6ehZftcgSykzm2FZ/Kdq4eqa6xYgsUpJwPwRsN7NtZtYI7AUO5J9096vu3u3uve7eC7wEPODu/UtSscgSG5vMsaVuONq4onCXMJUNd3fPAY8AzwNHgWfc/W0ze9zMHljqAkWW2+hkjo3E3YYKdwlUouEA7n4QOFi077E5jr335ssSqZ6xqSy3+cVoQ+EugdIdqiJFpq6N0zV7JdpQuEugFO4iRdquDcYPeuDKWXCf/wdEUkjhLlKkazoO9633QHYcJi5VtyCRRVC4ixRwd9Zm43Dv/aXo+5XT1StIZJEU7iIFpnKzrPchZqweNn042ql+dwmQwl2kQGYyx0YbZrx5PazpjXYq3CVACneRApnJLJtsiMm2jdDSBc2dcPVs+R8USRmFu0iBsakcm2yYbMemaEfnFrXcJUgKd5ECY+Pj3GJXmO2Mp1PqUrhLmBTuIgVyl6KRMda1JdqRD3eNdZfAKNxFCsWt9IZ1vdF21xaYHoNrl6tXk8giKNxFCtTFF0+be7ZFO7ri7hl1zUhgFO4iBRrHz5H1OlrXxRdU890zCncJjMJdpEDL+DkusI6GhnhxbIW7BErhLlKgffI8F1bd8u6O5i5oWq1wl+Ao3EUKdE1dYLju1nd3mEHnZoW7BEfhLpKXm6JrZpjLjevfu79ri+5SleAo3EXyrg4AMNpUItw11l0Co3AXyYu7XiZaN7x3f9cWmBqFyStVKEpkcRTuInlxuF9r3fje/RrrLgFSuIvkXTlDzlcx21GiWyZ+XiQUCneR2OyVMwz6Otpbmt/7RNfW6PsVXVSVcCjcRWKzl88w4D10NNe/94mWNdDYrpa7BEXhLpJ35Qzn6L4x3M009a8ER+EuApCbpm5skAHvpr2p4cbndSOTBEbhLgIwOoDhpbtlQC13CY7CXQTg6jkAzvu60uHeuQmmrsJUZpkLE1kchbsIQGYQgHd8Telwzw+PHB1cxqJEFk/hLgLXw/2Cr6WjuUSf++r17zlOJO0U7iIAo4NM17UxTgvtTaVa7vGUBAp3CYTCXQQgM0imoZu6VUZrY92Nz3fcdv04kRAkCncz221mx8zsuJk9WuL5z5vZETN708xeMLOtlS9VZAllBrla3017Uz1mduPzTe3Roh3qc5dAlA13M6sDngDuA3YA+8xsR9FhrwF97v4h4Fng9ypdqMiSygxyadXa0l0yeR23qeUuwUjSct8FHHf3E+4+DTwN7Ck8wN1fdPeJePMlYFNlyxRZQu6QucCQzTEMMq9jvcJdgpEk3DcChTMmDcT75vIQ8H9KPWFmD5tZv5n1Dw0NJa9SZClNXIKZad7xrgThfmH56hK5CUnCvUQHJCWXpDGzTwN9wO+Xet7d97t7n7v39fT0JK9SZCllzgNwbnZN6WGQeavjlvvs7DIVJrJ4ScJ9ANhcsL0JOF98kJl9HPgd4AF3n6pMeSLLIL5Iei7XWb7lPpuDieFlKkxk8ZKE+yFgu5ltM7NGYC9woPAAM7sT+BOiYL9Y+TJFllDcj35qurPMBVXdyCThKBvu7p4DHgGeB44Cz7j722b2uJk9EB/2+0A78G0ze93MDszxciLpkw/3qfb5u2U0BYEEZJ5myrvc/SBwsGjfYwWPP17hukSWT2YQb+thYqRu/m4ZTUEgAdEdqiKjg+TaojtQ5w339lsBU7hLEBTuIplBsi23AmXCva4B2noU7hIEhbtIZpCJ5mhobkepVZgKrV6vPncJgsJdVraZLIwPMWxrAdi8tnX+43UjkwRC4S4rWxzUA7k1AGxdlyTcb7jNQyR1FO6yssXhfmKqgw2dzTQ3lJjut1DHepgYgZzu05N0U7jLyha3wo+OtbF1XVv5468Ph1TXjKSbwl1Wtvji6BtXmuntThDuuktVAqFwl5UtM4ivauDEtRa2dZfpbweFuwRD4S4rW2aQbGt0c1Jvkm4ZTUEggVC4y8qWGSTTGI1xT9Qt07oW6hrVcpfUU7jLyjY6yIitxQy2lBvjDmCm5fYkCAp3WdkyFxic7WJDZ0v5YZB5HRs0WkZST+EuK9dUBqYznJzuLH/zUqHV62FUNzJJuincZeWKW98/mWhP1t+el5+CwEuuNimSCgp3Wbni1veJqdVsSzJSJq9jPWTHYWp0iQoTuXkKd1m54pb7O75m4S33gp8XSSOFu6xc8dQD7/gaehfa5w7qd5dUU7jLyjU6yFRdG9esufxUv4V0l6oEQOEuK1dmkMt13QsbBgnROPf450XSSuEuK1dmkHe8i20L6W8HaGyDpk5NQSCppnCXlStzgdPZroWNcc9bvV4td0k1hbusTLOzeGaQgVznwlvuoCkIJPUU7rIynX8Nm81xwtcnmw2yWPf74Z0jMHm18rWJVIDCXVamV79Orq6Fv5z5ML1J5nEvdsdeyF2Dt56tfG0iFaBwl5VnKgNvfYcfr/sEE9a6sGGQeRvuglv/Prz6jcrXJ1IBCndZeQ5/F7LjfL/5V9nQ1UJT/QKGQeaZwV2/CYNvwPnXK1+jyE1SuMvK8+o3oOd2fji+dXEXU/M+9GtQ3wyvfrNytYlUiMJdVpYLh+HcK/hdv8HJkYnFDYPMa1kDO/bAW9+G6fHK1ShSAQp3WVle/SbUNfKd3C+Tmczxs7etvrnXu+vBaHbII9+rTH0iFZIo3M1st5kdM7PjZvZoieebzOx/xM+/bGa9lS5U5KZlr8GbT3N+wyf44sGzfPQDPfyTD2++udfc+ouw7n3wii6sSrqUDXczqwOeAO4DdgD7zGxH0WEPAZfd/X3Al4H/VOlCRW7akQMweZVHT97JHZu7eOLX76Kh7ib/ec1fWD37Elz8cWXqFKmAJGf2LuC4u59w92ngaWBP0TF7gHzT5VngY2ZmlStTZJFy0zDQz8zffJXJF/4jZ/xWBtf08eSDH6a1sb4y73HHP4VVDfC/Px+14C8ehdnZyry2yCIlObs3AmcLtgeAn5/rGHfPmdlVYB0wXIkiCx367lfoOfxfK/2yUiMMMGZZ5Y4xS7eP0ESWOuCCr+OrDQ/xjYfuZk1bY+XetL0H7n0U/va/wP/6XLSvaTW0dUcV2ar4S+0dif2DL8IH/9GSvkWScC91RhYvHpnkGMzsYeBhgC1btiR46xvVt6/jUuu2Rf2srAyz1OEYmPGTxh6GOj/E6Lqd1Hdt4HN/7zY2dLVU/k0/8q/hl78AI38HA4fgXH80NYHPRmut+kzl31PC1dy15G+RJNwHgMKrTpuA4iVo8scMmFk90AlcKn4hd98P7Afo6+tb1OrCd37y0/DJTy/mR0WWlhl0vy/62rmv2tXICpekz/0QsN3MtplZI7AXOFB0zAHgwfjxp4C/ctfS8CIi1VK25R73oT8CPA/UAU+6+9tm9jjQ7+4HgP8GfMvMjhO12PcuZdEiIjK/RMMF3P0gcLBo32MFjyeBX6tsaSIisli6Q1VEpAYp3EVEapDCXUSkBincRURqkMJdRKQGWbWGo5vZEHB6kT/ezRJMbVAhaa0trXVBemtLa12Q3trSWhfUTm1b3b2n3EFVC/ebYWb97t5X7TpKSWttaa0L0ltbWuuC9NaW1rpg5dWmbhkRkRqkcBcRqUGhhvv+ahcwj7TWlta6IL21pbUuSG9taa0LVlhtQfa5i4jI/EJtuYuIyDyCC/dyi3Uvcy1PmtlFMztcsG+tmf3AzH4af19Thbo2m9mLZnbUzN42s99KQ21m1mxmPzKzN+K6fjfevy1eWP2n8ULrFVwmacE11pnZa2b2XFpqM7NTZvaWmb1uZv3xvqqfZ3EdXWb2rJn9OD7ffqHatZnZB+LfVf5r1Mx+u9p1FdT3r+Lz/7CZPRX/XVT8PAsq3BMu1r2cvg7sLtr3KPCCu28HXoi3l1sO+IK73w7cDXw2/j1Vu7Yp4Ffc/Q5gJ7DbzO4mWlD9y3Fdl4kWXK+W3wKOFmynpbaPuvvOguFy1f4s874C/KW7/yxwB9Hvrqq1ufux+He1E/g5YAL4n9WuC8DMNgKfA/rc/YNE06jvZSnOM3cP5gv4BeD5gu0vAV+qck29wOGC7WPA+vjxeuBYCn5v3wM+kabagFbgVaL1eIeB+lKf8TLXtInoj/5XgOeIlo+sem3AKaC7aF/VP0tgNXCS+NpdmmorqOWTwF+npS7eXW96LdGU688Bv7oU51lQLXdKL9a9sUq1zOVWdx8EiL/fUs1izKwXuBN4mRTUFnd7vA5cBH4A/B1wxd1z8SHV/Ez/EPgiMBtvryMdtTnwfTN7JV6HGFLwWQI/AwwBfxZ3Zf2pmbWlpLa8vcBT8eOq1+Xu54D/DJwBBoGrwCsswXkWWrgnWohbImbWDnwH+G13H612PQDuPuPRv8ubgF3A7aUOW96qwMz+IXDR3V8p3F3i0Gqcb/e4+11E3ZGfNbOPVKGGUuqBu4A/dvc7gXGq1z10g7jf+gHg29WuJS/u598DbAM2AG1En2uxmz7PQgv3JIt1V9s7ZrYeIP5+sRpFmFkDUbD/d3f/bppqA3D3K8APia4JdMULq0P1PtN7gAfM7BTwNFHXzB+moTZ3Px9/v0jUd7yLdHyWA8CAu78cbz9LFPZpqA2i0HzV3d+Jt9NQ18eBk+4+5O5Z4LvAL7IE51lo4Z5kse5qK1ws/EGi/u5lZWZGtK7tUXf/g7TUZmY9ZtYVP24hOtGPAi8SLaxelboA3P1L7r7J3XuJzqu/cvdfr3ZtZtZmZh35x0R9yIdJwXnm7heAs2b2gXjXx4Ajaagtto93u2QgHXWdAe42s9b47zT/O6v8eVatCx03cUHifuAnRH21v1PlWp4i6jfLErViHiLqp30B+Gn8fW0V6volon/r3gRej7/ur3ZtwIeA1+K6DgOPxft/BvgRcJzoX+imKn+u9wLPpaG2+P3fiL/ezp/z1f4sC+rbCfTHn+lfAGvSUBvRBfsRoLNgX9Xriuv4XeDH8d/At4CmpTjPdIeqiEgNCq1bRkREElC4i4jUIIW7iEgNUriLiNQghbuISA1SuIuI1CCFu4hIDVK4i4jUoP8PiLkean7GbCwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "num = np.arange(-39,40);print(num)\n",
    "plt.figure(1)\n",
    "a1 = norm.cdf(num);print(a1)\n",
    "a2 = norm.pdf(num);print(a2)\n",
    "plt.plot(a1)\n",
    "plt.plot(a2)\n",
    "plt.figure(2)\n",
    "r = norm.pdf(num)/norm.cdf(num)\n",
    "plt.plot(r);print(r.shape)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4736461348785476e-196"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.pdf(-30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aa=np.random.randn(4,2);cc=np.ones((4,2))*10;print(aa)\n",
    "bb=np.arange(4)[:,None];print(bb)\n",
    "bb=np.tile(bb,[1,2]);print(bb.shape)\n",
    "term1=np.zeros((4,2))\n",
    "term1[bb!=0]=(aa[bb!=0]/cc[bb!=0])\n",
    "print(term1)\n",
    "# a1=np.where(bb==0,0,term1);print('a1',a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(np.linalg.eigvals(V) > 0)# check PSD condition for V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "  'a': lambda x: x * 5,\n",
    "  'b': lambda x: x + 7,\n",
    "  'c': lambda x: x - 2\n",
    "}[value](x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
