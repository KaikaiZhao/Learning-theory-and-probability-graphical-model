{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihoods we have used:\n",
    "1. regression: Gaussian\n",
    "2. count regression: \n",
    "    Poisson: lambda=e^f,  Poisson2: lambda=ln(1+e^f)\n",
    "3. binary classification: \n",
    "    Bernoulli: sigmoid,   Bernoulli2: Probit\n",
    "    \n",
    "For the sigmoid function, we use **expit** which is provided by *scipy.special*, because it is stable, fast and fairly accurate. Additionally, it is equivalent to sigmoid. For all methods, initial variational parameters are found by running the Laplace approximation on the subset/active set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv,norm,lstsq,cholesky\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy.io as sio\n",
    "from sklearn import preprocessing\n",
    "import random,time,GPy\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import loggamma,roots_hermitenorm,gamma,expit\n",
    "from scipy.special import ndtr as std_norm_cdf\n",
    "\n",
    "_sqrt_2pi = np.sqrt(2*np.pi)\n",
    "_lim_val = np.finfo(np.float64).max\n",
    "_lim_val_exp = np.log(_lim_val)\n",
    "\n",
    "def std_norm_pdf(x): # define a standard normal pdf(from GPy)\n",
    "    x = np.clip(x,-1e300,1e300)\n",
    "    return np.exp(-np.square(x)/2)/_sqrt_2pi\n",
    "\n",
    "def safe_exp(f):\n",
    "    clip_f = np.clip(f, -np.inf, _lim_val_exp)\n",
    "    return np.exp(clip_f)\n",
    "\n",
    "def safe_ln(x, minval=0.0000000001):\n",
    "    return np.log(x.clip(min=minval))\n",
    "\n",
    "def rbf_kernel(X1, X2, l=1.0, sigma_f=1.0):\n",
    "    '''\n",
    "    Isotropic squared exponential kernel. Computes \n",
    "    a covariance matrix from points in X1 and X2.    \n",
    "    Args:\n",
    "        X1: Array of m points (m x d).\n",
    "        X2: Array of n points (n x d).\n",
    "\n",
    "    Returns:\n",
    "        Covariance matrix (m x n).\n",
    "    '''\n",
    "    sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n",
    "    return sigma_f**2 * np.exp(-0.5 / l**2 * sqdist)\n",
    "\n",
    "def kernel(X1, X2, *args, **kwargs):\n",
    "    return rbf_kernel(X1, X2, *args, **kwargs)\n",
    "\n",
    "def MFE(true_labels, pred_labels):\n",
    "    '''\n",
    "    Calculating mean fraction error(MFE) for count regression, the math is used as follows:\n",
    "    MFE = mean( abs( (true_labels - pred_labels)./true_labels ) ) \n",
    "    Written by Kaikai\n",
    "    '''    \n",
    "    if true_labels.size != pred_labels.size:\n",
    "        print('The size of true_labels and pred_labels is supposed to be identical.')\n",
    "        return -1    \n",
    "    true_labels = true_labels.flatten().astype('double'); pred_labels = pred_labels.flatten().astype('double')\n",
    "    true_labels_temp = true_labels.copy()\n",
    "    if 0 in true_labels: # replace zero with a very small positive value in order to avoid dividing by zero\n",
    "        print('There are elements of zero value in true labels.')\n",
    "        true_labels_temp[true_labels==0] = 1\n",
    "    \n",
    "    MFE = np.mean( np.abs( (true_labels - pred_labels)/true_labels_temp ) )\n",
    "    return MFE\n",
    "\n",
    "def calc_vlb(m,V, a, lik='Gaussian'):\n",
    "    prior_mean_u = a[0]; prior_mean_f = a[1] # prior mean for inducing points    \n",
    "    A = a[2] # Knm*inv(Kmm)\n",
    "    Kmm = a[3]; Kmm_inv = a[4]; Kmn = a[5]; Knn_diag = a[6]; y = a[7] # the ground truth for training data\n",
    "    noise_var = a[8]\n",
    "    num_train = len(prior_mean_f);num_inducing = len(prior_mean_u)\n",
    "    m_q = prior_mean_f + np.dot(A, (m-prior_mean_u)) # Eq.(3a) in paper\n",
    "    v_q = ( Knn_diag.ravel() + np.diag(np.dot(A, np.dot(V-Kmm, A.T))) )[:, None] # Eq.(3b) in paper\n",
    "    c1 = m - prior_mean_u; c2 = np.dot(Kmm_inv, c1)\n",
    "    (Sign,LogDetKmm) = np.linalg.slogdet(Kmm); LogDetKmm = Sign*LogDetKmm\n",
    "    (SignV,LogDetV) = np.linalg.slogdet(V); LogDetV = SignV*LogDetV;#print(v_q[:50])     \n",
    "    if lik=='Bernoulli':\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q));#vlb_lik = expit(y*f);print('yf',expit(y*f))\n",
    "        vlb_lik = np.sum( 1/_sqrt_2pi*np.dot( safe_ln(expit(y*f)) ,w) ) # sigmoid liklihood\n",
    "#         vlb_lik = np.sum( 1.0/np.sqrt(2*np.pi)*np.dot( np.log(std_norm_cdf(y*f)+1e-10) ,w) ) # Probit liklihood\n",
    "    elif lik=='Gaussian':\n",
    "        vlb_lik = -np.log(np.sqrt(2*np.pi*noise_var)) - np.sum((y-m_q)**2+v_q)/(2*noise_var)\n",
    "    elif lik=='Poisson':\n",
    "        vlb_lik = np.dot(y.T,m_q) - np.sum(loggamma(y+1)) - np.sum(np.exp(m_q+0.5*v_q))\n",
    "    elif lik=='Poisson2': # another link func: lambda=ln(1+e^f)\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q));\n",
    "        term1 = -np.sum(loggamma(y+1)); term2 = -np.sum( 1/_sqrt_2pi*np.dot( np.log(1+safe_exp(f)),w) )\n",
    "        term3 = np.sum( 1/_sqrt_2pi*np.dot( y*safe_ln(np.log(1+safe_exp(f))),w) )\n",
    "        vlb_lik = term1 + term2 + term3\n",
    "    vlb_kl = 0.5*( LogDetKmm - LogDetV + np.dot(c1.T, c2) + np.trace(np.dot(Kmm_inv,V)) - len(prior_mean_u) )\n",
    "    vlb = vlb_lik - vlb_kl\n",
    "    return vlb \n",
    "\n",
    "def get_init_hyperparameters(Z,Z_label,lik='Poisson'):\n",
    "    # For all methods, initial variational parameters are found by running the Laplace approximation on the subset/active set.\n",
    "    dim_data = Z.shape[1]\n",
    "    kern = GPy.kern.RBF(dim_data, variance=1.0, lengthscale=1.0)\n",
    "    likelihood = {\n",
    "          'Poisson': GPy.likelihoods.Poisson(),\n",
    "          'Poisson2': GPy.likelihoods.Poisson(GPy.likelihoods.link_functions.Log_ex_1()),\n",
    "          'Gaussian': GPy.likelihoods.Gaussian(),\n",
    "          'Bernoulli': GPy.likelihoods.Bernoulli()\n",
    "        }[lik]\n",
    "    laplace_inf = GPy.inference.latent_function_inference.Laplace()\n",
    "    model_lap = GPy.core.GP(X=Z, Y=Z_label, likelihood=likelihood, inference_method=laplace_inf, kernel=kern)\n",
    "    model_lap.optimize()\n",
    "#     print(model_lap)\n",
    "    return model_lap\n",
    "\n",
    "def Adam(theta, g_t, t, alpha=0.01, m_t=0, v_t=0, opt='minimize'):\n",
    "    beta_1 = 0.9; beta_2 = 0.999; epsilon = 1e-8     #initialize the values of the parameters    \n",
    "    m_t = beta_1*m_t + (1-beta_1)*g_t                #updates the moving averages of the gradient\n",
    "    v_t = beta_2*v_t + (1-beta_2)*(g_t*g_t)          #updates the moving averages of the squared gradient\n",
    "    m_cap = m_t/(1-(beta_1**t))                      #calculates the bias-corrected estimates\n",
    "    v_cap = v_t/(1-(beta_2**t))                      #calculates the bias-corrected estimates\n",
    "    if opt=='maximize':\n",
    "        theta = theta + (alpha*m_cap)/(np.sqrt(v_cap)+epsilon)    #updates the parameters\n",
    "    else:\n",
    "        theta = theta - (alpha*m_cap)/(np.sqrt(v_cap)+epsilon)    #updates the parameters\n",
    "    return theta, m_t, v_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GH_quad(mu_star,std_star): # gauss_hermite_quad to calculate numerical integration, w and z denote weights and sample points, respectively. \n",
    "    z,w = roots_hermitenorm(n=50, mu=False); z,w=z[:,None],w[:,None]\n",
    "    z = np.kron(std_star,z.T) + mu_star\n",
    "    return z,w\n",
    "\n",
    "def predict(X, xStar, l, sigma2, m, V,Kmm,Kmm_inv,y_min=4,y_max=20,noise_var=0.1,lik='Poisson'):\n",
    "    num_xStar = xStar.shape[0]; Kmn = kernel(X, xStar, l, np.sqrt(sigma2))\n",
    "    Knn_diag = np.eye(num_xStar)*sigma2+noise_var*np.eye(num_xStar)\n",
    "    A = np.dot(Kmn.T, Kmm_inv)\n",
    "    mu_star = np.dot(A,m); v2_star = Knn_diag + np.dot(A, np.dot(V-Kmm,A.T))\n",
    "    std_star = np.sqrt(np.diag(v2_star))[:,None]\n",
    "    if lik=='Bernoulli':\n",
    "        return calc_Bernoulli_pred(mu_star,std_star)\n",
    "    elif lik=='Gaussian':\n",
    "        return calc_Gauss_pred(mu_star,std_star,noise_var)\n",
    "    else: # Poisson, Poisson2\n",
    "        return calc_Poisson_pred(mu_star,std_star,num_xStar,y_min,y_max,lik)\n",
    "\n",
    "def calc_Gauss_pred(mu_star,std_star,noise_var):\n",
    "    return mu_star, std_star+np.sqrt(noise_var)\n",
    "\n",
    "def calc_Poisson_pred(mu_star,std_star,num_xStar,y_min,y_max,lik='Poisson'):\n",
    "    f,w = GH_quad(mu_star,std_star)\n",
    "    y_range = np.arange(y_min,y_max+1)\n",
    "    lik_func = {\n",
    "        'Poisson': lambda f,y: 1/gamma(y+1)*safe_exp(-safe_exp(f)+f*y),\n",
    "        'Poisson2': lambda f,y: 1/gamma(y+1)*expit(-f)*( safe_ln(1.0+safe_exp(f))**y )\n",
    "    }[lik]\n",
    "    poisson_lik = np.zeros((num_xStar,len(y_range)))\n",
    "    for i,y in enumerate(y_range):\n",
    "        poisson_lik[:,i] = 1/_sqrt_2pi*np.dot(lik_func(f,y),w).ravel()\n",
    "    return poisson_lik\n",
    "\n",
    "def calc_Bernoulli_pred(mu_star,std_star):\n",
    "#     v = std_star**2; kappa_v = (1+np.pi*v/8.0)**(-1/2) # Probit liklihood\n",
    "#     p = sigmoid(kappa_v*mu_star) # p(y=1|x_*,m,V)\n",
    "    f,w = GH_quad(mu_star,std_star) # sigmoid liklihood\n",
    "    p = 1/np.sqrt(2*np.pi)*np.dot( expit(f), w ) # p(y=1|x_*,m,V)\n",
    "    return p\n",
    "\n",
    "def calc_m_q(m, A, prior_mean_u, prior_mean_f):\n",
    "    return prior_mean_f + np.dot(A, (m-prior_mean_u))\n",
    "\n",
    "def calc_v_q(V, A, Kmm, Knn_diag):\n",
    "    return ( Knn_diag.ravel() + np.diag(np.dot(A, np.dot(V-Kmm, A.T))) )[:,None] # Eq.(3b) in paper\n",
    "\n",
    "def calc_rho(m_q, v_q, y, lik='Poisson', noise_var=0.1):\n",
    "    if lik=='Bernoulli':\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q))\n",
    "        return 1.0/_sqrt_2pi*np.dot(  y*expit(-y*f) , w ) # sigmoid liklihood\n",
    "#         return 1.0/_sqrt_2pi*np.dot( y*std_norm_pdf(f) / (std_norm_cdf(y*f)+1e-10), w ) # Probit liklihood        \n",
    "    elif lik=='Gaussian':\n",
    "        return 1.0/noise_var*(y-m_q)\n",
    "    elif lik=='Poisson':\n",
    "        return -np.exp(m_q + 0.5*v_q) + y\n",
    "    elif lik=='Poisson2':\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q))\n",
    "        y = np.tile(y,[1,f.shape[1]]); term2 = expit(f); t0 = safe_ln(1+safe_exp(f))# avoid dividing by zeros\n",
    "        d1_log_lik = np.zeros(t0.shape)\n",
    "        d1_log_lik[t0!=0] = (y[t0!=0]/t0[t0!=0] - 1)*term2[t0!=0]\n",
    "        return 1.0/_sqrt_2pi*np.dot(  d1_log_lik, w )\n",
    "    \n",
    "def calc_lambda(m_q, v_q, y, lik='Poisson', noise_var=0.1):\n",
    "    if lik=='Bernoulli':\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q))\n",
    "        return 1.0/np.sqrt(2*np.pi)*np.dot(  -expit(y*f)*expit(-y*f), w ) # sigmoid\n",
    "#         return 1.0/_sqrt_2pi*np.dot( -std_norm_pdf(f)**2 / (std_norm_cdf(y*f)**2+1e-10) - y*f*std_norm_pdf(f) / (std_norm_cdf(y*f)+1e-10), w )\n",
    "    elif lik=='Gaussian':\n",
    "        return -1.0/noise_var*np.ones((len(m_q),1))\n",
    "    elif lik=='Poisson':\n",
    "        return -np.exp(m_q + 0.5*v_q)\n",
    "    elif lik=='Poisson2':\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q));y = np.tile(y,[1,f.shape[1]]);\n",
    "        term2 = expit(f)*expit(f); t0 = safe_ln(1+safe_exp(f));d2_log_lik = np.zeros(t0.shape)\n",
    "        d2_log_lik[t0!=0] = ((y[t0!=0]/t0[t0!=0]-1)*safe_exp(-f[t0!=0])-y[t0!=0]/(t0[t0!=0])**2)*term2[t0!=0]\n",
    "        return 1.0/_sqrt_2pi*np.dot(  d2_log_lik, w )\n",
    "\n",
    "def dVLb_dm(m, rho, prior_mean_u, Kmm_inv, A):\n",
    "    dm = np.dot(A.T,rho) - np.dot(Kmm_inv, m-prior_mean_u) # Eq.(11a) in paper\n",
    "    return dm\n",
    "\n",
    "def dVLb_dL(L, lam, Kmm_inv, A, noise_var=1e-8):# optimizing the cholesky factor L guarantees the PSD of V automatically\n",
    "    dL = np.dot( np.dot( np.dot(A.T, np.diag(lam.ravel())), A ), L ) + inv(L+np.sqrt(noise_var)*np.eye(len(L))).T - np.dot(Kmm_inv, L)\n",
    "    return dL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(X,Y,ix,X_test=None,y_test=None,max_iter=100,lr=1*10**-5,FPb_cond=1*10**0,stop_cond=1*10**-2,VLB_opt='GD',lik='Poisson'):\n",
    "    num_train = X.shape[0];num_inducing=len(ix);num_test=X_test.shape[0]; \n",
    "    prior_mean_u = np.zeros((num_inducing, 1)); prior_mean_f = np.zeros((num_train, 1))\n",
    "    Z = X[ix, :]; Z_label = Y[ix] # Z is the inducing set\n",
    "    model_lap = get_init_hyperparameters(Z,Z_label,lik=lik)#(Z_label+1)/2    \n",
    "    # variational parameters from initialization\n",
    "    f_mean, f_var = model_lap._raw_predict(X) \n",
    "    length_scale = model_lap.rbf.lengthscale[0]; sigma2 = model_lap.rbf.variance[0];\n",
    "    if lik=='Gaussian': noise_var=model_lap.Gaussian_noise.variance[0] \n",
    "    else: noise_var=1*10**-8\n",
    "    #  we use variational mean from Laplace appr\n",
    "    m = f_mean[ix]; V = np.diag(f_var[ix].ravel()); L = cholesky(V)\n",
    "    Kmm = kernel(Z, Z, l = length_scale, sigma_f = np.sqrt(sigma2)) + noise_var*np.eye(len(Z))\n",
    "    Kmm_inv = inv(Kmm); Kmn = kernel(Z, X, l = length_scale, sigma_f = np.sqrt(sigma2)); Knn_diag = sigma2*np.ones((num_train, 1))\n",
    "    A = np.dot(Kmn.T, Kmm_inv) # Knm*inv(Kmm)    \n",
    "    a = (prior_mean_u,prior_mean_f,A,Kmm,Kmm_inv,Kmn,Knn_diag,Y,noise_var)\n",
    "    num_iter = 0; VLB = []; VLB_time = []; err = []; FPb_converge = True\n",
    "    var = np.hstack([m.flatten(), L.flatten()])\n",
    "    start_time = time.time(); VLB.append(-calc_vlb(m,V, a,lik)[0][0])\n",
    "    if lik=='Poisson' or lik=='Poisson2':\n",
    "        y_min = min(Y); y_max = max(Y)\n",
    "        poisson_lik = predict(Z, X_test, length_scale, sigma2, m, V,Kmm,Kmm_inv,y_min=y_min,y_max=y_max,noise_var=noise_var,lik=lik)\n",
    "        res = np.argmax(poisson_lik, axis=1)+y_min; err.append( MFE(y_test, res) )\n",
    "    elif lik=='Gaussian':\n",
    "        mu_star, std_star = predict(Z, X_test, length_scale, sigma2, m, V,Kmm,Kmm_inv,noise_var=noise_var,lik=lik)\n",
    "        err.append( 1/num_test*( np.sum( (y_test-mu_star)**2 ) ) )\n",
    "    elif lik=='Bernoulli':\n",
    "        p = predict(Z, X_test, length_scale, sigma2, m, V,Kmm,Kmm_inv,noise_var=noise_var,lik=lik); \n",
    "        res = np.where(p>=0.5,1,-1)\n",
    "        err.append( np.sum( np.where(y_test*res<0,1,0) )/num_test );\n",
    "    VLB_time.append(time.time()-start_time);print('Before iterations, err:{:.6f}'.format(err[-1]) )\n",
    "    while 1:\n",
    "        num_iter = num_iter +1;\n",
    "        m_q = calc_m_q(m, A, prior_mean_u, prior_mean_f); v_q = calc_v_q(V, A, Kmm, Knn_diag);#print('m_q:',m_q[:10])\n",
    "        # According to Table 1 in paper, expectations of the derivatives wrt N(f|m,v) for Possion likelihood\n",
    "        rho = calc_rho(m_q, v_q, Y,lik, noise_var); lam = calc_lambda(m_q, v_q,Y,lik, noise_var);        \n",
    "        if VLB_opt=='GD':\n",
    "            dm = dVLb_dm(m, rho, prior_mean_u, Kmm_inv, A)\n",
    "            dL = dVLb_dL(L, lam, Kmm_inv, A, noise_var)\n",
    "            gradients = np.hstack([dm.flatten(), dL.flatten()])\n",
    "            var, m_t, v_t = Adam(var,gradients,num_iter,alpha=lr,opt='maximize')\n",
    "            m = var[:num_inducing][:,None] # variantional mean\n",
    "            L = var[num_inducing:].reshape(num_inducing,num_inducing) # variational variance V=L*L.T\n",
    "            V = np.dot(L, L.T);\n",
    "            \n",
    "        elif VLB_opt=='FPi':\n",
    "            dm = dVLb_dm(m, rho, prior_mean_u, Kmm_inv, A)\n",
    "            m, m_t, v_t = Adam(m,dm,num_iter,alpha=lr,opt='maximize') # print('old V',V[:1,:1])\n",
    "            V = inv(Kmm_inv-np.dot( np.dot(A.T, np.diag(lam.ravel())), A ));\n",
    "        elif VLB_opt=='FPb':            \n",
    "            if FPb_converge:\n",
    "                dm = dVLb_dm(m, rho, prior_mean_u, Kmm_inv, A); #print('old m:',m[:1]);\n",
    "                m, m_t, v_t = Adam(m,dm,num_iter,alpha=lr,opt='maximize')\n",
    "            else:\n",
    "                V = inv(Kmm_inv-np.dot( np.dot(A.T, np.diag(lam.ravel())), A))\n",
    "                \n",
    "        elif VLB_opt=='FPi-mean':\n",
    "            d = A.T; gamma = -lam; print('old m:',m[:1]);            \n",
    "            m = np.dot(V, np.dot(d, rho + np.dot(A,m)*gamma) ); \n",
    "            V = inv(Kmm_inv-np.dot( np.dot(A.T, np.diag(lam.ravel())), A))\n",
    "\n",
    "        VLB.append(-calc_vlb(m,V, a,lik)[0][0]); VLB_time.append(time.time()-start_time)\n",
    "        if lik=='Poisson' or lik=='Poisson2':\n",
    "            poisson_lik = predict(Z, X_test, length_scale, sigma2, m, V,Kmm,Kmm_inv,y_min=y_min,y_max=y_max,noise_var=noise_var,lik=lik)\n",
    "            res = np.argmax(poisson_lik, axis=1)+y_min; err.append( MFE(y_test, res) )\n",
    "        elif lik=='Gaussian':\n",
    "            mu_star, std_star = predict(Z, X_test, length_scale, sigma2, m, V,Kmm,Kmm_inv,noise_var=noise_var,lik=lik)\n",
    "            err.append( 1/num_test*( np.sum( (y_test-mu_star)**2 ) ) )\n",
    "        elif lik=='Bernoulli':\n",
    "            p = predict(Z, X_test, length_scale, sigma2, m, V,Kmm,Kmm_inv,noise_var=noise_var,lik=lik); res = np.where(p>=0.5,1,-1)\n",
    "            err.append( np.sum( np.where(y_test*res<0,1,0) )/num_test );\n",
    "        if num_iter>0:\n",
    "            delta_vlb = abs(VLB[-1]-VLB[-2])\n",
    "            if VLB_opt=='FPb' and delta_vlb<=FPb_cond:\n",
    "                FPb_converge = not FPb_converge; print('m or V converged')\n",
    "            if num_iter>=5 and delta_vlb<=stop_cond:\n",
    "                print('After {} iterations it converged: delta_VLB:{:.6f}, VLB:{:.6f}, err:{:.6f}'.format(num_iter,delta_vlb,VLB[-1],err[-1]) );break                \n",
    "            if num_iter==max_iter:\n",
    "                print('It has reached the maximum number of iterations, i.e. {}, with delta_VLB:{:.6f}, VLB:{:.6f} and err:{:.6f}'.format(max_iter,delta_vlb,VLB[-1],err[-1]));break\n",
    "            if num_iter%20 == 0:\n",
    "                print('iter:{}, delta_VLB:{:.6f}, VLB:{:.6f}, err:{:.6f}'.format(num_iter, delta_vlb, VLB[-1], err[-1]));\n",
    "            \n",
    "    return Z,Z_label, VLB, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv, A,VLB_time,err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "FPim = Train(X_train,y_train,ix,X_test,y_test,max_iter=4,lr=2*10**-4,VLB_opt='FPi-mean',lik='Gaussian')\n",
    "Zs,Zs_label, VLB_FPim, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time_FPim,err_FPim=FPim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GD = Train(X_train,y_train,ix,X_test,y_test,max_iter=800,lr=4*10**-4,VLB_opt='GD',lik='Bernoulli');\n",
    "Zs,Zs_label, VLB, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time,err = GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FPb = Train(X_train,y_train,ix,X_test,y_test,max_iter=40,lr=1*10**-4,VLB_opt='FPb',lik='Bernoulli')\n",
    "Zs,Zs_label, VLB_FPb, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time_FPb,err_FPb = FPb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPi = Train(X_train,y_train,ix,X_test,y_test,max_iter=40,lr=1*10**-4,VLB_opt='FPi',lik='Bernoulli');\n",
    "Zs,Zs_label, VLB_FPi, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time_FPi,err_FPi = FPi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GD2 = Train(X_train,y_train,ix,X_test,y_test,max_iter=400,lr=4*10**-4,VLB_opt='GD',lik='Poisson2');\n",
    "Zs,Zs_label, VLB2, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time2,err2 = GD2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FPb2 = Train(X_train,y_train,ix,X_test,y_test,max_iter=50,lr=2*10**-4,VLB_opt='FPb',lik='Poisson2')\n",
    "Zs,Zs_label, VLB_FPb2, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time_FPb2,err_FPb2 = FPb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPi2 = Train(X_train,y_train,ix,X_test,y_test,max_iter=50,lr=2*10**-4,VLB_opt='FPi',lik='Poisson2')\n",
    "Zs,Zs_label, VLB_FPi2, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time_FPi2,err_FPi2=FPi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FPi = Train(X_train,y_train,ix,X_test,y_test,max_iter=50,lr=2*10**-4,VLB_opt='FPi',lik='Poisson')\n",
    "Zs,Zs_label, VLB_FPi, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time_FPi,err_FPi=FPi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPi = Train(X_train,y_train,ix,X_test,y_test,max_iter=50,lr=2*10**-4,VLB_opt='FPi',lik='Bernoulli')\n",
    "Zs,Zs_label, VLB_FPi, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time_FPi,err_FPi=FPi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import matplotlib.pyplot as plt\n",
    "host = host_subplot(111)\n",
    "par = host.twinx()\n",
    "host.set_xlabel(\"Training time(log)\")\n",
    "host.set_ylabel(\"Error\")\n",
    "par.set_ylabel(\"negVLB\")\n",
    "# p0, = host.plot(VLB_time, err, 'bo--', lw=1, markersize=5, fillstyle='none', label=\"Poisson GD error\")\n",
    "# p1, = host.plot(VLB_time2, err2, 'ro--', lw=1, markersize=5, fillstyle='none', label=\"Poisson2 GD error\")\n",
    "# p2, = par.plot(VLB_time, VLB, 'bo-', lw=1, markersize=5, label=\"Poisson GD negVLB\")\n",
    "# p3, = par.plot(VLB_time2, VLB2, 'ro-', lw=1, markersize=5, label=\"Poisson2 GD negVLB\")\n",
    "# p0, = host.plot(VLB_time_FPb, err_FPb, 'bo--', lw=1, markersize=5, fillstyle='none', label=\"Poisson FPb error\")\n",
    "# p1, = host.plot(VLB_time_FPb2, err_FPb2, 'ro--', lw=1, markersize=5, fillstyle='none', label=\"Poisson2 FPb error\")\n",
    "# p2, = par.plot(VLB_time_FPb, VLB_FPb, 'bo-', lw=1, markersize=5, label=\"Poisson FPb negVLB\")\n",
    "# p3, = par.plot(VLB_time_FPb2, VLB_FPb2, 'ro-', lw=1, markersize=5, label=\"Poisson2 FPb negVLB\")\n",
    "p0, = host.plot(VLB_time_FPi, err_FPi, 'bo--', lw=1, markersize=5, fillstyle='none', label=\"Poisson FPi error\")\n",
    "p1, = host.plot(VLB_time_FPi2, err_FPi2, 'ro--', lw=1, markersize=5, fillstyle='none', label=\"Poisson2 FPi error\")\n",
    "p2, = par.plot(VLB_time_FPi, VLB_FPi, 'bo-', lw=1, markersize=5, label=\"Poisson FPb negVLB\")\n",
    "p3, = par.plot(VLB_time_FPi2, VLB_FPi2, 'ro-', lw=1, markersize=5, label=\"Poisson2 FPb negVLB\")\n",
    "host.set_xscale('log',basex=10)\n",
    "leg = plt.legend()\n",
    "host.yaxis.get_label().set_color(p0.get_color());par.yaxis.get_label().set_color(p0.get_color())\n",
    "leg.texts[0].set_color(p0.get_color());\n",
    "leg.texts[1].set_color(p1.get_color())\n",
    "leg.texts[2].set_color(p2.get_color());\n",
    "leg.texts[3].set_color(p3.get_color())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import matplotlib.pyplot as plt\n",
    "host = host_subplot(111)\n",
    "par = host.twinx()\n",
    "host.set_xlabel(\"Training time(log)\")\n",
    "host.set_ylabel(\"Error\")\n",
    "par.set_ylabel(\"negVLB\")\n",
    "p0, = host.plot(VLB_time, err, 'bo--', lw=1, markersize=5, fillstyle='none', label=\"error GD\")\n",
    "p1, = host.plot(VLB_time_FPi, err_FPi, 'ro--', lw=1, markersize=5, fillstyle='none', label=\"error FPi\")\n",
    "p2, = host.plot(VLB_time_FPb, err_FPb, 'go--', lw=1, markersize=5, fillstyle='none', label=\"error FPb\")\n",
    "# p3, = host.plot(VLB_time_FPi_mean, err_FPi_mean, 'yo--', markersize=5, lw=1, fillstyle='none', label=\"FPi-mean\")\n",
    "p3, = par.plot(VLB_time, VLB, 'bo-', lw=1, markersize=5, label=\"negVLB GD\")\n",
    "p4, = par.plot(VLB_time_FPi, VLB_FPi, 'ro-', lw=1, markersize=5, label=\"negVLB FPi\")\n",
    "p5, = par.plot(VLB_time_FPb, VLB_FPb, 'go-', lw=1, markersize=5, label=\"negVLB FPb\")\n",
    "# p7, = par.plot(VLB_time_FPi_mean, VLB_FPi_mean, 'yo-', lw=1, markersize=5, label=\"FPi-mean\")\n",
    "host.set_xscale('log',basex=10)\n",
    "leg = plt.legend()\n",
    "host.yaxis.get_label().set_color(p0.get_color());par.yaxis.get_label().set_color(p0.get_color())\n",
    "leg.texts[0].set_color(p0.get_color());\n",
    "leg.texts[1].set_color(p1.get_color())\n",
    "leg.texts[2].set_color(p2.get_color());\n",
    "leg.texts[3].set_color(p3.get_color())\n",
    "leg.texts[4].set_color(p4.get_color());\n",
    "leg.texts[5].set_color(p5.get_color())\n",
    "# leg.texts[6].set_color(p6.get_color());\n",
    "# leg.texts[7].set_color(p7.get_color())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some 1D training data(regression)\n",
    "num_train = 2000;num_test=700                         # 500 training poitns\n",
    "X_train = np.linspace(0, 10, num_train)[:,None]       # Inputs evenly spaced between 0 and 10\n",
    "F = np.sin(X_train)                   # True function (f = sin(x))\n",
    "y_train = F + 0.01*np.random.randn(num_train)[:,None]  # Observations\n",
    "X_test = np.linspace(0, 10, num_test)[:,None]       # Inputs evenly spaced between 0 and 10\n",
    "F_test = np.sin(X_test)                   # True function (f = sin(x))\n",
    "y_test = F_test + 0.01*np.random.randn(num_test)[:,None]  # Observations\n",
    "np.random.seed(3);num_inducing=100\n",
    "ix = random.sample(range(num_train), num_inducing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(y_train >= 0, 1, -1); y_test = np.where(y_test >= 0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data(count regression)\n",
    "mat_contents = sio.loadmat('count_dataset_ucsdpeds1l_N4000_D30.mat')\n",
    "x = mat_contents['x']; y = mat_contents['y']\n",
    "x = x.astype('double')\n",
    "\n",
    "# shuffle the data and split data into training set and test set\n",
    "data = np.concatenate((x,y), axis=1)\n",
    "np.random.shuffle(data)\n",
    "num_data = data.shape[0]; dim_data = data.shape[1] - 1;\n",
    "num_train = int(0.5*np.ceil(num_data)); num_test = num_data - num_train\n",
    "x_train = data[:num_train, :-1]; y_train = data[:num_train, -1];\n",
    "x_test = data[num_train:, :-1]; y_test = data[num_train:, -1];\n",
    "y_train = y_train[:, None]; y_test = y_test[:, None]\n",
    "# data Standardization with zero mean and unit variance\n",
    "scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "X_train = scaler.transform(x_train); X_test = scaler.transform(x_test)\n",
    "np.random.seed(3);num_inducing=100\n",
    "ix = random.sample(range(num_train), num_inducing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data from file, make sure banana.csv is in the same directory as this notebook\n",
    "data = np.genfromtxt('banana.csv', delimiter=',')\n",
    "\n",
    "# Dimension of data\n",
    "D = data.shape[1]-1\n",
    "\n",
    "# Seperate our data (input) from its corresponding label output\n",
    "# .. note we have to rescale from [-1,1] to [0,1] for a Bernoulli distribution\n",
    "X, y = data[:,:D], data[:,-1][:, None]\n",
    "\n",
    "# We will plot our data as well\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "# Plot 0 class in blue\n",
    "plt.plot(X[np.where(y == -1),0],X[np.where(y == -1),1],'bo', mew=0.5, alpha=0.5)\n",
    "# Plot 1 class in red\n",
    "plt.plot(X[np.where(y == 1),0],X[np.where(y == 1),1],'ro', mew=0.5, alpha=0.5)\n",
    "\n",
    "# Annotate plot\n",
    "plt.xlabel(\"$x_1$\"), plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Banana Dataset (red=1, blue=0)\")\n",
    "plt.axis(\"square\"), plt.xlim((-3, 3)), plt.ylim((-3, 3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from file, make sure banana.csv is in the same directory as this notebook\n",
    "data = np.genfromtxt('banana.csv', delimiter=',')\n",
    "\n",
    "# Dimension of data\n",
    "D = data.shape[1]-1\n",
    "n = data.shape[0]\n",
    "# Seperate our data (input) from its corresponding label output\n",
    "# .. note we have to rescale from [-1,1] to [0,1] for a Bernoulli distribution\n",
    "X, y = data[:,:D], data[:,-1][:, None]\n",
    "num_train = 3000; num_test = n - num_train\n",
    "X_train, y_train = X[:num_train,:], y[:num_train]\n",
    "X_test, y_test = X[:num_test,:], y[:num_test]\n",
    "np.random.seed(3);num_inducing=100 # randomly select active set and then keep them fixed\n",
    "ix = random.sample(range(num_train), num_inducing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "num = np.arange(-39,40);print(num)\n",
    "plt.figure(1)\n",
    "a1 = norm.cdf(num);print(a1)\n",
    "a2 = norm.pdf(num);print(a2)\n",
    "plt.plot(a1)\n",
    "plt.plot(a2)\n",
    "plt.figure(2)\n",
    "r = norm.pdf(num)/norm.cdf(num)\n",
    "plt.plot(r);print(r.shape)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4736461348785476e-196"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.pdf(-30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aa=np.random.randn(4,2);cc=np.ones((4,2))*10;print(aa)\n",
    "bb=np.arange(4)[:,None];print(bb)\n",
    "bb=np.tile(bb,[1,2]);print(bb.shape)\n",
    "term1=np.zeros((4,2))\n",
    "term1[bb!=0]=(aa[bb!=0]/cc[bb!=0])\n",
    "print(term1)\n",
    "# a1=np.where(bb==0,0,term1);print('a1',a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(np.linalg.eigvals(V) > 0)# check PSD condition for V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "  'a': lambda x: x * 5,\n",
    "  'b': lambda x: x + 7,\n",
    "  'c': lambda x: x - 2\n",
    "}[value](x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
