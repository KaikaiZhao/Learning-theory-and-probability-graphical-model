{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihoods we have used:\n",
    "1. regression: Gaussian\n",
    "2. count regression: \n",
    "    Poisson: lambda=e^f,  Poisson2: lambda=ln(1+e^f)\n",
    "3. binary classification: \n",
    "    Bernoulli: sigmoid,   Bernoulli2: Probit\n",
    "    \n",
    "For the sigmoid function, we use **expit** which is provided by *scipy.special*, because it is stable, fast and fairly accurate. Additionally, it is equivalent to sigmoid. For all methods, initial variational parameters are found by running the Laplace approximation on the subset/active set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv,norm,lstsq,cholesky\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy.io as sio\n",
    "from sklearn import preprocessing\n",
    "import random,time,GPy\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import loggamma,roots_hermitenorm,gamma,expit\n",
    "from scipy.special import ndtr as std_norm_cdf\n",
    "\n",
    "_sqrt_2pi = np.sqrt(2*np.pi)\n",
    "_lim_val = np.finfo(np.float64).max\n",
    "_lim_val_exp = np.log(_lim_val)\n",
    "\n",
    "def std_norm_pdf(x): # define a standard normal pdf(from GPy)\n",
    "    x = np.clip(x,-1e300,1e300)\n",
    "    return np.exp(-np.square(x)/2)/_sqrt_2pi\n",
    "\n",
    "def safe_exp(f):\n",
    "    clip_f = np.clip(f, -np.inf, _lim_val_exp)\n",
    "    return np.exp(clip_f)\n",
    "\n",
    "def safe_ln(x, minval=0.0000000001):\n",
    "    return np.log(x.clip(min=minval))\n",
    "\n",
    "def kernel(X1, X2=None, l=1.0, sigma_f=1.0, kernel_type='SE'):\n",
    "    '''\n",
    "    Isotropic squared exponential kernel. Computes \n",
    "    a covariance matrix from points in X1 and X2.    \n",
    "    Args:\n",
    "        X1: Array of m points (m x d).    X2: Array of n points (n x d).\n",
    "        l: length_scale    sigma_f: signal variance\n",
    "        kernel_type: SE(squared exponential), Matern32, Matern52, diag\n",
    "    Returns:\n",
    "        Covariance matrix (m x n).\n",
    "    '''\n",
    "    if kernel_type=='diag': return sigma_f**2*np.ones((X1.shape[0],1))\n",
    "    sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T); sqdist = np.absolute(sqdist)\n",
    "    if kernel_type=='SE': return sigma_f**2 * np.exp(-0.5 / l**2 * sqdist);\n",
    "    elif kernel_type=='Matern32': return sigma_f**2 * (1+3**0.5*np.sqrt(sqdist)/l) * np.exp(-3**0.5*np.sqrt(sqdist)/l);\n",
    "    elif kernel_type=='Matern52': return sigma_f**2 * (1+3**0.5*np.sqrt(sqdist)/l) * np.exp(-3**0.5*np.sqrt(sqdist)/l);\n",
    "#     kernel_matrix = {\n",
    "#         'SE': sigma_f**2 * np.exp(-0.5 / l**2 * sqdist),\n",
    "#         'Matern32': sigma_f**2 * (1+3**0.5*np.sqrt(sqdist)/l) * np.exp(-3**0.5*np.sqrt(sqdist)/l),\n",
    "#         'Matern52': sigma_f**2 * (1+5**0.5*np.sqrt(sqdist)/l+5*sqdist/(3*l**2)) * np.exp(-5**0.5*np.sqrt(sqdist)/l),\n",
    "#     }[kernel_type]\n",
    "#     return kernel_matrix#sigma_f**2 * np.exp(-0.5 / l**2 * sqdist)\n",
    "\n",
    "# def kernel(X1, X2, *args, **kwargs):\n",
    "#     return rbf_kernel(X1, X2, *args, **kwargs)\n",
    "\n",
    "def MFE(true_labels, pred_labels):\n",
    "    '''\n",
    "    Calculating mean fraction error(MFE) for count regression, the math is used as follows:\n",
    "    MFE = mean( abs( (true_labels - pred_labels)./true_labels ) ) \n",
    "    Written by Kaikai\n",
    "    '''    \n",
    "    if true_labels.size != pred_labels.size:\n",
    "        print('The size of true_labels and pred_labels is supposed to be identical.')\n",
    "        return -1    \n",
    "    true_labels = true_labels.flatten().astype('double'); pred_labels = pred_labels.flatten().astype('double')\n",
    "    true_labels_temp = true_labels.copy()\n",
    "    if 0 in true_labels: # replace zero with a very small positive value in order to avoid dividing by zero\n",
    "        print('There are elements of zero value in true labels.')\n",
    "        true_labels_temp[true_labels==0] = 1\n",
    "    \n",
    "    MFE = np.mean( np.abs( (true_labels - pred_labels)/true_labels_temp ) )\n",
    "    return MFE\n",
    "\n",
    "def calc_vlb(m,V, a, lik='Gaussian'):\n",
    "    prior_mean_u = a[0]; prior_mean_f = a[1] # prior mean for inducing points    \n",
    "    A = a[2] # Knm*inv(Kmm)\n",
    "    Kmm = a[3]; Kmm_inv = a[4]; Kmn = a[5]; Knn_diag = a[6]; y = a[7] # the ground truth for training data\n",
    "    noise_var = a[8]\n",
    "    num_train = len(prior_mean_f);num_inducing = len(prior_mean_u)\n",
    "    m_q = prior_mean_f + np.dot(A, (m-prior_mean_u)) # Eq.(3a) in paper\n",
    "    v_q = ( Knn_diag.ravel() + np.diag(np.dot(A, np.dot(V-Kmm, A.T))) )[:, None] # Eq.(3b) in paper\n",
    "#     v_q = np.absolute(v_q)\n",
    "    c1 = m - prior_mean_u; c2 = np.dot(Kmm_inv, c1);#print(v_q[:20],min(v_q))\n",
    "    (Sign,LogDetKmm) = np.linalg.slogdet(Kmm); LogDetKmm = Sign*LogDetKmm\n",
    "    (SignV,LogDetV) = np.linalg.slogdet(V); LogDetV = SignV*LogDetV;#print(v_q[:50])     \n",
    "    if lik=='Bernoulli':\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q));#vlb_lik = expit(y*f);print('yf',expit(y*f))\n",
    "        vlb_lik = np.sum( 1/_sqrt_2pi*np.dot( safe_ln(expit(y*f)) ,w) ) # sigmoid liklihood\n",
    "#         vlb_lik = np.sum( 1.0/np.sqrt(2*np.pi)*np.dot( np.log(std_norm_cdf(y*f)+1e-10) ,w) ) # Probit liklihood\n",
    "    elif lik=='Gaussian':\n",
    "        vlb_lik = -np.log(np.sqrt(2*np.pi*noise_var)) - np.sum((y-m_q)**2+v_q)/(2*noise_var)\n",
    "    elif lik=='Poisson':\n",
    "        vlb_lik = np.dot(y.T,m_q) - np.sum(loggamma(y+1)) - np.sum(np.exp(m_q+0.5*v_q))\n",
    "    elif lik=='Poisson2': # another link func: lambda=ln(1+e^f)\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q));\n",
    "        term1 = -np.sum(loggamma(y+1)); term2 = -np.sum( 1/_sqrt_2pi*np.dot( np.log(1+safe_exp(f)),w) )\n",
    "        term3 = np.sum( 1/_sqrt_2pi*np.dot( y*safe_ln(np.log(1+safe_exp(f))),w) )\n",
    "        vlb_lik = term1 + term2 + term3\n",
    "    vlb_kl = 0.5*( LogDetKmm - LogDetV + np.dot(c1.T, c2) + np.trace(np.dot(Kmm_inv,V)) - len(prior_mean_u) )\n",
    "    vlb = vlb_lik - vlb_kl\n",
    "    return vlb \n",
    "\n",
    "def get_init_hyperparameters(Z,Z_label,lik='Poisson',K='SE'):\n",
    "    # For all methods, initial variational parameters are found by running the Laplace approximation on the subset/active set.\n",
    "    dim_data = Z.shape[1]\n",
    "    if K=='SE': kern=GPy.kern.RBF(dim_data, variance=1.0, lengthscale=1.0);\n",
    "    elif K=='Matern32': kern=GPy.kern.Matern32(dim_data, variance=1.0, lengthscale=1.0)\n",
    "    elif K=='Matern52': kern=GPy.kern.Matern52(dim_data, variance=1.0, lengthscale=1.0)\n",
    "#     kern = {\n",
    "#         'SE': GPy.kern.RBF(dim_data, variance=1.0, lengthscale=1.0),\n",
    "#         'Matern32': GPy.kern.Matern32(dim_data, variance=1.0, lengthscale=1.0),\n",
    "#         'Matern52': GPy.kern.Matern52(dim_data, variance=1.0, lengthscale=1.0)\n",
    "#     }[K]\n",
    "#     kern = GPy.kern.RBF(dim_data, variance=1.0, lengthscale=1.0)\n",
    "    likelihood = {\n",
    "          'Poisson': GPy.likelihoods.Poisson(),\n",
    "          'Poisson2': GPy.likelihoods.Poisson(GPy.likelihoods.link_functions.Log_ex_1()),\n",
    "          'Gaussian': GPy.likelihoods.Gaussian(),\n",
    "          'Bernoulli': GPy.likelihoods.Bernoulli()\n",
    "        }[lik]\n",
    "    laplace_inf = GPy.inference.latent_function_inference.Laplace()\n",
    "    model_lap = GPy.core.GP(X=Z, Y=Z_label, likelihood=likelihood, inference_method=laplace_inf, kernel=kern)\n",
    "    model_lap.optimize();\n",
    "    if K=='SE': length_scale=model_lap.rbf.lengthscale[0];signal_variance=model_lap.rbf.variance[0]\n",
    "    elif K=='Mat32': length_scale=model_lap.Mat32.lengthscale[0];signal_variance=model_lap.Mat32.variance[0]\n",
    "    elif K=='Mat52': length_scale=model_lap.Mat32.lengthscale[0];signal_variance=model_lap.Mat32.variance[0]\n",
    "    print(model_lap)\n",
    "    return model_lap,length_scale,signal_variance\n",
    "\n",
    "def Adam(theta, g_t, t, alpha=0.01, m_t=0, v_t=0, opt='minimize'):\n",
    "    beta_1 = 0.9; beta_2 = 0.999; epsilon = 1e-8     #initialize the values of the parameters    \n",
    "    m_t = beta_1*m_t + (1-beta_1)*g_t                #updates the moving averages of the gradient\n",
    "    v_t = beta_2*v_t + (1-beta_2)*(g_t*g_t)          #updates the moving averages of the squared gradient\n",
    "    m_cap = m_t/(1-(beta_1**t))                      #calculates the bias-corrected estimates\n",
    "    v_cap = v_t/(1-(beta_2**t))                      #calculates the bias-corrected estimates\n",
    "    if opt=='maximize':\n",
    "        theta = theta + (alpha*m_cap)/(np.sqrt(v_cap)+epsilon)    #updates the parameters\n",
    "    else:\n",
    "        theta = theta - (alpha*m_cap)/(np.sqrt(v_cap)+epsilon)    #updates the parameters\n",
    "    return theta, m_t, v_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GH_quad(mu_star,std_star): # gauss_hermite_quad to calculate numerical integration, w and z denote weights and sample points, respectively. \n",
    "    z,w = roots_hermitenorm(n=50, mu=False); z,w=z[:,None],w[:,None]\n",
    "    z = np.kron(std_star,z.T) + mu_star\n",
    "    return z,w\n",
    "\n",
    "def calc_err(X, xStar, yTrue, l, sigma2, m, V,Kmm,Kmm_inv,noise_var=0.1,lik='Poisson',K='SE'):\n",
    "    num_xStar = xStar.shape[0]; Kmn = kernel(X, xStar, l, np.sqrt(sigma2),kernel_type=K)\n",
    "    Knn_diag = kernel(xStar, l, np.sqrt(sigma2),kernel_type='diag')#np.eye(num_xStar)*sigma2+noise_var*np.eye(num_xStar)\n",
    "    A = np.dot(Kmn.T, Kmm_inv);\n",
    "    mu_star = np.dot(A,m); v2_star = Knn_diag + np.dot(A, np.dot(V-Kmm,A.T))\n",
    "    std_star = np.sqrt(np.diag(v2_star))[:,None]\n",
    "    if lik=='Bernoulli':\n",
    "        p = calc_Bernoulli_pred(mu_star,std_star); res = np.where(p>=0.5,1,-1)\n",
    "        return np.sum( np.where(yTrue*res<0,1,0) )/num_xStar         \n",
    "    elif lik=='Gaussian':\n",
    "        mu, std = calc_Gauss_pred(mu_star,std_star,noise_var)\n",
    "        return 1/num_xStar*( np.sum( (yTrue-mu)**2 ) )        \n",
    "    else: # Poisson, Poisson2\n",
    "        y_min = min(yTrue); y_max = max(yTrue); \n",
    "        pmf = calc_Poisson_pred(mu_star,std_star,num_xStar,y_min,y_max,lik)\n",
    "        res = np.argmax(pmf, axis=1)+y_min; err = MFE(yTrue, res)\n",
    "        return err\n",
    "\n",
    "def calc_Gauss_pred(mu_star,std_star,noise_var):\n",
    "    return mu_star, std_star+np.sqrt(noise_var)\n",
    "\n",
    "def calc_Poisson_pred(mu_star,std_star,num_xStar,y_min,y_max,lik='Poisson'):\n",
    "    f,w = GH_quad(mu_star,std_star)\n",
    "    y_range = np.arange(y_min,y_max+1)\n",
    "    lik_func = {\n",
    "        'Poisson': lambda f,y: 1/gamma(y+1)*safe_exp(-safe_exp(f)+f*y),\n",
    "        'Poisson2': lambda f,y: 1/gamma(y+1)*expit(-f)*( safe_ln(1.0+safe_exp(f))**y )\n",
    "    }[lik]\n",
    "    poisson_lik = np.zeros((num_xStar,len(y_range)))\n",
    "    for i,y in enumerate(y_range):\n",
    "        poisson_lik[:,i] = 1/_sqrt_2pi*np.dot(lik_func(f,y),w).ravel()\n",
    "    return poisson_lik\n",
    "\n",
    "def calc_Bernoulli_pred(mu_star,std_star):\n",
    "#     v = std_star**2; kappa_v = (1+np.pi*v/8.0)**(-1/2) # Probit liklihood\n",
    "#     p = sigmoid(kappa_v*mu_star) # p(y=1|x_*,m,V)\n",
    "    f,w = GH_quad(mu_star,std_star) # sigmoid liklihood\n",
    "    p = 1/np.sqrt(2*np.pi)*np.dot( expit(f), w ) # p(y=1|x_*,m,V)\n",
    "    return p\n",
    "\n",
    "def calc_m_q(m, A, prior_mean_u, prior_mean_f):\n",
    "    return prior_mean_f + np.dot(A, (m-prior_mean_u))\n",
    "\n",
    "def calc_v_q(V, A, Kmm, Knn_diag):\n",
    "    return ( Knn_diag.ravel() + np.diag(np.dot(A, np.dot(V-Kmm, A.T))) )[:,None] # Eq.(3b) in paper\n",
    "\n",
    "def calc_rho(m_q, v_q, y, lik='Poisson', noise_var=0.1):\n",
    "    if lik=='Bernoulli':\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q))\n",
    "        return 1.0/_sqrt_2pi*np.dot(  y*expit(-y*f) , w ) # sigmoid liklihood\n",
    "#         return 1.0/_sqrt_2pi*np.dot( y*std_norm_pdf(f) / (std_norm_cdf(y*f)+1e-10), w ) # Probit liklihood        \n",
    "    elif lik=='Gaussian':\n",
    "        return 1.0/noise_var*(y-m_q)\n",
    "    elif lik=='Poisson':\n",
    "        return -np.exp(m_q + 0.5*v_q) + y\n",
    "    elif lik=='Poisson2':\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q))\n",
    "        y = np.tile(y,[1,f.shape[1]]); term2 = expit(f); t0 = safe_ln(1+safe_exp(f))# avoid dividing by zeros\n",
    "        d1_log_lik = np.zeros(t0.shape)\n",
    "        d1_log_lik[t0!=0] = (y[t0!=0]/t0[t0!=0] - 1)*term2[t0!=0]\n",
    "        return 1.0/_sqrt_2pi*np.dot(  d1_log_lik, w )\n",
    "    \n",
    "def calc_lambda(m_q, v_q, y, lik='Poisson', noise_var=0.1):\n",
    "    if lik=='Bernoulli':\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q))\n",
    "        return 1.0/np.sqrt(2*np.pi)*np.dot(  -expit(y*f)*expit(-y*f), w ) # sigmoid\n",
    "#         return 1.0/_sqrt_2pi*np.dot( -std_norm_pdf(f)**2 / (std_norm_cdf(y*f)**2+1e-10) - y*f*std_norm_pdf(f) / (std_norm_cdf(y*f)+1e-10), w )\n",
    "    elif lik=='Gaussian':\n",
    "        return -1.0/noise_var*np.ones((len(m_q),1))\n",
    "    elif lik=='Poisson':\n",
    "        return -np.exp(m_q + 0.5*v_q)\n",
    "    elif lik=='Poisson2':\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q));y = np.tile(y,[1,f.shape[1]]);\n",
    "        term2 = expit(f)*expit(f); t0 = safe_ln(1+safe_exp(f));d2_log_lik = np.zeros(t0.shape)\n",
    "        d2_log_lik[t0!=0] = ((y[t0!=0]/t0[t0!=0]-1)*safe_exp(-f[t0!=0])-y[t0!=0]/(t0[t0!=0])**2)*term2[t0!=0]\n",
    "        return 1.0/_sqrt_2pi*np.dot(  d2_log_lik, w )\n",
    "\n",
    "def dVLb_dm(m, rho, prior_mean_u, Kmm_inv, A):\n",
    "    dm = np.dot(A.T,rho) - np.dot(Kmm_inv, m-prior_mean_u) # Eq.(11a) in paper\n",
    "    return dm\n",
    "\n",
    "def dVLb_dL(L, lam, Kmm_inv, A, noise_var=1e-8):# optimizing the cholesky factor L guarantees the PSD of V automatically\n",
    "    dL = np.dot( np.dot( np.dot(A.T, np.diag(lam.ravel())), A ), L ) + inv(L+np.sqrt(noise_var)*np.eye(len(L))).T - np.dot(Kmm_inv, L)\n",
    "    return dL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(X,Y,ix,X_test=None,y_test=None,max_iter=100,lr=1*10**-5,FPb_cond=1*10**0,stop_cond=1*10**-3,VLB_opt='GD',lik='Poisson',K='SE'):\n",
    "    num_train = X.shape[0];num_inducing=len(ix);num_test=X_test.shape[0]; \n",
    "    prior_mean_u = np.zeros((num_inducing, 1)); prior_mean_f = np.zeros((num_train, 1))\n",
    "    Z = X[ix, :]; Z_label = Y[ix] # Z is the inducing set\n",
    "    model_lap,ls,sf2 = get_init_hyperparameters(Z,Z_label,lik=lik,K=K)#(Z_label+1)/2    \n",
    "    # variational parameters from initialization\n",
    "    f_mean, f_var = model_lap._raw_predict(X) \n",
    "#     ls = model_lap.rbf.lengthscale[0]; sf2 = model_lap.rbf.variance[0]; # length_scale,sigma_f2\n",
    "    if lik=='Gaussian': noise_var=model_lap.Gaussian_noise.variance[0] \n",
    "    else: noise_var=1*10**-8\n",
    "    #  we use variational mean from Laplace appr\n",
    "    m = f_mean[ix]; V = np.diag(f_var[ix].ravel()); L = cholesky(V);#print(min(f_var))\n",
    "    Kmm = kernel(Z, Z, l = ls, sigma_f = np.sqrt(sf2),kernel_type=K) + noise_var*np.eye(len(Z))\n",
    "    Kmm_inv = inv(Kmm); Kmn = kernel(Z, X, l = ls, sigma_f = np.sqrt(sf2),kernel_type=K); \n",
    "    Knn_diag = kernel(X,l=ls,sigma_f=np.sqrt(sf2),kernel_type='diag')# sf2*np.ones((num_train, 1))\n",
    "    A = np.dot(Kmn.T, Kmm_inv) # Knm*inv(Kmm)    \n",
    "    a = (prior_mean_u,prior_mean_f,A,Kmm,Kmm_inv,Kmn,Knn_diag,Y,noise_var)\n",
    "    num_iter = 0; VLB = []; VLB_time = []; err = []; FPb_converge = False# True\n",
    "    var = np.hstack([m.flatten(), L.flatten()])\n",
    "    start_time = time.time(); VLB.append(-calc_vlb(m,V, a,lik)[0][0])\n",
    "    err.append(calc_err(Z,X_test,y_test,ls,sf2,m,V,Kmm,Kmm_inv,noise_var=0.1,lik=lik,K=K))   \n",
    "    VLB_time.append(time.time()-start_time);print('Before iterations, VLB:{:.6f} err:{:.6f}'.format(VLB[-1],err[-1]) )\n",
    "    while 1:\n",
    "        num_iter = num_iter +1;#break\n",
    "        m_q = calc_m_q(m, A, prior_mean_u, prior_mean_f); v_q = calc_v_q(V, A, Kmm, Knn_diag);#print('m_q:',m_q[:10])\n",
    "        # According to Table 1 in paper, expectations of the derivatives wrt N(f|m,v) for Possion likelihood\n",
    "        rho = calc_rho(m_q, v_q, Y,lik, noise_var); lam = calc_lambda(m_q, v_q,Y,lik, noise_var);        \n",
    "        if VLB_opt=='GD':\n",
    "            dm = dVLb_dm(m, rho, prior_mean_u, Kmm_inv, A)\n",
    "            dL = dVLb_dL(L, lam, Kmm_inv, A, noise_var)\n",
    "            gradients = np.hstack([dm.flatten(), dL.flatten()])\n",
    "            var, m_t, v_t = Adam(var,gradients,num_iter,alpha=lr,opt='maximize')\n",
    "            m = var[:num_inducing][:,None] # variantional mean\n",
    "            L = var[num_inducing:].reshape(num_inducing,num_inducing) # variational variance V=L*L.T\n",
    "            V = np.dot(L, L.T);\n",
    "            \n",
    "        elif VLB_opt=='FPi':\n",
    "            dm = dVLb_dm(m, rho, prior_mean_u, Kmm_inv, A)\n",
    "            m, m_t, v_t = Adam(m,dm,num_iter,alpha=lr,opt='maximize') # print('old V',V[:1,:1])\n",
    "            V = inv(Kmm_inv-np.dot( np.dot(A.T, np.diag(lam.ravel())), A ));\n",
    "        elif VLB_opt=='FPb':            \n",
    "            if FPb_converge:\n",
    "                dm = dVLb_dm(m, rho, prior_mean_u, Kmm_inv, A); #print('old m:',m[:1]);\n",
    "                m, m_t, v_t = Adam(m,dm,num_iter,alpha=lr,opt='maximize')\n",
    "            else:\n",
    "                V = inv(Kmm_inv-np.dot( np.dot(A.T, np.diag(lam.ravel())), A))\n",
    "                \n",
    "        elif VLB_opt=='FPi-mean':\n",
    "            d = A.T; gamma = -lam; print('old m:',m[:1]); print('old V:',V[:2,:2])             \n",
    "            V = inv(Kmm_inv-np.dot( np.dot(A.T, np.diag(lam.ravel())), A))\n",
    "            m = np.dot(V, np.dot(d, rho + np.dot(A,m)*gamma) );\n",
    "\n",
    "        VLB.append(-calc_vlb(m,V, a,lik)[0][0]); VLB_time.append(time.time()-start_time)\n",
    "        err.append(calc_err(Z,X_test,y_test,ls,sf2,m,V,Kmm,Kmm_inv,noise_var=0.1,lik=lik))\n",
    "        if num_iter>0:\n",
    "            delta_vlb = abs(VLB[-1]-VLB[-2])\n",
    "            if num_iter%1 == 0:\n",
    "                print('iter:{}, delta_VLB:{:.6f}, VLB:{:.6f}, err:{:.6f}'.format(num_iter, VLB[-2]-VLB[-1], VLB[-1], err[-1]));\n",
    "            if VLB_opt=='FPb' and delta_vlb<=FPb_cond:\n",
    "                FPb_converge = not FPb_converge; print('m or V converged')\n",
    "            if num_iter>4 and delta_vlb<=stop_cond:\n",
    "                if np.average(VLB[-2:])<=np.average(VLB[-4:]):\n",
    "                    if num_iter>=max_iter: print('It has reached the maximum number of iterations.');break\n",
    "                    else:continue\n",
    "                else: print('After {} iterations it converged: delta_VLB:{:.6f}, VLB:{:.6f}, err:{:.6f}'.format(num_iter,delta_vlb,VLB[-1],err[-1]) );break                \n",
    "            if num_iter>8 and np.average(VLB[-4:])>=np.average(VLB[-8:]): print('negVLB did not decrease any more.'); break;            \n",
    "            elif num_iter==max_iter:\n",
    "                print('It has reached the maximum number of iterations, i.e. {}, with delta_VLB:{:.6f}, VLB:{:.6f} and err:{:.6f}'.format(max_iter,delta_vlb,VLB[-1],err[-1]));break            \n",
    "    return Z,Z_label, VLB, ls, sf2, m, V,noise_var, Kmm, Kmm_inv, A,VLB_time,err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name : gp\n",
      "Objective : -139.7463246047188\n",
      "Number of Parameters : 3\n",
      "Number of Optimization Parameters : 3\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1mgp.                    \u001b[0;0m  |                value  |  constraints  |  priors\n",
      "  \u001b[1mrbf.variance           \u001b[0;0m  |   5.1020070037083425  |      +ve      |        \n",
      "  \u001b[1mrbf.lengthscale        \u001b[0;0m  |   7.3702655490680025  |      +ve      |        \n",
      "  \u001b[1mGaussian_noise.variance\u001b[0;0m  |  0.00979866354836197  |      +ve      |        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " C:\\Users\\kevin\\AppData\\Local\\conda\\conda\\envs\\graph_analytics\\lib\\site-packages\\ipykernel_launcher.py:11: RuntimeWarning:invalid value encountered in sqrt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before iterations, VLB:1439.518616 err:0.015196\n",
      "old m: [[-0.62949157]]\n",
      "old V: [[0.0004353  0.        ]\n",
      " [0.         0.00054803]]\n",
      "iter:1, delta_VLB:301.592719, VLB:1137.925897, err:0.013048\n",
      "old m: [[-0.59704521]]\n",
      "old V: [[ 9.35748669e-03 -2.47003934e-05]\n",
      " [-2.47003934e-05  9.26640407e-03]]\n",
      "iter:2, delta_VLB:0.000000, VLB:1137.925897, err:0.013048\n",
      "old m: [[-0.59704521]]\n",
      "old V: [[ 9.35748669e-03 -2.47003934e-05]\n",
      " [-2.47003934e-05  9.26640407e-03]]\n",
      "iter:3, delta_VLB:0.000000, VLB:1137.925897, err:0.013048\n",
      "old m: [[-0.59704521]]\n",
      "old V: [[ 9.35748669e-03 -2.47003934e-05]\n",
      " [-2.47003934e-05  9.26640407e-03]]\n",
      "iter:4, delta_VLB:0.000000, VLB:1137.925897, err:0.013048\n",
      "old m: [[-0.59704521]]\n",
      "old V: [[ 9.35748669e-03 -2.47003934e-05]\n",
      " [-2.47003934e-05  9.26640407e-03]]\n",
      "iter:5, delta_VLB:0.000000, VLB:1137.925897, err:0.013048\n",
      "old m: [[-0.59704521]]\n",
      "old V: [[ 9.35748669e-03 -2.47003934e-05]\n",
      " [-2.47003934e-05  9.26640407e-03]]\n",
      "iter:6, delta_VLB:0.000000, VLB:1137.925897, err:0.013048\n",
      "old m: [[-0.59704521]]\n",
      "old V: [[ 9.35748669e-03 -2.47003934e-05]\n",
      " [-2.47003934e-05  9.26640407e-03]]\n",
      "iter:7, delta_VLB:-0.000000, VLB:1137.925897, err:0.013048\n",
      "old m: [[-0.59704521]]\n",
      "old V: [[ 9.35748669e-03 -2.47003934e-05]\n",
      " [-2.47003934e-05  9.26640407e-03]]\n",
      "iter:8, delta_VLB:0.000000, VLB:1137.925897, err:0.013048\n",
      "After 8 iterations it converged: delta_VLB:0.000000, VLB:1137.925897, err:0.013048\n"
     ]
    }
   ],
   "source": [
    "FPim = Train(X_train,y_train,ix,X_test,y_test,max_iter=10,VLB_opt='FPi-mean',lik='Gaussian',K='SE')#Gaussian,Bernoulli\n",
    "Zs,Zs_label, VLB_FPim, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time_FPim,err_FPim=FPim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name : gp\n",
      "Objective : -139.7463246047188\n",
      "Number of Parameters : 3\n",
      "Number of Optimization Parameters : 3\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1mgp.                    \u001b[0;0m  |                value  |  constraints  |  priors\n",
      "  \u001b[1mrbf.variance           \u001b[0;0m  |   5.1020070037083425  |      +ve      |        \n",
      "  \u001b[1mrbf.lengthscale        \u001b[0;0m  |   7.3702655490680025  |      +ve      |        \n",
      "  \u001b[1mGaussian_noise.variance\u001b[0;0m  |  0.00979866354836197  |      +ve      |        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " C:\\Users\\kevin\\AppData\\Local\\conda\\conda\\envs\\graph_analytics\\lib\\site-packages\\ipykernel_launcher.py:42: RuntimeWarning:invalid value encountered in sqrt\n",
      " C:\\Users\\kevin\\AppData\\Local\\conda\\conda\\envs\\graph_analytics\\lib\\site-packages\\ipykernel_launcher.py:43: RuntimeWarning:invalid value encountered in sqrt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before iterations, VLB:1439.518614 err:0.015196\n",
      "iter:1, delta_VLB:-2.458890, VLB:1441.977504, err:0.015062\n",
      "iter:2, delta_VLB:12.364359, VLB:1429.613146, err:0.014967\n",
      "iter:3, delta_VLB:-26.577157, VLB:1456.190303, err:0.014886\n",
      "iter:4, delta_VLB:39.131132, VLB:1417.059171, err:0.014813\n",
      "iter:5, delta_VLB:18.099830, VLB:1398.959341, err:0.014745\n",
      "iter:6, delta_VLB:20.430655, VLB:1378.528687, err:0.014682\n",
      "iter:7, delta_VLB:17.629383, VLB:1360.899303, err:0.014621\n",
      "iter:8, delta_VLB:4.438434, VLB:1356.460870, err:0.014564\n",
      "iter:9, delta_VLB:10.633414, VLB:1345.827456, err:0.014509\n",
      "iter:10, delta_VLB:7.577150, VLB:1338.250306, err:0.014456\n",
      "iter:11, delta_VLB:9.131454, VLB:1329.118852, err:0.014403\n",
      "iter:12, delta_VLB:5.872177, VLB:1323.246674, err:0.014352\n",
      "iter:13, delta_VLB:6.601086, VLB:1316.645589, err:0.014302\n",
      "iter:14, delta_VLB:6.434553, VLB:1310.211036, err:0.014253\n",
      "iter:15, delta_VLB:6.759608, VLB:1303.451427, err:0.014206\n",
      "iter:16, delta_VLB:4.817224, VLB:1298.634204, err:0.014159\n",
      "iter:17, delta_VLB:6.739969, VLB:1291.894235, err:0.014114\n",
      "iter:18, delta_VLB:3.938648, VLB:1287.955587, err:0.014069\n",
      "iter:19, delta_VLB:2.493603, VLB:1285.461984, err:0.014025\n",
      "iter:20, delta_VLB:9.264593, VLB:1276.197391, err:0.013980\n",
      "iter:21, delta_VLB:4.158248, VLB:1272.039143, err:0.013938\n",
      "iter:22, delta_VLB:6.155352, VLB:1265.883791, err:0.013895\n",
      "iter:23, delta_VLB:4.890172, VLB:1260.993619, err:0.013855\n",
      "iter:24, delta_VLB:4.780945, VLB:1256.212674, err:0.013815\n",
      "iter:25, delta_VLB:4.356197, VLB:1251.856477, err:0.013774\n",
      "iter:26, delta_VLB:4.035328, VLB:1247.821149, err:0.013737\n",
      "iter:27, delta_VLB:3.977767, VLB:1243.843382, err:0.013702\n",
      "iter:28, delta_VLB:4.461847, VLB:1239.381535, err:0.013664\n",
      "iter:29, delta_VLB:3.183978, VLB:1236.197557, err:0.013632\n",
      "iter:30, delta_VLB:4.307357, VLB:1231.890200, err:0.013600\n",
      "iter:31, delta_VLB:2.653422, VLB:1229.236778, err:0.013564\n",
      "iter:32, delta_VLB:3.725342, VLB:1225.511437, err:0.013540\n",
      "iter:33, delta_VLB:2.739885, VLB:1222.771551, err:0.013508\n",
      "iter:34, delta_VLB:2.619261, VLB:1220.152290, err:0.013482\n",
      "iter:35, delta_VLB:2.073050, VLB:1218.079241, err:0.013456\n",
      "iter:36, delta_VLB:2.065851, VLB:1216.013389, err:0.013429\n",
      "iter:37, delta_VLB:1.972986, VLB:1214.040403, err:0.013403\n",
      "iter:38, delta_VLB:1.115182, VLB:1212.925221, err:0.013382\n",
      "iter:39, delta_VLB:1.671932, VLB:1211.253288, err:0.013359\n",
      "iter:40, delta_VLB:0.566078, VLB:1210.687210, err:0.013338\n",
      "iter:41, delta_VLB:1.492084, VLB:1209.195126, err:0.013317\n",
      "iter:42, delta_VLB:0.127425, VLB:1209.067701, err:0.013301\n",
      "iter:43, delta_VLB:1.534967, VLB:1207.532734, err:0.013284\n",
      "iter:44, delta_VLB:-0.303337, VLB:1207.836070, err:0.013267\n",
      "iter:45, delta_VLB:1.574177, VLB:1206.261894, err:0.013255\n",
      "iter:46, delta_VLB:-0.313020, VLB:1206.574914, err:0.013241\n",
      "iter:47, delta_VLB:1.511765, VLB:1205.063149, err:0.013230\n",
      "iter:48, delta_VLB:-1.544533, VLB:1206.607682, err:0.013221\n",
      "iter:49, delta_VLB:2.276801, VLB:1204.330880, err:0.013211\n",
      "iter:50, delta_VLB:-2.714036, VLB:1207.044916, err:0.013207\n",
      "iter:51, delta_VLB:3.129545, VLB:1203.915371, err:0.013193\n",
      "iter:52, delta_VLB:-0.331566, VLB:1204.246937, err:0.013184\n",
      "iter:53, delta_VLB:0.829087, VLB:1203.417849, err:0.013178\n",
      "iter:54, delta_VLB:0.482619, VLB:1202.935230, err:0.013170\n",
      "iter:55, delta_VLB:-0.593620, VLB:1203.528850, err:0.013156\n",
      "iter:56, delta_VLB:1.422699, VLB:1202.106151, err:0.013154\n",
      "iter:57, delta_VLB:-3.963906, VLB:1206.070056, err:0.013140\n",
      "iter:58, delta_VLB:4.642064, VLB:1201.427992, err:0.013141\n",
      "iter:59, delta_VLB:-0.982966, VLB:1202.410959, err:0.013131\n",
      "iter:60, delta_VLB:1.538728, VLB:1200.872231, err:0.013130\n",
      "iter:61, delta_VLB:-0.437070, VLB:1201.309301, err:0.013122\n",
      "iter:62, delta_VLB:0.869506, VLB:1200.439795, err:0.013117\n",
      "iter:63, delta_VLB:-0.153355, VLB:1200.593150, err:0.013112\n",
      "iter:64, delta_VLB:0.520681, VLB:1200.072468, err:0.013109\n",
      "iter:65, delta_VLB:0.018872, VLB:1200.053596, err:0.013109\n",
      "iter:66, delta_VLB:0.255667, VLB:1199.797929, err:0.013100\n",
      "iter:67, delta_VLB:0.152369, VLB:1199.645560, err:0.013107\n",
      "iter:68, delta_VLB:0.070921, VLB:1199.574638, err:0.013092\n",
      "iter:69, delta_VLB:0.256509, VLB:1199.318130, err:0.013093\n",
      "iter:70, delta_VLB:-0.083621, VLB:1199.401750, err:0.013099\n",
      "iter:71, delta_VLB:0.354341, VLB:1199.047409, err:0.013091\n",
      "iter:72, delta_VLB:-0.232451, VLB:1199.279860, err:0.013101\n",
      "iter:73, delta_VLB:0.414345, VLB:1198.865514, err:0.013098\n",
      "iter:74, delta_VLB:-0.368173, VLB:1199.233687, err:0.013087\n",
      "iter:75, delta_VLB:0.509850, VLB:1198.723837, err:0.013098\n",
      "iter:76, delta_VLB:-0.480705, VLB:1199.204542, err:0.013090\n",
      "iter:77, delta_VLB:0.554620, VLB:1198.649922, err:0.013090\n",
      "iter:78, delta_VLB:-0.583880, VLB:1199.233802, err:0.013082\n",
      "iter:79, delta_VLB:0.615393, VLB:1198.618408, err:0.013088\n",
      "iter:80, delta_VLB:-0.689209, VLB:1199.307617, err:0.013078\n",
      "iter:81, delta_VLB:0.663333, VLB:1198.644284, err:0.013086\n",
      "iter:82, delta_VLB:-0.751159, VLB:1199.395443, err:0.013082\n",
      "negVLB did not decrease any more.\n"
     ]
    }
   ],
   "source": [
    "GD = Train(X_train,y_train,ix,X_test,y_test,max_iter=400,lr=2*10**-3,VLB_opt='GD',lik='Gaussian');\n",
    "Zs,Zs_label, VLB, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time,err = GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name : gp\n",
      "Objective : -139.7463246047188\n",
      "Number of Parameters : 3\n",
      "Number of Optimization Parameters : 3\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1mgp.                    \u001b[0;0m  |                value  |  constraints  |  priors\n",
      "  \u001b[1mrbf.variance           \u001b[0;0m  |   5.1020070037083425  |      +ve      |        \n",
      "  \u001b[1mrbf.lengthscale        \u001b[0;0m  |   7.3702655490680025  |      +ve      |        \n",
      "  \u001b[1mGaussian_noise.variance\u001b[0;0m  |  0.00979866354836197  |      +ve      |        \n",
      "Before iterations, VLB:1439.518614 err:0.015196\n",
      "iter:1, delta_VLB:176.156990, VLB:1263.361624, err:0.015196\n",
      "iter:2, delta_VLB:0.000000, VLB:1263.361624, err:0.015196\n",
      "m or V converged\n",
      "iter:3, delta_VLB:5.717626, VLB:1257.643998, err:0.015110\n",
      "iter:4, delta_VLB:5.045001, VLB:1252.598997, err:0.015034\n",
      "iter:5, delta_VLB:4.601513, VLB:1247.997484, err:0.014963\n",
      "iter:6, delta_VLB:4.284092, VLB:1243.713391, err:0.014896\n",
      "iter:7, delta_VLB:4.043024, VLB:1239.670367, err:0.014833\n",
      "iter:8, delta_VLB:3.851237, VLB:1235.819130, err:0.014772\n",
      "iter:9, delta_VLB:3.694573, VLB:1232.124557, err:0.014712\n",
      "iter:10, delta_VLB:3.561314, VLB:1228.563243, err:0.014654\n",
      "iter:11, delta_VLB:3.445146, VLB:1225.118097, err:0.014597\n",
      "iter:12, delta_VLB:3.341016, VLB:1221.777081, err:0.014541\n",
      "iter:13, delta_VLB:3.246358, VLB:1218.530723, err:0.014488\n",
      "iter:14, delta_VLB:3.159869, VLB:1215.370855, err:0.014434\n",
      "iter:15, delta_VLB:3.078714, VLB:1212.292141, err:0.014381\n",
      "iter:16, delta_VLB:3.001758, VLB:1209.290382, err:0.014330\n",
      "iter:17, delta_VLB:2.929523, VLB:1206.360859, err:0.014280\n",
      "iter:18, delta_VLB:2.861286, VLB:1203.499573, err:0.014230\n",
      "iter:19, delta_VLB:2.792419, VLB:1200.707154, err:0.014182\n",
      "iter:20, delta_VLB:2.723316, VLB:1197.983838, err:0.014134\n",
      "iter:21, delta_VLB:2.653338, VLB:1195.330500, err:0.014087\n",
      "iter:22, delta_VLB:2.582559, VLB:1192.747941, err:0.014041\n",
      "iter:23, delta_VLB:2.512478, VLB:1190.235462, err:0.013996\n",
      "iter:24, delta_VLB:2.444836, VLB:1187.790627, err:0.013952\n",
      "iter:25, delta_VLB:2.376606, VLB:1185.414021, err:0.013907\n",
      "iter:26, delta_VLB:2.310664, VLB:1183.103357, err:0.013865\n",
      "iter:27, delta_VLB:2.242532, VLB:1180.860825, err:0.013823\n",
      "iter:28, delta_VLB:2.174528, VLB:1178.686298, err:0.013781\n",
      "iter:29, delta_VLB:2.105009, VLB:1176.581289, err:0.013744\n",
      "iter:30, delta_VLB:2.036591, VLB:1174.544697, err:0.013702\n",
      "iter:31, delta_VLB:1.965095, VLB:1172.579602, err:0.013669\n",
      "iter:32, delta_VLB:1.893181, VLB:1170.686421, err:0.013631\n",
      "iter:33, delta_VLB:1.819273, VLB:1168.867148, err:0.013599\n",
      "iter:34, delta_VLB:1.749916, VLB:1167.117232, err:0.013566\n",
      "iter:35, delta_VLB:1.678343, VLB:1165.438889, err:0.013534\n",
      "iter:36, delta_VLB:1.604228, VLB:1163.834662, err:0.013507\n",
      "iter:37, delta_VLB:1.536513, VLB:1162.298148, err:0.013480\n",
      "iter:38, delta_VLB:1.466643, VLB:1160.831506, err:0.013450\n",
      "iter:39, delta_VLB:1.391462, VLB:1159.440044, err:0.013426\n",
      "iter:40, delta_VLB:1.316157, VLB:1158.123887, err:0.013396\n",
      "iter:41, delta_VLB:1.247159, VLB:1156.876728, err:0.013375\n",
      "iter:42, delta_VLB:1.174954, VLB:1155.701774, err:0.013350\n",
      "iter:43, delta_VLB:1.105123, VLB:1154.596651, err:0.013335\n",
      "iter:44, delta_VLB:1.036902, VLB:1153.559750, err:0.013309\n",
      "iter:45, delta_VLB:0.975605, VLB:1152.584145, err:0.013297\n",
      "m or V converged\n",
      "iter:46, delta_VLB:0.000000, VLB:1152.584145, err:0.013297\n",
      "m or V converged\n",
      "iter:47, delta_VLB:0.917964, VLB:1151.666181, err:0.013271\n",
      "m or V converged\n",
      "iter:48, delta_VLB:0.000000, VLB:1151.666181, err:0.013271\n",
      "m or V converged\n",
      "iter:49, delta_VLB:0.865039, VLB:1150.801141, err:0.013265\n",
      "m or V converged\n",
      "iter:50, delta_VLB:0.000000, VLB:1150.801141, err:0.013265\n",
      "m or V converged\n",
      "iter:51, delta_VLB:0.819230, VLB:1149.981912, err:0.013242\n",
      "m or V converged\n",
      "iter:52, delta_VLB:0.000000, VLB:1149.981912, err:0.013242\n",
      "m or V converged\n",
      "iter:53, delta_VLB:0.779785, VLB:1149.202127, err:0.013234\n",
      "m or V converged\n",
      "iter:54, delta_VLB:0.000000, VLB:1149.202127, err:0.013234\n",
      "m or V converged\n",
      "iter:55, delta_VLB:0.725127, VLB:1148.477000, err:0.013222\n",
      "m or V converged\n",
      "iter:56, delta_VLB:0.000000, VLB:1148.477000, err:0.013222\n",
      "m or V converged\n",
      "iter:57, delta_VLB:0.675235, VLB:1147.801766, err:0.013215\n",
      "m or V converged\n",
      "iter:58, delta_VLB:0.000000, VLB:1147.801766, err:0.013215\n",
      "m or V converged\n",
      "iter:59, delta_VLB:0.630843, VLB:1147.170922, err:0.013197\n",
      "m or V converged\n",
      "iter:60, delta_VLB:0.000000, VLB:1147.170922, err:0.013197\n",
      "m or V converged\n",
      "iter:61, delta_VLB:0.586170, VLB:1146.584753, err:0.013201\n",
      "m or V converged\n",
      "iter:62, delta_VLB:0.000000, VLB:1146.584753, err:0.013201\n",
      "m or V converged\n",
      "iter:63, delta_VLB:0.558852, VLB:1146.025901, err:0.013181\n",
      "m or V converged\n",
      "iter:64, delta_VLB:0.000000, VLB:1146.025901, err:0.013181\n",
      "m or V converged\n",
      "iter:65, delta_VLB:0.536865, VLB:1145.489036, err:0.013179\n",
      "m or V converged\n",
      "iter:66, delta_VLB:0.000000, VLB:1145.489036, err:0.013179\n",
      "m or V converged\n",
      "iter:67, delta_VLB:0.499443, VLB:1144.989593, err:0.013165\n",
      "m or V converged\n",
      "iter:68, delta_VLB:0.000000, VLB:1144.989593, err:0.013165\n",
      "m or V converged\n",
      "iter:69, delta_VLB:0.460775, VLB:1144.528818, err:0.013162\n",
      "m or V converged\n",
      "iter:70, delta_VLB:0.000000, VLB:1144.528818, err:0.013162\n",
      "m or V converged\n",
      "iter:71, delta_VLB:0.444827, VLB:1144.083991, err:0.013144\n",
      "m or V converged\n",
      "iter:72, delta_VLB:0.000000, VLB:1144.083991, err:0.013144\n",
      "m or V converged\n",
      "iter:73, delta_VLB:0.411910, VLB:1143.672081, err:0.013150\n",
      "m or V converged\n",
      "iter:74, delta_VLB:0.000000, VLB:1143.672081, err:0.013150\n",
      "m or V converged\n",
      "iter:75, delta_VLB:0.395115, VLB:1143.276966, err:0.013129\n",
      "m or V converged\n",
      "iter:76, delta_VLB:0.000000, VLB:1143.276966, err:0.013129\n",
      "m or V converged\n",
      "iter:77, delta_VLB:0.359107, VLB:1142.917859, err:0.013131\n",
      "m or V converged\n",
      "iter:78, delta_VLB:0.000000, VLB:1142.917859, err:0.013131\n",
      "m or V converged\n",
      "iter:79, delta_VLB:0.344944, VLB:1142.572915, err:0.013118\n",
      "m or V converged\n",
      "iter:80, delta_VLB:0.000000, VLB:1142.572915, err:0.013118\n",
      "m or V converged\n",
      "iter:81, delta_VLB:0.309223, VLB:1142.263692, err:0.013114\n",
      "m or V converged\n",
      "iter:82, delta_VLB:0.000000, VLB:1142.263692, err:0.013114\n",
      "m or V converged\n",
      "iter:83, delta_VLB:0.307304, VLB:1141.956388, err:0.013114\n",
      "m or V converged\n",
      "iter:84, delta_VLB:0.000000, VLB:1141.956388, err:0.013114\n",
      "m or V converged\n",
      "iter:85, delta_VLB:0.285461, VLB:1141.670927, err:0.013102\n",
      "m or V converged\n",
      "iter:86, delta_VLB:0.000000, VLB:1141.670927, err:0.013102\n",
      "m or V converged\n",
      "iter:87, delta_VLB:0.247714, VLB:1141.423214, err:0.013115\n",
      "m or V converged\n",
      "iter:88, delta_VLB:0.000000, VLB:1141.423214, err:0.013115\n",
      "m or V converged\n",
      "iter:89, delta_VLB:0.286938, VLB:1141.136276, err:0.013100\n",
      "m or V converged\n",
      "iter:90, delta_VLB:0.000000, VLB:1141.136276, err:0.013100\n",
      "m or V converged\n",
      "iter:91, delta_VLB:0.211647, VLB:1140.924628, err:0.013103\n",
      "m or V converged\n",
      "iter:92, delta_VLB:0.000000, VLB:1140.924628, err:0.013103\n",
      "m or V converged\n",
      "iter:93, delta_VLB:0.225020, VLB:1140.699608, err:0.013092\n",
      "m or V converged\n",
      "iter:94, delta_VLB:0.000000, VLB:1140.699608, err:0.013092\n",
      "m or V converged\n",
      "iter:95, delta_VLB:0.178839, VLB:1140.520769, err:0.013106\n",
      "m or V converged\n",
      "iter:96, delta_VLB:0.000000, VLB:1140.520769, err:0.013106\n",
      "m or V converged\n",
      "iter:97, delta_VLB:0.211733, VLB:1140.309036, err:0.013092\n",
      "m or V converged\n",
      "iter:98, delta_VLB:0.000000, VLB:1140.309036, err:0.013092\n",
      "m or V converged\n",
      "iter:99, delta_VLB:0.156836, VLB:1140.152200, err:0.013097\n",
      "m or V converged\n",
      "iter:100, delta_VLB:0.000000, VLB:1140.152200, err:0.013097\n",
      "m or V converged\n",
      "iter:101, delta_VLB:0.224335, VLB:1139.927865, err:0.013089\n",
      "m or V converged\n",
      "iter:102, delta_VLB:0.000000, VLB:1139.927865, err:0.013089\n",
      "m or V converged\n",
      "iter:103, delta_VLB:0.122527, VLB:1139.805339, err:0.013086\n",
      "m or V converged\n",
      "iter:104, delta_VLB:0.000000, VLB:1139.805339, err:0.013086\n",
      "m or V converged\n",
      "iter:105, delta_VLB:0.175765, VLB:1139.629574, err:0.013081\n",
      "m or V converged\n",
      "iter:106, delta_VLB:0.000000, VLB:1139.629574, err:0.013081\n",
      "m or V converged\n",
      "iter:107, delta_VLB:0.071469, VLB:1139.558105, err:0.013094\n",
      "m or V converged\n",
      "iter:108, delta_VLB:0.000000, VLB:1139.558105, err:0.013094\n",
      "m or V converged\n",
      "iter:109, delta_VLB:0.131273, VLB:1139.426832, err:0.013081\n",
      "m or V converged\n",
      "iter:110, delta_VLB:0.000000, VLB:1139.426832, err:0.013081\n",
      "m or V converged\n",
      "iter:111, delta_VLB:0.104890, VLB:1139.321943, err:0.013087\n",
      "m or V converged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:112, delta_VLB:0.000000, VLB:1139.321943, err:0.013087\n",
      "m or V converged\n",
      "iter:113, delta_VLB:0.125783, VLB:1139.196159, err:0.013072\n",
      "m or V converged\n",
      "iter:114, delta_VLB:0.000000, VLB:1139.196159, err:0.013072\n",
      "m or V converged\n",
      "iter:115, delta_VLB:0.055158, VLB:1139.141001, err:0.013081\n",
      "m or V converged\n",
      "iter:116, delta_VLB:0.000000, VLB:1139.141001, err:0.013081\n",
      "m or V converged\n",
      "iter:117, delta_VLB:0.113090, VLB:1139.027911, err:0.013076\n",
      "m or V converged\n",
      "iter:118, delta_VLB:0.000000, VLB:1139.027911, err:0.013076\n",
      "m or V converged\n",
      "iter:119, delta_VLB:0.049791, VLB:1138.978121, err:0.013075\n",
      "m or V converged\n",
      "iter:120, delta_VLB:0.000000, VLB:1138.978121, err:0.013075\n",
      "m or V converged\n",
      "iter:121, delta_VLB:0.071896, VLB:1138.906224, err:0.013067\n",
      "m or V converged\n",
      "iter:122, delta_VLB:0.000000, VLB:1138.906224, err:0.013067\n",
      "m or V converged\n",
      "iter:123, delta_VLB:0.027046, VLB:1138.879178, err:0.013076\n",
      "m or V converged\n",
      "iter:124, delta_VLB:0.000000, VLB:1138.879178, err:0.013076\n",
      "m or V converged\n",
      "iter:125, delta_VLB:0.083159, VLB:1138.796019, err:0.013066\n",
      "m or V converged\n",
      "iter:126, delta_VLB:0.000000, VLB:1138.796019, err:0.013066\n",
      "m or V converged\n",
      "iter:127, delta_VLB:0.055384, VLB:1138.740634, err:0.013072\n",
      "m or V converged\n",
      "iter:128, delta_VLB:0.000000, VLB:1138.740634, err:0.013072\n",
      "m or V converged\n",
      "iter:129, delta_VLB:0.027635, VLB:1138.712999, err:0.013056\n",
      "m or V converged\n",
      "iter:130, delta_VLB:0.000000, VLB:1138.712999, err:0.013056\n",
      "m or V converged\n",
      "iter:131, delta_VLB:0.063064, VLB:1138.649935, err:0.013068\n",
      "m or V converged\n",
      "iter:132, delta_VLB:0.000000, VLB:1138.649935, err:0.013068\n",
      "m or V converged\n",
      "iter:133, delta_VLB:0.052337, VLB:1138.597599, err:0.013056\n",
      "m or V converged\n",
      "iter:134, delta_VLB:0.000000, VLB:1138.597599, err:0.013056\n",
      "m or V converged\n",
      "iter:135, delta_VLB:0.001422, VLB:1138.596176, err:0.013071\n",
      "m or V converged\n",
      "iter:136, delta_VLB:0.000000, VLB:1138.596176, err:0.013071\n",
      "m or V converged\n",
      "iter:137, delta_VLB:0.079235, VLB:1138.516941, err:0.013056\n",
      "m or V converged\n",
      "iter:138, delta_VLB:0.000000, VLB:1138.516941, err:0.013056\n",
      "m or V converged\n",
      "iter:139, delta_VLB:-0.018537, VLB:1138.535478, err:0.013061\n",
      "m or V converged\n",
      "iter:140, delta_VLB:0.000000, VLB:1138.535478, err:0.013061\n",
      "m or V converged\n",
      "After 140 iterations it converged: delta_VLB:0.000000, VLB:1138.535478, err:0.013061\n"
     ]
    }
   ],
   "source": [
    "FPb = Train(X_train,y_train,ix,X_test,y_test,max_iter=400,lr=2*10**-3,VLB_opt='FPb',lik='Gaussian')#Gaussian,Bernoulli\n",
    "Zs,Zs_label, VLB_FPb, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time_FPb,err_FPb = FPb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name : gp\n",
      "Objective : -139.7463246047188\n",
      "Number of Parameters : 3\n",
      "Number of Optimization Parameters : 3\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1mgp.                    \u001b[0;0m  |                value  |  constraints  |  priors\n",
      "  \u001b[1mrbf.variance           \u001b[0;0m  |   5.1020070037083425  |      +ve      |        \n",
      "  \u001b[1mrbf.lengthscale        \u001b[0;0m  |   7.3702655490680025  |      +ve      |        \n",
      "  \u001b[1mGaussian_noise.variance\u001b[0;0m  |  0.00979866354836197  |      +ve      |        \n",
      "Before iterations, VLB:1439.518614 err:0.015196\n",
      "iter:1, delta_VLB:185.027110, VLB:1254.491504, err:0.015062\n",
      "iter:2, delta_VLB:6.314797, VLB:1248.176707, err:0.014967\n",
      "iter:3, delta_VLB:5.230898, VLB:1242.945809, err:0.014886\n",
      "iter:4, delta_VLB:4.606285, VLB:1238.339524, err:0.014813\n",
      "iter:5, delta_VLB:4.192426, VLB:1234.147097, err:0.014745\n",
      "iter:6, delta_VLB:3.894105, VLB:1230.252992, err:0.014682\n",
      "iter:7, delta_VLB:3.665560, VLB:1226.587433, err:0.014621\n",
      "iter:8, delta_VLB:3.482860, VLB:1223.104572, err:0.014564\n",
      "iter:9, delta_VLB:3.330855, VLB:1219.773717, err:0.014509\n",
      "iter:10, delta_VLB:3.202386, VLB:1216.571331, err:0.014456\n",
      "iter:11, delta_VLB:3.091552, VLB:1213.479779, err:0.014403\n",
      "iter:12, delta_VLB:2.992336, VLB:1210.487443, err:0.014352\n",
      "iter:13, delta_VLB:2.903497, VLB:1207.583946, err:0.014302\n",
      "iter:14, delta_VLB:2.823444, VLB:1204.760502, err:0.014253\n",
      "iter:15, delta_VLB:2.748344, VLB:1202.012158, err:0.014206\n",
      "iter:16, delta_VLB:2.675437, VLB:1199.336721, err:0.014159\n",
      "iter:17, delta_VLB:2.603935, VLB:1196.732786, err:0.014114\n",
      "iter:18, delta_VLB:2.534563, VLB:1194.198223, err:0.014069\n",
      "iter:19, delta_VLB:2.465261, VLB:1191.732962, err:0.014025\n",
      "iter:20, delta_VLB:2.398304, VLB:1189.334658, err:0.013980\n",
      "iter:21, delta_VLB:2.334914, VLB:1186.999743, err:0.013938\n",
      "iter:22, delta_VLB:2.270981, VLB:1184.728762, err:0.013895\n",
      "iter:23, delta_VLB:2.209254, VLB:1182.519509, err:0.013855\n",
      "iter:24, delta_VLB:2.145361, VLB:1180.374148, err:0.013815\n",
      "iter:25, delta_VLB:2.082939, VLB:1178.291209, err:0.013774\n",
      "iter:26, delta_VLB:2.018418, VLB:1176.272791, err:0.013737\n",
      "iter:27, delta_VLB:1.955608, VLB:1174.317183, err:0.013702\n",
      "iter:28, delta_VLB:1.889021, VLB:1172.428163, err:0.013664\n",
      "iter:29, delta_VLB:1.823384, VLB:1170.604778, err:0.013632\n",
      "iter:30, delta_VLB:1.757914, VLB:1168.846865, err:0.013600\n",
      "iter:31, delta_VLB:1.691861, VLB:1167.155004, err:0.013564\n",
      "iter:32, delta_VLB:1.625943, VLB:1165.529060, err:0.013540\n",
      "iter:33, delta_VLB:1.560659, VLB:1163.968401, err:0.013508\n",
      "iter:34, delta_VLB:1.494020, VLB:1162.474381, err:0.013482\n",
      "iter:35, delta_VLB:1.428120, VLB:1161.046260, err:0.013456\n",
      "iter:36, delta_VLB:1.363856, VLB:1159.682404, err:0.013429\n",
      "iter:37, delta_VLB:1.290585, VLB:1158.391819, err:0.013403\n",
      "iter:38, delta_VLB:1.226386, VLB:1157.165433, err:0.013382\n",
      "iter:39, delta_VLB:1.160179, VLB:1156.005254, err:0.013359\n",
      "iter:40, delta_VLB:1.093830, VLB:1154.911424, err:0.013338\n",
      "iter:41, delta_VLB:1.024487, VLB:1153.886937, err:0.013317\n",
      "iter:42, delta_VLB:0.967794, VLB:1152.919142, err:0.013301\n",
      "iter:43, delta_VLB:0.913009, VLB:1152.006133, err:0.013284\n",
      "iter:44, delta_VLB:0.857632, VLB:1151.148502, err:0.013267\n",
      "iter:45, delta_VLB:0.801385, VLB:1150.347116, err:0.013255\n",
      "iter:46, delta_VLB:0.747848, VLB:1149.599269, err:0.013241\n",
      "iter:47, delta_VLB:0.702805, VLB:1148.896463, err:0.013230\n",
      "iter:48, delta_VLB:0.654809, VLB:1148.241655, err:0.013221\n",
      "iter:49, delta_VLB:0.613557, VLB:1147.628098, err:0.013211\n",
      "iter:50, delta_VLB:0.579421, VLB:1147.048677, err:0.013207\n",
      "iter:51, delta_VLB:0.533563, VLB:1146.515114, err:0.013193\n",
      "iter:52, delta_VLB:0.518418, VLB:1145.996696, err:0.013184\n",
      "iter:53, delta_VLB:0.469868, VLB:1145.526828, err:0.013178\n",
      "iter:54, delta_VLB:0.449826, VLB:1145.077002, err:0.013170\n",
      "iter:55, delta_VLB:0.417255, VLB:1144.659746, err:0.013156\n",
      "iter:56, delta_VLB:0.416656, VLB:1144.243090, err:0.013154\n",
      "iter:57, delta_VLB:0.378056, VLB:1143.865034, err:0.013140\n",
      "iter:58, delta_VLB:0.364498, VLB:1143.500535, err:0.013141\n",
      "iter:59, delta_VLB:0.330219, VLB:1143.170316, err:0.013131\n",
      "iter:60, delta_VLB:0.333788, VLB:1142.836528, err:0.013130\n",
      "iter:61, delta_VLB:0.290438, VLB:1142.546090, err:0.013122\n",
      "iter:62, delta_VLB:0.280950, VLB:1142.265140, err:0.013117\n",
      "iter:63, delta_VLB:0.263151, VLB:1142.001989, err:0.013112\n",
      "iter:64, delta_VLB:0.266029, VLB:1141.735960, err:0.013109\n",
      "iter:65, delta_VLB:0.226317, VLB:1141.509643, err:0.013109\n",
      "iter:66, delta_VLB:0.227458, VLB:1141.282184, err:0.013100\n",
      "iter:67, delta_VLB:0.202545, VLB:1141.079639, err:0.013107\n",
      "iter:68, delta_VLB:0.214038, VLB:1140.865602, err:0.013092\n",
      "iter:69, delta_VLB:0.179833, VLB:1140.685769, err:0.013093\n",
      "iter:70, delta_VLB:0.197806, VLB:1140.487963, err:0.013099\n",
      "iter:71, delta_VLB:0.170605, VLB:1140.317358, err:0.013091\n",
      "iter:72, delta_VLB:0.165552, VLB:1140.151806, err:0.013101\n",
      "iter:73, delta_VLB:0.137334, VLB:1140.014472, err:0.013098\n",
      "iter:74, delta_VLB:0.137166, VLB:1139.877306, err:0.013087\n",
      "iter:75, delta_VLB:0.143626, VLB:1139.733680, err:0.013098\n",
      "iter:76, delta_VLB:0.121108, VLB:1139.612572, err:0.013090\n",
      "iter:77, delta_VLB:0.106278, VLB:1139.506294, err:0.013090\n",
      "iter:78, delta_VLB:0.102352, VLB:1139.403942, err:0.013082\n",
      "iter:79, delta_VLB:0.094553, VLB:1139.309389, err:0.013088\n",
      "iter:80, delta_VLB:0.083125, VLB:1139.226263, err:0.013078\n",
      "iter:81, delta_VLB:0.066423, VLB:1139.159840, err:0.013086\n",
      "iter:82, delta_VLB:0.095859, VLB:1139.063981, err:0.013082\n",
      "iter:83, delta_VLB:0.078023, VLB:1138.985958, err:0.013074\n",
      "iter:84, delta_VLB:0.048944, VLB:1138.937015, err:0.013083\n",
      "iter:85, delta_VLB:0.045370, VLB:1138.891645, err:0.013083\n",
      "iter:86, delta_VLB:0.065380, VLB:1138.826264, err:0.013079\n",
      "iter:87, delta_VLB:0.028975, VLB:1138.797290, err:0.013084\n",
      "iter:88, delta_VLB:0.076665, VLB:1138.720625, err:0.013073\n",
      "iter:89, delta_VLB:0.004911, VLB:1138.715714, err:0.013085\n",
      "iter:90, delta_VLB:0.070761, VLB:1138.644952, err:0.013067\n",
      "iter:91, delta_VLB:-0.012262, VLB:1138.657214, err:0.013084\n",
      "iter:92, delta_VLB:0.071387, VLB:1138.585827, err:0.013064\n",
      "iter:93, delta_VLB:0.006713, VLB:1138.579114, err:0.013078\n",
      "iter:94, delta_VLB:0.043701, VLB:1138.535413, err:0.013062\n",
      "iter:95, delta_VLB:0.014352, VLB:1138.521061, err:0.013073\n",
      "iter:96, delta_VLB:0.044853, VLB:1138.476208, err:0.013063\n",
      "iter:97, delta_VLB:-0.001139, VLB:1138.477347, err:0.013074\n",
      "iter:98, delta_VLB:0.055949, VLB:1138.421398, err:0.013056\n",
      "iter:99, delta_VLB:-0.000236, VLB:1138.421634, err:0.013066\n",
      "iter:100, delta_VLB:0.027703, VLB:1138.393931, err:0.013053\n",
      "iter:101, delta_VLB:-0.000096, VLB:1138.394027, err:0.013066\n",
      "iter:102, delta_VLB:0.026691, VLB:1138.367336, err:0.013058\n",
      "iter:103, delta_VLB:0.004291, VLB:1138.363045, err:0.013069\n",
      "iter:104, delta_VLB:0.024251, VLB:1138.338794, err:0.013058\n",
      "iter:105, delta_VLB:-0.004656, VLB:1138.343450, err:0.013071\n",
      "iter:106, delta_VLB:0.028154, VLB:1138.315297, err:0.013061\n",
      "iter:107, delta_VLB:-0.005852, VLB:1138.321149, err:0.013070\n",
      "iter:108, delta_VLB:0.029238, VLB:1138.291911, err:0.013063\n",
      "iter:109, delta_VLB:-0.015805, VLB:1138.307716, err:0.013072\n",
      "iter:110, delta_VLB:0.042745, VLB:1138.264971, err:0.013068\n",
      "iter:111, delta_VLB:-0.032069, VLB:1138.297040, err:0.013072\n",
      "iter:112, delta_VLB:0.050488, VLB:1138.246552, err:0.013066\n",
      "iter:113, delta_VLB:-0.037299, VLB:1138.283851, err:0.013071\n",
      "iter:114, delta_VLB:0.052868, VLB:1138.230983, err:0.013064\n",
      "iter:115, delta_VLB:-0.039415, VLB:1138.270398, err:0.013077\n",
      "iter:116, delta_VLB:0.069243, VLB:1138.201155, err:0.013062\n",
      "iter:117, delta_VLB:-0.050518, VLB:1138.251673, err:0.013069\n",
      "iter:118, delta_VLB:0.059680, VLB:1138.191992, err:0.013064\n",
      "iter:119, delta_VLB:-0.029829, VLB:1138.221821, err:0.013073\n",
      "iter:120, delta_VLB:0.033892, VLB:1138.187929, err:0.013064\n",
      "iter:121, delta_VLB:-0.036740, VLB:1138.224669, err:0.013074\n",
      "iter:122, delta_VLB:0.061246, VLB:1138.163423, err:0.013070\n",
      "iter:123, delta_VLB:-0.059679, VLB:1138.223102, err:0.013070\n",
      "iter:124, delta_VLB:0.052000, VLB:1138.171102, err:0.013066\n",
      "iter:125, delta_VLB:-0.051098, VLB:1138.222200, err:0.013073\n",
      "iter:126, delta_VLB:0.047977, VLB:1138.174222, err:0.013065\n",
      "iter:127, delta_VLB:-0.048577, VLB:1138.222800, err:0.013073\n",
      "iter:128, delta_VLB:0.047239, VLB:1138.175561, err:0.013065\n",
      "negVLB did not decrease any more.\n"
     ]
    }
   ],
   "source": [
    "FPi = Train(X_train,y_train,ix,X_test,y_test,max_iter=400,lr=2*10**-3,VLB_opt='FPi',lik='Gaussian');#Bernoulli,Gaussian\n",
    "Zs,Zs_label, VLB_FPi, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time_FPi,err_FPi = FPi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GD2 = Train(X_train,y_train,ix,X_test,y_test,max_iter=400,lr=4*10**-4,VLB_opt='GD',lik='Poisson2');\n",
    "Zs,Zs_label, VLB2, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time2,err2 = GD2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FPb2 = Train(X_train,y_train,ix,X_test,y_test,max_iter=50,lr=2*10**-4,VLB_opt='FPb',lik='Poisson2')\n",
    "Zs,Zs_label, VLB_FPb2, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time_FPb2,err_FPb2 = FPb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPi2 = Train(X_train,y_train,ix,X_test,y_test,max_iter=50,lr=2*10**-4,VLB_opt='FPi',lik='Poisson2')\n",
    "Zs,Zs_label, VLB_FPi2, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time_FPi2,err_FPi2=FPi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FPi = Train(X_train,y_train,ix,X_test,y_test,max_iter=50,lr=2*10**-4,VLB_opt='FPi',lik='Poisson')\n",
    "Zs,Zs_label, VLB_FPi, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time_FPi,err_FPi=FPi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPi = Train(X_train,y_train,ix,X_test,y_test,max_iter=50,lr=2*10**-4,VLB_opt='FPi',lik='Bernoulli')\n",
    "Zs,Zs_label, VLB_FPi, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time_FPi,err_FPi=FPi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import matplotlib.pyplot as plt\n",
    "host = host_subplot(111)\n",
    "par = host.twinx()\n",
    "host.set_xlabel(\"Training time(log)\")\n",
    "host.set_ylabel(\"Error\")\n",
    "par.set_ylabel(\"negVLB\")\n",
    "# p0, = host.plot(VLB_time, err, 'bo--', lw=1, markersize=5, fillstyle='none', label=\"Poisson GD error\")\n",
    "# p1, = host.plot(VLB_time2, err2, 'ro--', lw=1, markersize=5, fillstyle='none', label=\"Poisson2 GD error\")\n",
    "# p2, = par.plot(VLB_time, VLB, 'bo-', lw=1, markersize=5, label=\"Poisson GD negVLB\")\n",
    "# p3, = par.plot(VLB_time2, VLB2, 'ro-', lw=1, markersize=5, label=\"Poisson2 GD negVLB\")\n",
    "# p0, = host.plot(VLB_time_FPb, err_FPb, 'bo--', lw=1, markersize=5, fillstyle='none', label=\"Poisson FPb error\")\n",
    "# p1, = host.plot(VLB_time_FPb2, err_FPb2, 'ro--', lw=1, markersize=5, fillstyle='none', label=\"Poisson2 FPb error\")\n",
    "# p2, = par.plot(VLB_time_FPb, VLB_FPb, 'bo-', lw=1, markersize=5, label=\"Poisson FPb negVLB\")\n",
    "# p3, = par.plot(VLB_time_FPb2, VLB_FPb2, 'ro-', lw=1, markersize=5, label=\"Poisson2 FPb negVLB\")\n",
    "p0, = host.plot(VLB_time_FPi, err_FPi, 'bo--', lw=1, markersize=5, fillstyle='none', label=\"Poisson FPi error\")\n",
    "p1, = host.plot(VLB_time_FPi2, err_FPi2, 'ro--', lw=1, markersize=5, fillstyle='none', label=\"Poisson2 FPi error\")\n",
    "p2, = par.plot(VLB_time_FPi, VLB_FPi, 'bo-', lw=1, markersize=5, label=\"Poisson FPb negVLB\")\n",
    "p3, = par.plot(VLB_time_FPi2, VLB_FPi2, 'ro-', lw=1, markersize=5, label=\"Poisson2 FPb negVLB\")\n",
    "host.set_xscale('log',basex=10)\n",
    "leg = plt.legend()\n",
    "host.yaxis.get_label().set_color(p0.get_color());par.yaxis.get_label().set_color(p0.get_color())\n",
    "leg.texts[0].set_color(p0.get_color());\n",
    "leg.texts[1].set_color(p1.get_color())\n",
    "leg.texts[2].set_color(p2.get_color());\n",
    "leg.texts[3].set_color(p3.get_color())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import matplotlib.pyplot as plt\n",
    "host = host_subplot(111)\n",
    "par = host.twinx()\n",
    "host.set_xlabel(\"Training time(log)\")\n",
    "host.set_ylabel(\"Error\")\n",
    "par.set_ylabel(\"negVLB\")\n",
    "p0, = host.plot(VLB_time, err, 'bo--', lw=1, markersize=5, fillstyle='none', label=\"error GD\")\n",
    "p1, = host.plot(VLB_time_FPi, err_FPi, 'ro--', lw=1, markersize=5, fillstyle='none', label=\"error FPi\")\n",
    "p2, = host.plot(VLB_time_FPb, err_FPb, 'go--', lw=1, markersize=5, fillstyle='none', label=\"error FPb\")\n",
    "p3, = host.plot(VLB_time_FPim, err_FPim, 'co--', markersize=5, lw=1, fillstyle='none', label=\"error FPi-mean\")\n",
    "p4, = par.plot(VLB_time, VLB, 'bo-', lw=1, markersize=5, label=\"negVLB GD\")\n",
    "p5, = par.plot(VLB_time_FPi, VLB_FPi, 'ro-', lw=1, markersize=5, label=\"negVLB FPi\")\n",
    "p6, = par.plot(VLB_time_FPb, VLB_FPb, 'go-', lw=1, markersize=5, label=\"negVLB FPb\")\n",
    "p7, = par.plot(VLB_time_FPim, VLB_FPim, 'c+-', lw=1, markersize=5, label=\"negVLB FPi-mean\")\n",
    "host.set_xscale('log',basex=10)\n",
    "leg = plt.legend()\n",
    "host.yaxis.get_label().set_color(p0.get_color());par.yaxis.get_label().set_color(p0.get_color())\n",
    "leg.texts[0].set_color(p0.get_color());\n",
    "leg.texts[1].set_color(p1.get_color())\n",
    "leg.texts[2].set_color(p2.get_color());\n",
    "leg.texts[3].set_color(p3.get_color())\n",
    "leg.texts[4].set_color(p4.get_color());\n",
    "leg.texts[5].set_color(p5.get_color())\n",
    "leg.texts[6].set_color(p6.get_color());\n",
    "leg.texts[7].set_color(p7.get_color())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some 1D training data(regression)\n",
    "num_train = 2000;num_test=700                         # 500 training poitns\n",
    "X_train = np.linspace(0, 10, num_train)[:,None]       # Inputs evenly spaced between 0 and 10\n",
    "F = np.sin(X_train)                   # True function (f = sin(x))\n",
    "y_train = F + 0.01*np.random.randn(num_train)[:,None]  # Observations\n",
    "X_test = np.linspace(0, 10, num_test)[:,None]       # Inputs evenly spaced between 0 and 10\n",
    "F_test = np.sin(X_test)                   # True function (f = sin(x))\n",
    "y_test = F_test + 0.01*np.random.randn(num_test)[:,None]  # Observations\n",
    "np.random.seed(4);num_inducing=200\n",
    "ix = random.sample(range(num_train), num_inducing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(y_train >= 0, 1, -1); y_test = np.where(y_test >= 0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data(count regression) count_dataset_ucsdpeds1l_N4000_D30,count_dataset_abalone_N4177_D9,count_dataset_flares_N1065_D24\n",
    "mat_contents = sio.loadmat('count_dataset_spacega_N3107_D6.mat')#count_dataset_epid_N6238_D17,count_dataset_bikehour2011_N8734_D49\n",
    "x = mat_contents['x']; y = mat_contents['y'] #count_dataset_segment_N2319_D19;class_dataset_madelon_N2600_D500;class_dataset_usps35_N1540_D256\n",
    "x = x.astype('double')#class_dataset_musk_N6598_D166;class_dataset_yeast_N1484_D8,count_dataset_spacega_N3107_D6\n",
    "\n",
    "# shuffle the data and split data into training set and test set\n",
    "data = np.concatenate((x,y), axis=1)\n",
    "np.random.shuffle(data)\n",
    "num_data = data.shape[0]; dim_data = data.shape[1] - 1;\n",
    "num_train = int(0.5*np.ceil(num_data)); num_test = num_data - num_train\n",
    "x_train = data[:num_train, :-1]; y_train = data[:num_train, -1];\n",
    "x_test = data[num_train:, :-1]; y_test = data[num_train:, -1];\n",
    "y_train = y_train[:, None]; y_test = y_test[:, None]\n",
    "# data Standardization with zero mean and unit variance\n",
    "scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "X_train = scaler.transform(x_train); X_test = scaler.transform(x_test)\n",
    "np.random.seed(1);num_inducing=200\n",
    "ix = random.sample(range(num_train), num_inducing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4176362])"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data from file, make sure banana.csv is in the same directory as this notebook\n",
    "data = np.genfromtxt('banana.csv', delimiter=',')\n",
    "\n",
    "# Dimension of data\n",
    "D = data.shape[1]-1\n",
    "\n",
    "# Seperate our data (input) from its corresponding label output\n",
    "# .. note we have to rescale from [-1,1] to [0,1] for a Bernoulli distribution\n",
    "X, y = data[:,:D], data[:,-1][:, None]\n",
    "\n",
    "# We will plot our data as well\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "# Plot 0 class in blue\n",
    "plt.plot(X[np.where(y == -1),0],X[np.where(y == -1),1],'bo', mew=0.5, alpha=0.5)\n",
    "# Plot 1 class in red\n",
    "plt.plot(X[np.where(y == 1),0],X[np.where(y == 1),1],'ro', mew=0.5, alpha=0.5)\n",
    "\n",
    "# Annotate plot\n",
    "plt.xlabel(\"$x_1$\"), plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Banana Dataset (red=1, blue=0)\")\n",
    "plt.axis(\"square\"), plt.xlim((-3, 3)), plt.ylim((-3, 3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from file, make sure banana.csv is in the same directory as this notebook\n",
    "data = np.genfromtxt('banana.csv', delimiter=',')\n",
    "\n",
    "# Dimension of data\n",
    "D = data.shape[1]-1\n",
    "n = data.shape[0]\n",
    "# Seperate our data (input) from its corresponding label output\n",
    "# .. note we have to rescale from [-1,1] to [0,1] for a Bernoulli distribution\n",
    "X, y = data[:,:D], data[:,-1][:, None]\n",
    "num_train = 3000; num_test = n - num_train\n",
    "X_train, y_train = X[:num_train,:], y[:num_train]\n",
    "X_test, y_test = X[:num_test,:], y[:num_test]\n",
    "np.random.seed(3);num_inducing=100 # randomly select active set and then keep them fixed\n",
    "ix = random.sample(range(num_train), num_inducing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "num = np.arange(-39,40);print(num)\n",
    "plt.figure(1)\n",
    "a1 = norm.cdf(num);print(a1)\n",
    "a2 = norm.pdf(num);print(a2)\n",
    "plt.plot(a1)\n",
    "plt.plot(a2)\n",
    "plt.figure(2)\n",
    "r = norm.pdf(num)/norm.cdf(num)\n",
    "plt.plot(r);print(r.shape)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4736461348785476e-196"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.pdf(-30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(np.linalg.eigvals(V) > 0)# check PSD condition for V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "  'a': lambda x: x * 5,\n",
    "  'b': lambda x: x + 7,\n",
    "  'c': lambda x: x - 2\n",
    "}[value](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "X_train, y_train = load_svmlight_file(\"spacega.txt\")#segment,cpusmall,mg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3107, 6)\n",
      "(3107, 1)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "X_train=scipy.sparse.csr_matrix.todense(X_train)\n",
    "y_train=y_train[:,None]\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "sio.savemat('count_dataset_spacega_N3107_D6.mat',{'x':X_train,'y':y_train})#count_dataset_segment_N2319_D19,count_dataset_cpusmall_N8192_D12,count_dataset_mg_N1385_D6,count_dataset_spacega_N3107_D6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "fruitfly_data = fetch_openml(name='fruitfly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, 4)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruitfly_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125,)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruitfly_data.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2310, 19)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
